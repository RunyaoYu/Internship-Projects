{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Mlaj48Wr3C"
      },
      "source": [
        "# Uniper Account Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLz8KDbiZFwy",
        "outputId": "22f962e3-3c8d-492e-fb5f-5e951bb75e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=c7599ee9b2309fcf591945ea264b659fcfdf0c6beb8414f38ea6d9d17a0e0b14\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ],
      "source": [
        "! pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QJuf6DesWr3E",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from docx import Document\n",
        "import matplotlib.pyplot as plt\n",
        "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
        "import pickle\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "import dill as pkl\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "TODAY = date.today().strftime(\"%Y%m%d\")\n",
        "CLIENT = \"Uniper\"\n",
        "MIN_NUM_OF_SAMPLES = 5\n",
        "COUNTRY = 'DE'  # possible choices: DE, UK, AT, SE, UBX\n",
        "SAVE_CLFS = True\n",
        "PREDICTIONS_EXCEL = False\n",
        "RES_DIR = Path(f\"./retraining_october21/{COUNTRY.lower()}\")\n",
        "SCAN_ID_COL = \"gl_document_scan_id\"  # document identifier col used when reducing\n",
        "                                     # global df to relevant examples for attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTIz8YBpXgI9",
        "outputId": "4fabbb8a-97b3-4995-8d00-b4bfbdebb794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GiE7fS74Wr3G"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(RES_DIR):\n",
        "    os.makedirs(RES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GdHVzayLWr3G"
      },
      "outputs": [],
      "source": [
        "# %load training_utils.py\n",
        "\"\"\"This file contains helper functionality to train/evaluate models\n",
        "   and create reports\"\"\"\n",
        "\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from docx import Document\n",
        "import matplotlib.pyplot as plt\n",
        "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
        "\n",
        "\n",
        "def reduce_to_relevant(df: DataFrame, col: str, min_num_samples: int) -> DataFrame:\n",
        "    \"\"\"Reduces df to instances with values in col that appear at least\n",
        "       min_num_samples times and returns reduced df.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame.\n",
        "        col (str): Name of the column that holds the feature of interest.\n",
        "        min_num_samples (int): Minimum number of times a value has to appear\n",
        "            in column 'col'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Reduced DataFrame containing only those values in 'col'\n",
        "            that appear more than 'min_num_samples' times.\n",
        "\n",
        "    \"\"\"\n",
        "    # find rows for values that appear at least min_num_samples times\n",
        "    relevant = [x for x in df[col].value_counts().index\n",
        "                if df[col].value_counts()[x] >= min_num_samples]\n",
        "    # create boolean mask\n",
        "    mask = [(x in relevant) for x in df[col]]\n",
        "\n",
        "    print(\n",
        "        f\"Reduced to {len(df[mask])} samples from {len(relevant)} relevant classes. (N={min_num_samples})\"\n",
        "    )\n",
        "\n",
        "    return df[mask]\n",
        "\n",
        "\n",
        "def get_reduced_df(\n",
        "        df: DataFrame,\n",
        "        feature_col: str,\n",
        "        scan_id_col: str,\n",
        "        min_num_samples: int) -> DataFrame:\n",
        "    \"\"\"Drops duplicates and reduces df to relevant examples that appear\n",
        "       at least min_num_samples times and are unambiguous.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame.\n",
        "        feature_col (str): Name of the column that holds the feature of interest.\n",
        "        scan_id_col (str): Name of the column that holds the unique\n",
        "            document identifier.\n",
        "        min_num_samples (int): Minimum number of times a value has to appear\n",
        "            in column 'feature_col'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Reduced DataFrame containing only those values in 'feature_col'\n",
        "            that appear more than 'min_num_samples' times and are unambiguous.\n",
        "\n",
        "    \"\"\"\n",
        "    # keep only documents with unambiguous value for this col\n",
        "    df_ = df.drop_duplicates(subset=[scan_id_col, feature_col])\\\n",
        "            .groupby(scan_id_col)\\\n",
        "            .filter(lambda x: len(x) == 1)\n",
        "\n",
        "    return reduce_to_relevant(df_, feature_col, min_num_samples)\n",
        "\n",
        "\n",
        "def split_for_target_col(df, col, test_size=0.2, random_state=666):\n",
        "    \"\"\"\n",
        "    Performs train test split with specified col as target variable.\n",
        "    Returns: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    return train_test_split(\n",
        "        df,\n",
        "        df[col],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_top_n_results_with_confs(\n",
        "        clazzes: Sequence[str],\n",
        "        probs: List[float],\n",
        "        n: int = 1) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Constructs list of (class, proba) tuples for top n results.\n",
        "\n",
        "    Args:\n",
        "        clazzes (Sequence[str]): Sequence of class names as stored in\n",
        "            clf.classes_ attribute of sklearn classifier.\n",
        "        probs (List[float]): List with probabilities for each class in clazzes.\n",
        "        n (int): Number of most probable results to return. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: List of (class, proba) tuples for top n results.\n",
        "\n",
        "    \"\"\"\n",
        "    return sorted(\n",
        "        zip(clazzes, probs),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )[:n]\n",
        "\n",
        "\n",
        "def get_results_for_target(\n",
        "        target_clf: Any,\n",
        "        df: DataFrame) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Computes predictions with provided classifier on DataFrame df.\n",
        "\n",
        "    Args:\n",
        "        target_clf (Any): Sklearn classifier that offers 'predict_proba()'.\n",
        "        df (DataFrame): Input DataFrame as expected by 'target_clf'.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: List of (class, proba) tuples for\n",
        "            instances in df.\n",
        "\n",
        "    \"\"\"\n",
        "    probs = target_clf.predict_proba(df)\n",
        "    clazzes = target_clf.classes_\n",
        "    results = []\n",
        "    for prob_list in probs:\n",
        "        results.append(_get_top_n_results_with_confs(clazzes, prob_list)[0])\n",
        "    return results\n",
        "\n",
        "\n",
        "def _get_text_col_from_df(df: DataFrame):\n",
        "    return df['text']\n",
        "\n",
        "\n",
        "def save_clf_to_disk(\n",
        "        clf,\n",
        "        attribute_name: str,\n",
        "        folder: Path,\n",
        "        date: str,\n",
        "        client: str,\n",
        "        country: str,\n",
        "        min_num_samples: int,\n",
        "        add_zip=True) -> None:\n",
        "\n",
        "    # safe to pkl with full info in file name\n",
        "    pkl_path = folder / f\"{date}_{client}_clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl\"\n",
        "    with open(pkl_path, 'wb') as file:\n",
        "        pkl.dump(clf, file)\n",
        "\n",
        "    if add_zip:\n",
        "        # create zip with non changing name for easy deployment\n",
        "        zip_path = folder / f\"clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl.zip\"\n",
        "        with ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as file:\n",
        "            # second argument avoids recreation of folder structure in zip-archive\n",
        "            file.write(pkl_path, pkl_path.parts[-1])\n",
        "\n",
        "\n",
        "def enrich_kw_list(kw_list):\n",
        "    \"\"\"Enriches keywords by replacing street names with possible synonyms. Since we demand ALL keywords to\n",
        "       be found for a positive match, this function returns a list of keyword lists, one for every possible\n",
        "       synonym.\n",
        "    \n",
        "    \"\"\"\n",
        "    new_kw_lists = [kw_list]\n",
        "    street_synonyms = [\"str.\", \"strasse\", \"straße\"]\n",
        "    for street_syn in street_synonyms:\n",
        "        remaining_syns = [x for x in street_synonyms if x != street_syn]\n",
        "        if any(street_syn in kw for kw in kw_list):\n",
        "            new_kw_lists += [[kw.replace(street_syn, synonym) for kw in kw_list] for synonym in remaining_syns]\n",
        "    return new_kw_lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sM-OMSYeWr3J"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubr4sfWSWr3J",
        "outputId": "a4bf8a4c-3b11-4d8e-85da-40dcf30f0ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models trained on Sunday 06. February 2022 with package versions: \n",
            "\n",
            "scikit-learn: 1.0.2\n",
            "dill: 0.3.4\n"
          ]
        }
      ],
      "source": [
        "tday = date.today().strftime(\"%A %d. %B %Y\") \n",
        "print(f\"Models trained on {tday} with package versions: \\n\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"dill: {pkl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "stzaJKF6Wr3K"
      },
      "outputs": [],
      "source": [
        "# dicts used to restrict ground truth later\n",
        "country_group_to_countries = {\n",
        "    \"DE\": [\"DE\"],\n",
        "    \"SE\": [\"SE\"],\n",
        "    \"AT\": [\"AT\"],\n",
        "    \"UK\": [\"GB\"],\n",
        "    \"UBX\": [\"BE\", \"NL\", \"LU\"]    \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "O02m0SaeWr3K",
        "outputId": "b77ace3d-8d9f-4fc6-bd33-49a579fe4a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(160704, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 32962 samples from 3 relevant classes. (N=5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3485428f-f57a-489c-bc1e-1d79b26e1327\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gl_legal_entity_id</th>\n",
              "      <th>gl_accounts_id</th>\n",
              "      <th>gl_cost_center_id</th>\n",
              "      <th>gl_wbs_element_id</th>\n",
              "      <th>gl_vendor_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734082001</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002000582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734934025</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002449868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734067004</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002053354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734934001</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001000092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734030000</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001257969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90275</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734999000</td>\n",
              "      <td>2937052000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002006228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90276</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734940001</td>\n",
              "      <td>2937060000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001435553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90277</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734934012</td>\n",
              "      <td>2937039200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002037197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90278</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734950002</td>\n",
              "      <td>2937059000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002479202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90279</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734934012</td>\n",
              "      <td>2937039200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002037197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45775 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3485428f-f57a-489c-bc1e-1d79b26e1327')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3485428f-f57a-489c-bc1e-1d79b26e1327 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3485428f-f57a-489c-bc1e-1d79b26e1327');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      gl_legal_entity_id gl_accounts_id  ... gl_wbs_element_id gl_vendor_id\n",
              "0                   0037     1734082001  ...               NaN   0002000582\n",
              "1                   0037     1734934025  ...               NaN   0002449868\n",
              "2                   0037     1734067004  ...               NaN   0002053354\n",
              "3                   0037     1734934001  ...               NaN   0001000092\n",
              "4                   0037     1734030000  ...               NaN   0001257969\n",
              "...                  ...            ...  ...               ...          ...\n",
              "90275               9370     1734999000  ...               NaN   0002006228\n",
              "90276               9370     1734940001  ...               NaN   0001435553\n",
              "90277               9370     1734934012  ...               NaN   0002037197\n",
              "90278               9370     1734950002  ...               NaN   0002479202\n",
              "90279               9370     1734934012  ...               NaN   0002037197\n",
              "\n",
              "[45775 rows x 5 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pickle_texts = \"/content/drive/MyDrive/KPMG/Multiple_Classification/texts_all_rt202110.pkl\"\n",
        "uniper =  \"/content/drive/MyDrive/KPMG/Multiple_Classification/Uniper_GT_09_21.xlsx\"\n",
        "sc21_json = \"/content/drive/MyDrive/KPMG/Multiple_Classification/sc21v8.json\"\n",
        "\n",
        "with open(pickle_texts, \"rb\") as file:\n",
        "    df_lume = pickle.load(file)\n",
        "df_lume.drop_duplicates(inplace=True)\n",
        "print(df_lume.shape)\n",
        "\n",
        "df_ground_truth = pd.read_excel(uniper)\n",
        "len(df_ground_truth)\n",
        "\n",
        "df_ground_truth.dropna(subset=[SCAN_ID_COL], inplace=True)\n",
        "df_ground_truth[SCAN_ID_COL] = df_ground_truth[SCAN_ID_COL].apply(lambda x: x.lower())\n",
        "\n",
        "len(df_ground_truth[SCAN_ID_COL].unique())\n",
        "df_merged = df_lume.merge(df_ground_truth, left_on=[\"filename\"], right_on=[SCAN_ID_COL], how=\"inner\")\n",
        "\n",
        "df_merged.drop_duplicates(inplace=True)\n",
        "df_merged.shape\n",
        "\n",
        "# add left-hand zeros to gl_legal_entity_id, gl_accounts_id, gl_vendor_id\n",
        "df_merged['gl_legal_entity_id'] = df_merged['gl_legal_entity_id'].apply(lambda x: str(int(x)).zfill(4))\n",
        "df_merged['gl_accounts_id'] = df_merged['gl_accounts_id'].apply(lambda x: str(int(x)).zfill(10))\n",
        "df_merged['gl_vendor_id'] = df_merged['gl_vendor_id'].apply(lambda x: str(int(x)).zfill(10))\n",
        "\n",
        "# restict to respective country(group)\n",
        "df = df_merged[\n",
        "    df_merged['le_country_id'].isin(country_group_to_countries[COUNTRY])\n",
        "]\n",
        "\n",
        "df['le_country_id'].value_counts()\n",
        "\n",
        "df['gl_posting_id'] = df['gl_posting_id'].apply(lambda x: str(int(x)))\n",
        "\n",
        "dft = get_reduced_df(df, 'gl_posting_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
        "dft[dft['gl_posting_id'] == '50']\n",
        "df[['gl_legal_entity_id', 'gl_accounts_id', 'gl_cost_center_id', 'gl_wbs_element_id', 'gl_vendor_id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTdp9GP4Wr3L",
        "outputId": "f98e7485-6962-42b7-d0ca-349eaf84a889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(loc, value, pi)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished remapping, writing gl_document_scan_id of mapped invoices to excel file\n"
          ]
        }
      ],
      "source": [
        "with open(sc21_json, 'r') as file:\n",
        "    mapping_json = json.load(file)\n",
        "\n",
        "enrich_kw_list([\"a\", \"b\", \"heinestrasse\"])\n",
        "\n",
        "# apply mapping if df contains any of the relevant legeal entity ids\n",
        "\n",
        "remapped_docs = []\n",
        "\n",
        "if any([key in df['gl_legal_entity_id'].unique() for key in mapping_json.keys()]):\n",
        "    \n",
        "    #print(\"Starting to apply remapping script...\")\n",
        "    \n",
        "    type_to_col = {\n",
        "        'psp': 'gl_wbs_element_id',\n",
        "        'cc': 'gl_cost_center_id',\n",
        "        None: ['gl_cost_center_id', 'gl_order_id', 'gl_wbs_element_id']\n",
        "    }\n",
        "\n",
        "    # from collections import defaultdict\n",
        "    # old_vals = defaultdict(list)\n",
        "    # new_vals = defaultdict(list)\n",
        "\n",
        "    for legal_entity, mapping_list in mapping_json.items():\n",
        "        for mapping in mapping_list:\n",
        "\n",
        "            gl_acc, old_value, old_type, new_value, new_type, gl_vend, keywords = mapping\n",
        "\n",
        "            if old_value is None:\n",
        "                continue\n",
        "                \n",
        "            # added 20211021\n",
        "            if old_type is None and new_type is None:\n",
        "                continue\n",
        "\n",
        "            # fill up with left hand zeros\n",
        "            # gl account auch mit 0 füllen???\n",
        "            if old_type == 'cc':\n",
        "                old_value = old_value.zfill(10)\n",
        "            if new_type == 'cc':\n",
        "                new_value = new_value.zfill(10)\n",
        "            if gl_vend is not None:\n",
        "                gl_vend = gl_vend.zfill(10)\n",
        "\n",
        "            old_col = type_to_col[old_type]\n",
        "            new_col = type_to_col[new_type]\n",
        "\n",
        "            # create filter mask\n",
        "            if old_value == '*':\n",
        "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
        "                    & (df['gl_accounts_id'] == gl_acc)\n",
        "            else:\n",
        "                #print(f\"legal entity {legal_entity}, mapping {mapping}\")\n",
        "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
        "                    & (df['gl_accounts_id'] == gl_acc) \\\n",
        "                    & (df[old_col] == old_value)\n",
        "\n",
        "            if gl_vend is not None:\n",
        "                mask = mask & (df['gl_vendor_id'] == gl_vend)\n",
        "\n",
        "            if keywords:\n",
        "                kw_lists = enrich_kw_list(keywords)\n",
        "                kw_mask = df['text'].apply(\n",
        "                    lambda text: any([all([kw in text for kw in kws]) for kws in kw_lists])\n",
        "                )\n",
        "                mask = mask & kw_mask\n",
        "\n",
        "            # set new values, override old ones if necessary\n",
        "            if not df.loc[mask].empty:\n",
        "                #print(f'found value {old_value} of type {old_type} for legal entity {legal_entity}')\n",
        "                if old_col == new_col:\n",
        "    #                 print('old_col == new_col')\n",
        "                    df.loc[mask, new_col] = new_value\n",
        "    #                 old_vals[legal_entity].append(old_value)\n",
        "    #                 new_vals[legal_entity].append(new_value)\n",
        "                else:\n",
        "    #                 print('old_col != new_col')\n",
        "                    df.loc[mask, old_col] = np.nan\n",
        "                    df.loc[mask, new_col] = new_value\n",
        "                \n",
        "                if isinstance(df.loc[mask, SCAN_ID_COL], str):\n",
        "                    remapped_docs.append(df.loc[mask, SCAN_ID_COL])\n",
        "                else:\n",
        "                    remapped_docs += list(df.loc[mask, SCAN_ID_COL])\n",
        "                \n",
        "    print(f\"finished remapping, writing {SCAN_ID_COL} of mapped invoices to excel file\")\n",
        "    pd.DataFrame.from_dict({SCAN_ID_COL: list(set(remapped_docs))})\\\n",
        "            .to_excel(RES_DIR / f\"{TODAY}_{CLIENT}_remapped_invoices.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2Pz0W_DrWr3M"
      },
      "outputs": [],
      "source": [
        "# Runyao's update\n",
        "def get_certain_class_after_vec_country(df_lume, label):\n",
        "    label = label\n",
        "    df_attr = get_reduced_df(df_lume, label, SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
        "    x_train, x_test, y_train, y_test = split_for_target_col_stratified(df_attr, label)\n",
        "    vectorizer = TfidfVectorizer(max_features=20000, max_df=0.75, sublinear_tf=True,)\n",
        "    X_train = x_train['text']\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test = x_test['text']\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return x_train, x_test, y_train, y_test, X_train_vec, X_test_vec\n",
        "\n",
        "\n",
        "def split_for_target_col_stratified(df, col, test_size=0.2, random_state=42,):\n",
        "    return train_test_split(df,\n",
        "        df[col],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df[col]\n",
        "    )\n",
        "\n",
        "\n",
        "def train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl):\n",
        "\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    y_train_pred = model.predict(X_train_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    if save_pkl == True:\n",
        "      pkl_path = \"/content/retraining_october21/model.pkl\"\n",
        "      with open(pkl_path, \"wb\") as file:\n",
        "        pkl.dump(model, file)\n",
        "\n",
        "    print(model.best_params_)\n",
        "    print(model.best_score_)\n",
        "    print(\"Training Accuracy: {:.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test, y_test_pred)))\n",
        "    print(classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Iw_t-L4Ua9Rb"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "def train_model(label, save_pkl=False):\n",
        "  x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_gl_id, label)\n",
        "\n",
        "  XGB = XGBClassifier(random_state=42)\n",
        "  parameters = {'n_estimators': [100],'max_depth':[5]}\n",
        "  model = GridSearchCV(XGB, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
        "  #model.get_params().keys()\n",
        "  train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Llnvq5Wr3N",
        "outputId": "56ba9201-fd2f-4971-ac75-30299117051d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 28168 samples from 176 relevant classes. (N=5)\n"
          ]
        }
      ],
      "source": [
        "df_gl_id = get_reduced_df(df, 'gl_accounts_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "QH-kjT8VCtsk",
        "outputId": "01f84dc3-a3eb-47aa-cfe0-2a5c2c64f8d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 28164 samples from 21 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
            "{'max_depth': 5, 'n_estimators': 100}\n",
            "0.9822025330713539\n",
            "Training Accuracy: 0.996\n",
            "Test Accuracy: 0.983\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        0037       1.00      1.00      1.00        56\n",
            "        0065       0.99      0.99      0.99       299\n",
            "        0301       0.97      0.99      0.98      1574\n",
            "        0303       1.00      0.99      1.00      1216\n",
            "        0330       0.98      0.98      0.98       314\n",
            "        0362       1.00      0.98      0.99       109\n",
            "        0377       0.94      1.00      0.97        32\n",
            "        0601       1.00      0.99      0.99       292\n",
            "        0801       0.98      0.96      0.97       898\n",
            "        0806       1.00      1.00      1.00        42\n",
            "        0811       1.00      0.83      0.91        12\n",
            "        0821       1.00      1.00      1.00       209\n",
            "        2101       0.99      1.00      0.99       260\n",
            "        2111       1.00      0.98      0.99       122\n",
            "        3104       0.94      0.94      0.94        35\n",
            "        3401       1.00      1.00      1.00        12\n",
            "        3420       0.98      1.00      0.99        44\n",
            "        9301       1.00      0.25      0.40         4\n",
            "        9310       0.92      0.80      0.86        41\n",
            "        9351       0.95      1.00      0.97        37\n",
            "        9370       0.96      0.96      0.96        25\n",
            "\n",
            "    accuracy                           0.98      5633\n",
            "   macro avg       0.98      0.94      0.95      5633\n",
            "weighted avg       0.98      0.98      0.98      5633\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "label = 'gl_legal_entity_id' # define label here\n",
        "train_model(label, save_pkl=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOI9cVIXPH8-",
        "outputId": "a38e1d76-4262-4e9a-8a17-de447382261b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 23189 samples from 768 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "label = 'gl_vendor_id' # depth 5\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAtHriwlROxq"
      },
      "outputs": [],
      "source": [
        "def train_model(label, save_pkl=False):\n",
        "  x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df, label)\n",
        "\n",
        "  XGB = XGBClassifier(random_state=42)\n",
        "  parameters = {'n_estimators': [100],'max_depth':[3]}\n",
        "  model = GridSearchCV(XGB, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
        "  #model.get_params().keys()\n",
        "  train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KykJ-0cqRZ7_"
      },
      "outputs": [],
      "source": [
        "label = 'gl_vendor_id' # depth 3\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oah1S1Wr3Q"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw6ynIIqWr3R",
        "outputId": "30337bf3-1e97-4556-976f-d0a95886453d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(31656, 31656)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare cost element column\n",
        "\n",
        "df_kostenstelle = df[df['gl_cost_center_id'].notna()]\n",
        "df_kostenstelle['Target'] = df_kostenstelle['gl_cost_center_id']\n",
        "\n",
        "df_psp = df[df['gl_wbs_element_id'].notna()]\n",
        "df_psp['Target'] = df_psp['gl_wbs_element_id']\n",
        "\n",
        "df_auftrag = df[df['gl_order_id'].notna()]\n",
        "df_auftrag['Target'] = df_auftrag['gl_order_id']\n",
        "\n",
        "df_pka = pd.concat([df_auftrag,df_kostenstelle, df_psp], axis=0)\n",
        "df_pka['Target'] = df_pka['Target'].apply(lambda x: str(x))\n",
        "\n",
        "df_pka.dropna(subset=['text'], inplace=True)\n",
        "df_pka.drop_duplicates(subset=['text'], inplace=True)\n",
        "\n",
        "len(df_pka),len(df_pka['text'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-GkQOTikWr3R"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/retraining_october21/model.pkl\", \"rb\") as file:\n",
        "    clf_bukr = pkl.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdakO5t9Wr3R"
      },
      "source": [
        "## Train classifiers (always uses N=1 here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqLoduwcWr3S",
        "outputId": "fb55237c-af12-4195-86b3-84db9b6fc7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "company code 0037: 279 entries for 6 classes\n",
            "company code 0065: 1721 entries for 174 classes\n",
            "company code 0301: 7176 entries for 782 classes\n",
            "company code 0303: 8809 entries for 1207 classes\n",
            "company code 0330: 1605 entries for 107 classes\n",
            "company code 0362: 563 entries for 36 classes\n",
            "company code 0377: 173 entries for 23 classes\n",
            "company code 0601: 2245 entries for 276 classes\n",
            "company code 0801: 4789 entries for 568 classes\n",
            "company code 0806: 229 entries for 20 classes\n",
            "company code 0811: 53 entries for 10 classes\n",
            "company code 0821: 1069 entries for 105 classes\n",
            "company code 2101: 1321 entries for 137 classes\n",
            "company code 2111: 624 entries for 57 classes\n",
            "company code 3104: 172 entries for 21 classes\n",
            "company code 3401: 62 entries for 1 classes\n",
            "company code 3420: 279 entries for 20 classes\n",
            "company code 9301: 23 entries for 9 classes\n",
            "company code 9310: 187 entries for 61 classes\n",
            "company code 9351: 168 entries for 21 classes\n",
            "company code 9370: 104 entries for 18 classes\n"
          ]
        }
      ],
      "source": [
        "local_min_num = 1\n",
        "\n",
        "for company_code in clf_bukr.classes_:\n",
        "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
        "    num_classes = len(df_cost_elem[\"Target\"].unique())\n",
        "    print(f\"company code {str(company_code).zfill(4)}: {len(df_cost_elem)} entries for {num_classes} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZSVVncmAhNsm"
      },
      "outputs": [],
      "source": [
        "def train_com_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl):\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    y_train_pred = model.predict(X_train_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    if save_pkl == True:\n",
        "      pkl_path = \"/content/retraining_october21/model.pkl\"\n",
        "      with open(pkl_path, \"wb\") as file:\n",
        "        pkl.dump(model, file)\n",
        "\n",
        "    print(\"Training Accuracy: {:.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test, y_test_pred)))\n",
        "    print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qYWAhMJVOW2A"
      },
      "outputs": [],
      "source": [
        "def train_com_model(label, save_pkl=False):\n",
        "  x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_ce, label)\n",
        "  model = XGBClassifier(max_depth=5, random_state=42)\n",
        "  train_com_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KksReKi6Wr3S",
        "outputId": "0532221b-1426-4c36-f2b3-59d6fdebe1fa",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company Code: 0037\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 279 samples from 6 relevant classes. (N=1)\n",
            "Reduced to 271 samples from 2 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  0000037001       1.00      0.98      0.99        52\n",
            "  0000037330       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.98        55\n",
            "   macro avg       0.88      0.99      0.92        55\n",
            "weighted avg       0.99      0.98      0.98        55\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0065\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1721 samples from 174 relevant classes. (N=1)\n",
            "Reduced to 1498 samples from 37 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.860\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "0065.1206O3.201.00.10.03       1.00      0.80      0.89        10\n",
            "0065.1206O3.201.00.10.06       1.00      1.00      1.00         2\n",
            "0065.1206OA.203.00.10.02       0.00      0.00      0.00         1\n",
            "0065.1206OA.203.00.10.07       1.00      0.60      0.75         5\n",
            "0065.1206OA.203.00.10.08       0.33      0.25      0.29         4\n",
            "0065.1206OA.203.00.10.14       0.75      1.00      0.86         3\n",
            "0065.1206OA.207.00.10.07       0.67      0.50      0.57         4\n",
            "0065.1206OA.207.00.10.08       0.00      0.00      0.00         1\n",
            "0065.1206OA.208.00.10.07       0.57      1.00      0.73         4\n",
            "0065.1209FA.220.00.97.11       0.92      1.00      0.96        12\n",
            "0065.1602JA.206.00.60.10       1.00      1.00      1.00         1\n",
            "0065.1608FA.200.00.00.10       1.00      1.00      1.00         2\n",
            "0065.1613OA.202.05.69.10       1.00      1.00      1.00         1\n",
            "0065.SALCOM.200.00.10.10       1.00      1.00      1.00         6\n",
            "            5550007411.0       0.00      0.00      0.00         1\n",
            "            5550007412.0       0.00      0.00      0.00         1\n",
            "            5560004423.0       0.00      0.00      0.00         1\n",
            "              K0065_1001       1.00      1.00      1.00         1\n",
            "              K0065_1004       1.00      0.90      0.95        10\n",
            "              K0065_1006       0.50      1.00      0.67         1\n",
            "              K0065_1007       0.78      0.90      0.84        20\n",
            "              K0065_1008       0.92      0.96      0.94        48\n",
            "              K0065_1011       0.94      0.83      0.88        18\n",
            "              K0065_1013       1.00      0.93      0.97        30\n",
            "              K0065_1102       0.62      1.00      0.77         5\n",
            "              K0065_1108       1.00      0.90      0.95        20\n",
            "              K0065_1201       1.00      1.00      1.00         9\n",
            "              K0065_1315       1.00      1.00      1.00         1\n",
            "              K0065_1401       0.75      0.50      0.60         6\n",
            "              K0065_1402       0.86      1.00      0.93        32\n",
            "              K0065_1403       0.83      1.00      0.91         5\n",
            "              K0065_1404       0.80      0.89      0.84         9\n",
            "              K0065_1405       0.67      0.67      0.67         3\n",
            "              K0065_1406       0.57      0.50      0.53         8\n",
            "              K0065_1416       0.00      0.00      0.00         3\n",
            "              K0065_1420       0.71      0.83      0.77         6\n",
            "              K0065_1423       0.75      0.50      0.60         6\n",
            "\n",
            "                accuracy                           0.86       300\n",
            "               macro avg       0.70      0.72      0.70       300\n",
            "            weighted avg       0.86      0.86      0.85       300\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0301\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 7176 samples from 782 relevant classes. (N=1)\n",
            "Reduced to 6173 samples from 238 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.797\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "              0010100902       0.89      0.96      0.93        26\n",
            "              0010100950       0.00      0.00      0.00         1\n",
            "              0011100810       0.67      0.67      0.67         3\n",
            "              0012000163       1.00      1.00      1.00        10\n",
            "              0012000355       0.00      0.00      0.00         4\n",
            "              0012000580       1.00      1.00      1.00         1\n",
            "              0012000591       0.00      0.00      0.00         1\n",
            "              0012000603       1.00      1.00      1.00         2\n",
            "              0012006201       1.00      0.50      0.67         2\n",
            "              0022100133       0.50      1.00      0.67         1\n",
            "              0028000020       0.50      0.50      0.50         4\n",
            "              0028000050       0.38      0.38      0.38         8\n",
            "              0041003122       1.00      0.38      0.55         8\n",
            "              0041100723       1.00      1.00      1.00         1\n",
            "              0042004122       1.00      1.00      1.00         2\n",
            "              0042004411       1.00      1.00      1.00         4\n",
            "              0042004441       1.00      1.00      1.00         1\n",
            "              0042004511       1.00      1.00      1.00         1\n",
            "              0042100104       0.00      0.00      0.00         4\n",
            "              0042100111       0.00      0.00      0.00         2\n",
            "              0042100191       0.50      0.50      0.50         4\n",
            "              0042100221       0.22      0.67      0.33         3\n",
            "              0042100231       0.00      0.00      0.00         3\n",
            "              0042100561       0.00      0.00      0.00         1\n",
            "              0042100641       1.00      1.00      1.00         1\n",
            "              0042100811       0.00      0.00      0.00         1\n",
            "              0043005030       0.67      0.50      0.57         4\n",
            "              0043006030       1.00      0.67      0.80         3\n",
            "              0043100231       1.00      0.67      0.80         3\n",
            "              0046005030       0.00      0.00      0.00         1\n",
            "              0046090122       0.75      0.75      0.75         8\n",
            "              0046090511       1.00      0.50      0.67         2\n",
            "              0046100103       0.00      0.00      0.00         1\n",
            "              0046100104       0.86      0.86      0.86         7\n",
            "              0046100111       1.00      0.50      0.67         4\n",
            "              0046100191       1.00      0.60      0.75         5\n",
            "              0046100221       0.89      1.00      0.94         8\n",
            "              0046100231       0.67      0.50      0.57         4\n",
            "              0046100421       0.00      0.00      0.00         1\n",
            "              0046100621       0.82      0.90      0.86        10\n",
            "              0046100731       0.58      0.70      0.64        10\n",
            "              0046100821       1.00      1.00      1.00         1\n",
            "              0047001030       0.50      0.45      0.48        11\n",
            "              0047001122       1.00      0.90      0.95        10\n",
            "              0047001411       1.00      1.00      1.00         8\n",
            "              0047001441       1.00      1.00      1.00         1\n",
            "              0047001511       0.88      1.00      0.93         7\n",
            "              0047100103       1.00      0.50      0.67         2\n",
            "              0047100104       1.00      1.00      1.00         4\n",
            "              0047100111       0.40      0.50      0.44         8\n",
            "              0047100191       0.57      0.57      0.57         7\n",
            "              0047100221       1.00      0.50      0.67         4\n",
            "              0047100231       1.00      0.67      0.80         3\n",
            "              0047100421       0.50      0.50      0.50         2\n",
            "              0047100561       0.25      0.50      0.33         2\n",
            "              0047100621       0.38      0.60      0.46         5\n",
            "              0047100662       0.00      0.00      0.00         1\n",
            "              0047100723       0.00      0.00      0.00         2\n",
            "              0047100831       0.67      0.67      0.67         6\n",
            "              0049000221       0.00      0.00      0.00         2\n",
            "              0049001040       0.80      1.00      0.89         8\n",
            "              0052001040       1.00      0.75      0.86         4\n",
            "              0062020122       0.50      0.50      0.50         2\n",
            "              0062020441       0.67      0.67      0.67         3\n",
            "              0062020451       1.00      1.00      1.00        11\n",
            "              0062020541       0.00      0.00      0.00         1\n",
            "              0062023030       0.80      0.67      0.73         6\n",
            "              0062069031       1.00      1.00      1.00         3\n",
            "              0062100111       0.78      0.78      0.78         9\n",
            "              0062100191       0.83      0.83      0.83         6\n",
            "              0062100221       0.60      0.60      0.60         5\n",
            "              0062100231       0.78      0.90      0.84        31\n",
            "              0062100531       1.00      1.00      1.00         2\n",
            "              0062100723       0.00      0.00      0.00         1\n",
            "              0063041030       0.00      0.00      0.00         3\n",
            "              0063100102       0.00      0.00      0.00         1\n",
            "              0063100104       0.40      0.67      0.50         3\n",
            "              0063100221       1.00      1.00      1.00         1\n",
            "              0063100222       0.00      0.00      0.00         1\n",
            "              0063100231       0.75      0.60      0.67         5\n",
            "              0063100723       1.00      0.40      0.57         5\n",
            "              0064090511       0.00      0.00      0.00         1\n",
            "              0064100104       0.00      0.00      0.00         1\n",
            "              0064100111       0.25      0.50      0.33         2\n",
            "              0064100191       0.25      1.00      0.40         1\n",
            "              0064100221       0.67      0.67      0.67         6\n",
            "              0064100231       0.00      0.00      0.00         1\n",
            "              0064100621       0.50      0.67      0.57         3\n",
            "              0065000301       1.00      1.00      1.00         5\n",
            "              0065090122       1.00      1.00      1.00         7\n",
            "              0065090411       1.00      1.00      1.00         5\n",
            "              0065090511       1.00      1.00      1.00        15\n",
            "              0065100103       0.67      0.67      0.67         3\n",
            "              0065100104       0.44      0.67      0.53         6\n",
            "              0065100191       0.75      0.75      0.75         4\n",
            "              0065100221       0.00      0.00      0.00         3\n",
            "              0065100231       1.00      0.83      0.91         6\n",
            "              0065100232       1.00      0.75      0.86         4\n",
            "              0065100371       1.00      0.50      0.67         2\n",
            "              0065100561       1.00      0.50      0.67         4\n",
            "              0065100621       0.33      0.75      0.46         4\n",
            "              0065100641       0.00      0.00      0.00         2\n",
            "              0071090122       0.88      0.88      0.88         8\n",
            "              0071100104       0.60      1.00      0.75         3\n",
            "              0071100111       0.67      0.50      0.57         4\n",
            "              0071100221       0.67      0.50      0.57         4\n",
            "              0071100321       0.00      0.00      0.00         1\n",
            "              0071100641       0.00      0.00      0.00         2\n",
            "              0076090122       0.73      0.73      0.73        11\n",
            "              0076100221       0.71      0.62      0.67         8\n",
            "              0076100231       1.00      0.50      0.67         4\n",
            "              0076100321       0.00      0.00      0.00         1\n",
            "              0076100421       1.00      1.00      1.00         1\n",
            "              0076100821       1.00      1.00      1.00         1\n",
            "              0077003030       1.00      1.00      1.00         1\n",
            "              0077090122       0.92      0.79      0.85        14\n",
            "              0077090511       0.50      1.00      0.67         1\n",
            "              0077100111       0.00      0.00      0.00         2\n",
            "              0077100221       0.68      0.93      0.79        14\n",
            "              0077100231       0.50      0.33      0.40         3\n",
            "              0077100621       0.67      0.67      0.67         3\n",
            "              0077100641       0.50      0.50      0.50         2\n",
            "              0079090122       0.50      0.50      0.50         2\n",
            "              0079100231       0.00      0.00      0.00         1\n",
            "      0301.A62610.020.01       1.00      1.00      1.00         1\n",
            "         0301.A63310.045       1.00      1.00      1.00         2\n",
            "      0301.B61800.010.02       0.50      1.00      0.67         4\n",
            "      0301.G41000.999.55       0.58      1.00      0.74         7\n",
            "      0301.G42000.999.55       1.00      1.00      1.00         5\n",
            "      0301.G43001.999.55       0.83      1.00      0.91         5\n",
            "      0301.G43002.999.55       0.60      0.60      0.60         5\n",
            "      0301.G46000.120.55       0.62      0.62      0.62         8\n",
            "      0301.G46000.999.55       0.00      0.00      0.00         1\n",
            "      0301.G46000.999.57       1.00      1.00      1.00         1\n",
            "      0301.G47001.999.55       1.00      0.83      0.91         6\n",
            "      0301.G47001.999.56       1.00      1.00      1.00        10\n",
            "      0301.G62000.085.55       1.00      1.00      1.00         4\n",
            "      0301.G62000.999.62       0.75      0.75      0.75         4\n",
            "      0301.G63000.084.55       0.88      1.00      0.93         7\n",
            "      0301.G64000.098.56       0.86      1.00      0.92         6\n",
            "      0301.G65000.041.60       1.00      1.00      1.00         5\n",
            "      0301.G65000.999.62       0.75      1.00      0.86         3\n",
            "      0301.G71000.999.55       0.67      0.67      0.67         3\n",
            "      0301.G73700.999.55       1.00      1.00      1.00         3\n",
            "      0301.G76000.999.55       0.75      1.00      0.86         6\n",
            "      0301.G77000.999.55       0.62      0.83      0.71         6\n",
            "      0301.G99999.100.99       0.67      1.00      0.80         2\n",
            "   0301.O77904.004.14.02       1.00      1.00      1.00         1\n",
            "   0301.R41420.021.02.01       0.71      0.83      0.77         6\n",
            "   0301.R46610.010.06.31       1.00      1.00      1.00         1\n",
            "   0301.R46610.010.06.32       0.00      0.00      0.00         1\n",
            "      0301.R62620.011.06       0.00      0.00      0.00         1\n",
            "      0301.R65601.001.03       1.00      1.00      1.00         1\n",
            "      0301.R71602.003.01       0.00      0.00      0.00         1\n",
            "      0301.R71602.003.06       1.00      1.00      1.00         1\n",
            "   0301.T12050.021.42.14       1.00      1.00      1.00         4\n",
            "   0301.T12050.021.47.11       0.33      1.00      0.50         1\n",
            "   0301.T12050.024.46.05       1.00      1.00      1.00         5\n",
            "      0301.T46801.005.01       1.00      1.00      1.00         1\n",
            "         0301.W12014.011       0.67      1.00      0.80         2\n",
            "         0301.W12157.051       0.93      0.82      0.87        17\n",
            "         0301.W12157.052       0.94      0.96      0.95       127\n",
            "         0301.W12157.059       0.80      0.89      0.84         9\n",
            "         0301.W12158.008       1.00      1.00      1.00        18\n",
            "         0301.W12601.001       1.00      1.00      1.00         3\n",
            "      0301.W28331.011.50       1.00      1.00      1.00         2\n",
            "         0301.W42100.004       1.00      1.00      1.00        13\n",
            "         0301.W46304.004       1.00      1.00      1.00         3\n",
            "         0301.W46304.005       0.50      0.50      0.50         2\n",
            "         0301.W71301.003       1.00      1.00      1.00         2\n",
            "         0301.W76301.004       0.86      1.00      0.92         6\n",
            "         0301.W76301.005       0.50      0.50      0.50         2\n",
            "         0301.W77303.004       1.00      1.00      1.00         6\n",
            "         0301.W77303.005       1.00      1.00      1.00         2\n",
            "         0301.W77304.004       1.00      1.00      1.00         5\n",
            "         0301.W77602.001       1.00      1.00      1.00         2\n",
            "         0301.W77602.004       1.00      0.50      0.67         2\n",
            "         0301.X12709.001       1.00      1.00      1.00         4\n",
            "         0301.Y12003.100       1.00      1.00      1.00        35\n",
            "      0301.Y12703.020.09       1.00      1.00      1.00         1\n",
            "         0301.Y12707.011       1.00      0.50      0.67         2\n",
            "         0301.Y12710.002       1.00      1.00      1.00         3\n",
            "         0301.Y12800.102       1.00      1.00      1.00         2\n",
            "      0301.Y28313.001.02       0.00      0.00      0.00         1\n",
            "      0301.Y28314.041.01       1.00      1.00      1.00         1\n",
            "      0301.Y28314.041.10       1.00      1.00      1.00         3\n",
            "      0301.Y28331.012.01       1.00      1.00      1.00         2\n",
            "      0301.Y28341.005.51       0.50      1.00      0.67         1\n",
            "   0301.Y28570.042.01.02       1.00      1.00      1.00         2\n",
            "0301.Y28575.042.01.01.02       0.50      1.00      0.67         1\n",
            "0301.Y28576.042.01.01.02       1.00      0.33      0.50         3\n",
            "0301.Y28578.042.01.01.02       1.00      1.00      1.00         4\n",
            "0301.Y28579.022.01.01.02       1.00      1.00      1.00         2\n",
            "0301.Y28579.042.01.01.02       0.83      0.56      0.67        34\n",
            "0301.Y28580.042.01.01.02       0.64      0.78      0.70         9\n",
            "0301.Y28582.042.01.01.02       0.87      0.95      0.91        97\n",
            "      0301.Y29006.001.05       1.00      1.00      1.00         3\n",
            "      0301.Y29009.001.05       0.50      1.00      0.67         1\n",
            "      0301.Y29009.001.06       0.00      0.00      0.00         1\n",
            "      0301.Y29907.001.05       1.00      1.00      1.00         3\n",
            "         0301.Y43801.002       0.00      0.00      0.00         1\n",
            "      0301.Y47501.001.01       1.00      1.00      1.00         1\n",
            "         0301.Y47813.001       0.50      1.00      0.67         1\n",
            "      0301.Y62559.001.01       1.00      1.00      1.00         1\n",
            "      0301.Y62559.003.01       1.00      1.00      1.00         1\n",
            "      0301.Y62559.004.01       1.00      1.00      1.00         1\n",
            "   0301.Y63501.024.03.30       1.00      0.67      0.80         3\n",
            "   0301.Y63501.028.40.03       0.00      0.00      0.00         1\n",
            "         0301.Y65302.002       1.00      1.00      1.00         1\n",
            "         0301.Y65707.001       1.00      1.00      1.00         2\n",
            "         0301.Y73700.010       0.67      0.50      0.57         4\n",
            "      0301.Y73700.020.05       0.00      0.00      0.00         1\n",
            "      0301.Y73700.040.03       0.67      1.00      0.80         2\n",
            "         0301.Y74800.010       0.33      1.00      0.50         1\n",
            "              1030110500       0.50      0.50      0.50         4\n",
            "              1030120210       0.25      0.33      0.29         3\n",
            "              1030120220       0.88      1.00      0.93         7\n",
            "              1030120400       1.00      1.00      1.00         4\n",
            "              1030131000       1.00      1.00      1.00         6\n",
            "              1030131100       0.91      0.83      0.87        12\n",
            "              1030140200       1.00      1.00      1.00         1\n",
            "              2030112200       0.71      1.00      0.83         5\n",
            "              2030116000       0.75      1.00      0.86         3\n",
            "              2030121100       0.88      1.00      0.93         7\n",
            "              2030121200       1.00      1.00      1.00         2\n",
            "              2030122000       1.00      1.00      1.00         2\n",
            "              2030122100       0.44      0.44      0.44         9\n",
            "              2030122300       0.00      0.00      0.00         2\n",
            "              2030122400       0.00      0.00      0.00         2\n",
            "              2030122500       0.25      0.50      0.33         2\n",
            "              2030123000       0.33      0.50      0.40         2\n",
            "              2030123400       0.67      0.50      0.57         4\n",
            "              2030127000       0.00      0.00      0.00         3\n",
            "              2030128100       0.90      0.64      0.75        14\n",
            "              2030134200       1.00      1.00      1.00         1\n",
            "              2030138600       1.00      1.00      1.00         1\n",
            "              2030143100       0.00      0.00      0.00         2\n",
            "              2030144100       0.75      0.86      0.80         7\n",
            "\n",
            "                accuracy                           0.80      1235\n",
            "               macro avg       0.67      0.68      0.66      1235\n",
            "            weighted avg       0.79      0.80      0.79      1235\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0303\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 8809 samples from 1207 relevant classes. (N=1)\n",
            "Reduced to 7314 samples from 350 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.768\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0110110000       0.71      0.80      0.75        15\n",
            "        0110510000       0.58      0.88      0.70         8\n",
            "        0111510000       1.00      0.75      0.86         4\n",
            "        0112010000       0.85      0.85      0.85        13\n",
            "        0112510000       0.79      0.79      0.79        14\n",
            "        0114010000       1.00      0.50      0.67         2\n",
            "        0121014000       0.71      0.83      0.77         6\n",
            "        0121515000       0.83      0.83      0.83         6\n",
            "        0121520500       1.00      1.00      1.00         1\n",
            "        0122010000       0.83      0.71      0.77        21\n",
            "        0122015000       1.00      1.00      1.00         1\n",
            "        0122015200       0.00      0.00      0.00         2\n",
            "        0122015300       0.82      0.75      0.78        12\n",
            "        0122015330       0.00      0.00      0.00         3\n",
            "        0122015350       0.76      0.87      0.81        15\n",
            "        0122015400       1.00      1.00      1.00         1\n",
            "        0122015700       0.00      0.00      0.00         1\n",
            "        0122020000       0.44      0.44      0.44         9\n",
            "        0122020500       0.60      0.60      0.60         5\n",
            "        0122021500       0.75      0.75      0.75         4\n",
            "        0122022000       0.92      0.86      0.89        14\n",
            "        0122022010       0.00      0.00      0.00         2\n",
            "        0122022020       0.00      0.00      0.00         1\n",
            "        0122022030       1.00      0.50      0.67         2\n",
            "        0122022050       0.86      0.95      0.90        39\n",
            "        0122023000       0.00      0.00      0.00         2\n",
            "        0122510000       0.73      0.80      0.76        10\n",
            "        0122515000       0.83      0.71      0.77         7\n",
            "        0122515100       0.25      0.33      0.29         3\n",
            "        0122520000       0.60      0.81      0.69        32\n",
            "        0122520500       0.67      0.67      0.67         3\n",
            "        0122522000       0.33      0.50      0.40         2\n",
            "        0124010000       0.67      0.57      0.62         7\n",
            "        0124010500       0.60      0.60      0.60         5\n",
            "        0124015200       0.73      0.89      0.80         9\n",
            "        0124015400       0.82      0.90      0.86        10\n",
            "        0124015500       0.00      0.00      0.00         1\n",
            "        0124015600       0.62      0.89      0.73         9\n",
            "        0124015700       0.00      0.00      0.00         1\n",
            "        0124020000       0.54      0.87      0.67        46\n",
            "        0124021000       0.56      0.79      0.65        38\n",
            "        0124510000       0.33      0.56      0.42         9\n",
            "        0124515000       0.00      0.00      0.00         1\n",
            "        0199999902       0.14      0.33      0.20         3\n",
            "        0199999903       1.00      0.75      0.86         4\n",
            "        0199999952       1.00      1.00      1.00         7\n",
            "0303.WE1210.130.09       0.71      1.00      0.83         5\n",
            "0303.WE1220.152.01       0.80      0.80      0.80         5\n",
            "0303.Y68910.050.01       0.67      0.67      0.67         3\n",
            "0303.YM1120.300.01       0.80      0.80      0.80         5\n",
            "0303.YM1120.300.17       1.00      1.00      1.00         1\n",
            "0303.YS1101.100.04       1.00      1.00      1.00         3\n",
            "0303.YS1101.100.09       0.67      1.00      0.80         2\n",
            "        80034611.0       1.00      0.67      0.80         3\n",
            "        80051961.0       0.33      0.50      0.40         2\n",
            "        80052091.0       1.00      0.50      0.67         4\n",
            "        80052734.0       0.50      0.33      0.40         3\n",
            "        80053035.0       0.91      1.00      0.95        10\n",
            "        80053039.0       0.00      0.00      0.00         1\n",
            "        80053412.0       0.00      0.00      0.00         1\n",
            "        80053637.0       1.00      1.00      1.00         1\n",
            "        80054018.0       1.00      0.50      0.67         2\n",
            "        80055055.0       0.00      0.00      0.00         1\n",
            "        80055535.0       0.85      0.92      0.88        12\n",
            "        80055563.0       0.89      0.97      0.93        35\n",
            "        80055614.0       1.00      1.00      1.00         1\n",
            "        80055655.0       1.00      1.00      1.00         5\n",
            "        80055692.0       1.00      1.00      1.00         2\n",
            "        80055694.0       0.80      1.00      0.89        12\n",
            "        80055752.0       1.00      0.50      0.67         2\n",
            "        80055874.0       0.00      0.00      0.00         2\n",
            "        80055926.0       1.00      1.00      1.00         2\n",
            "        80056042.0       1.00      1.00      1.00         7\n",
            "        80056043.0       1.00      0.50      0.67         2\n",
            "        80056046.0       1.00      0.50      0.67         2\n",
            "        80056083.0       1.00      1.00      1.00         1\n",
            "        80056198.0       0.73      0.79      0.76        14\n",
            "        80056201.0       1.00      0.50      0.67         2\n",
            "        80056202.0       0.00      0.00      0.00         3\n",
            "        80056235.0       0.50      0.67      0.57         6\n",
            "        80056344.0       0.00      0.00      0.00         1\n",
            "        80056385.0       0.00      0.00      0.00         1\n",
            "        80056420.0       1.00      0.50      0.67         2\n",
            "        80056422.0       1.00      1.00      1.00         5\n",
            "        80056438.0       1.00      1.00      1.00         2\n",
            "        80056497.0       0.00      0.00      0.00         1\n",
            "        80056500.0       0.00      0.00      0.00         2\n",
            "        80056755.0       0.00      0.00      0.00         1\n",
            "        80056845.0       0.67      0.67      0.67         3\n",
            "        80056904.0       0.50      0.50      0.50         2\n",
            "        80057093.0       1.00      1.00      1.00         1\n",
            "        80057096.0       0.00      0.00      0.00         1\n",
            "        80057140.0       0.67      1.00      0.80         2\n",
            "        80057141.0       1.00      0.50      0.67         2\n",
            "        80057265.0       1.00      1.00      1.00         1\n",
            "        80057283.0       0.00      0.00      0.00         1\n",
            "        80057340.0       1.00      1.00      1.00         2\n",
            "        80057349.0       1.00      0.83      0.91         6\n",
            "        80057360.0       1.00      0.33      0.50         3\n",
            "        80057374.0       0.00      0.00      0.00         1\n",
            "        80057415.0       0.33      1.00      0.50         1\n",
            "        80057434.0       1.00      1.00      1.00         3\n",
            "        80057468.0       1.00      1.00      1.00         1\n",
            "        80057501.0       0.00      0.00      0.00         1\n",
            "        80057505.0       0.00      0.00      0.00         1\n",
            "        80057517.0       0.80      0.80      0.80         5\n",
            "        80057526.0       1.00      0.67      0.80         3\n",
            "        80057648.0       0.67      0.89      0.76         9\n",
            "        80057678.0       0.50      1.00      0.67         4\n",
            "        80057725.0       1.00      1.00      1.00         4\n",
            "        80057727.0       1.00      0.67      0.80         3\n",
            "        80057729.0       1.00      1.00      1.00         2\n",
            "        80057733.0       0.95      1.00      0.97        18\n",
            "        80057734.0       0.85      1.00      0.92        22\n",
            "        80057735.0       1.00      1.00      1.00         1\n",
            "        80057758.0       0.92      0.92      0.92        13\n",
            "        80057792.0       0.00      0.00      0.00         2\n",
            "        80057806.0       0.67      0.40      0.50         5\n",
            "        80057823.0       0.33      1.00      0.50         1\n",
            "        80057826.0       0.89      0.89      0.89        35\n",
            "        80057835.0       0.80      0.80      0.80         5\n",
            "        80057860.0       0.00      0.00      0.00         1\n",
            "        80057901.0       1.00      0.33      0.50         3\n",
            "        80057921.0       0.00      0.00      0.00         1\n",
            "        80057923.0       0.00      0.00      0.00         1\n",
            "        80057941.0       0.75      0.75      0.75         4\n",
            "        80057959.0       0.00      0.00      0.00         1\n",
            "        80057966.0       0.75      0.75      0.75         8\n",
            "        80057967.0       1.00      1.00      1.00         2\n",
            "        80057968.0       0.00      0.00      0.00         1\n",
            "        80057970.0       0.00      0.00      0.00         2\n",
            "        80057976.0       0.96      1.00      0.98        24\n",
            "        80057977.0       0.00      0.00      0.00         2\n",
            "        80057995.0       0.91      0.91      0.91        11\n",
            "        80058002.0       0.92      0.92      0.92        13\n",
            "        80058003.0       0.88      0.82      0.85        17\n",
            "        80058004.0       0.00      0.00      0.00         2\n",
            "        80058005.0       0.82      1.00      0.90         9\n",
            "        80058014.0       0.00      0.00      0.00         2\n",
            "        80058015.0       0.00      0.00      0.00         1\n",
            "        80058019.0       0.67      0.67      0.67         3\n",
            "        80058039.0       1.00      0.50      0.67         2\n",
            "        80058040.0       1.00      0.50      0.67         2\n",
            "        80058042.0       0.80      0.73      0.76        11\n",
            "        80058050.0       0.80      0.80      0.80         5\n",
            "        80058053.0       1.00      1.00      1.00         1\n",
            "        80058073.0       0.00      0.00      0.00         1\n",
            "        80058075.0       0.00      0.00      0.00         1\n",
            "        80058082.0       1.00      1.00      1.00         2\n",
            "        80058085.0       0.00      0.00      0.00         1\n",
            "        80058099.0       1.00      1.00      1.00         3\n",
            "        80058101.0       1.00      0.50      0.67         2\n",
            "        80058133.0       0.00      0.00      0.00         1\n",
            "        80058165.0       0.67      0.67      0.67         3\n",
            "        80058166.0       1.00      0.80      0.89         5\n",
            "        80058187.0       1.00      1.00      1.00         4\n",
            "        80058202.0       1.00      1.00      1.00         2\n",
            "        80058203.0       0.00      0.00      0.00         1\n",
            "        80058211.0       1.00      0.40      0.57         5\n",
            "        80058218.0       1.00      1.00      1.00         2\n",
            "        80058224.0       1.00      0.79      0.88        14\n",
            "        80058225.0       0.00      0.00      0.00         1\n",
            "        80058238.0       0.75      1.00      0.86         3\n",
            "        80058267.0       1.00      1.00      1.00         1\n",
            "        80058275.0       0.00      0.00      0.00         1\n",
            "        80058288.0       0.00      0.00      0.00         1\n",
            "        80058290.0       0.50      0.50      0.50         2\n",
            "        80058292.0       1.00      1.00      1.00         1\n",
            "        80058299.0       0.57      0.89      0.70         9\n",
            "        80058311.0       1.00      1.00      1.00         1\n",
            "        80058332.0       1.00      1.00      1.00         3\n",
            "        80058355.0       1.00      0.67      0.80         3\n",
            "        80058356.0       1.00      1.00      1.00         3\n",
            "        80058358.0       0.00      0.00      0.00         2\n",
            "        80058366.0       0.67      0.57      0.62         7\n",
            "        80058369.0       0.33      0.50      0.40         2\n",
            "        80058372.0       0.57      0.80      0.67         5\n",
            "        80058377.0       1.00      0.89      0.94        18\n",
            "        80058378.0       1.00      0.75      0.86         4\n",
            "        80058379.0       1.00      1.00      1.00         1\n",
            "        80058395.0       0.50      0.50      0.50         2\n",
            "        80058398.0       0.00      0.00      0.00         1\n",
            "        80058400.0       1.00      0.50      0.67         2\n",
            "        80058402.0       1.00      1.00      1.00         1\n",
            "        80058403.0       0.60      0.60      0.60         5\n",
            "        80058447.0       1.00      1.00      1.00         5\n",
            "        80058472.0       0.00      0.00      0.00         1\n",
            "        80058475.0       0.00      0.00      0.00         1\n",
            "        80058479.0       1.00      1.00      1.00         3\n",
            "        80058494.0       0.50      0.25      0.33         4\n",
            "        80058502.0       1.00      0.50      0.67         4\n",
            "        80058510.0       0.00      0.00      0.00         1\n",
            "        80058518.0       0.00      0.00      0.00         1\n",
            "        80058520.0       0.50      1.00      0.67         1\n",
            "        80058524.0       1.00      1.00      1.00         2\n",
            "        80058525.0       1.00      1.00      1.00         4\n",
            "        80058529.0       0.00      0.00      0.00         1\n",
            "        80058534.0       0.00      0.00      0.00         1\n",
            "        80058540.0       0.80      1.00      0.89         4\n",
            "        80058551.0       0.00      0.00      0.00         1\n",
            "        80058554.0       0.57      0.67      0.62         6\n",
            "        80058572.0       1.00      0.50      0.67         2\n",
            "        80058573.0       0.00      0.00      0.00         2\n",
            "        80058574.0       0.50      1.00      0.67         1\n",
            "        80058578.0       1.00      0.86      0.92         7\n",
            "        80058586.0       0.00      0.00      0.00         1\n",
            "        80058607.0       0.33      1.00      0.50         1\n",
            "        80058618.0       0.00      0.00      0.00         1\n",
            "        80058624.0       0.00      0.00      0.00         1\n",
            "        80058630.0       0.75      1.00      0.86         3\n",
            "        80058631.0       1.00      1.00      1.00         1\n",
            "        80058632.0       0.00      0.00      0.00         1\n",
            "        80058641.0       0.67      1.00      0.80         2\n",
            "        80058645.0       0.00      0.00      0.00         3\n",
            "        80058663.0       1.00      1.00      1.00         2\n",
            "        80058670.0       1.00      1.00      1.00         2\n",
            "        80058672.0       1.00      0.50      0.67         2\n",
            "        80058694.0       0.00      0.00      0.00         1\n",
            "        80058704.0       0.00      0.00      0.00         2\n",
            "        80058708.0       1.00      1.00      1.00         2\n",
            "        80058739.0       0.00      0.00      0.00         1\n",
            "        80058756.0       1.00      0.50      0.67         2\n",
            "        80058762.0       1.00      0.50      0.67         2\n",
            "        80058764.0       1.00      0.50      0.67         4\n",
            "        80058771.0       0.00      0.00      0.00         1\n",
            "        80058772.0       0.33      1.00      0.50         1\n",
            "        80058777.0       1.00      0.75      0.86         4\n",
            "        80058801.0       0.00      0.00      0.00         1\n",
            "        80058802.0       1.00      0.92      0.96        12\n",
            "        80058804.0       0.00      0.00      0.00         2\n",
            "        80058809.0       1.00      1.00      1.00         8\n",
            "        80058823.0       1.00      0.50      0.67         2\n",
            "        80058829.0       0.50      0.50      0.50         2\n",
            "        80058840.0       0.00      0.00      0.00         1\n",
            "        80058847.0       0.85      0.92      0.88        12\n",
            "        80058855.0       0.00      0.00      0.00         1\n",
            "        80058856.0       0.86      0.92      0.89        13\n",
            "        80058875.0       0.00      0.00      0.00         1\n",
            "        80058879.0       1.00      0.50      0.67         4\n",
            "        80058880.0       1.00      0.75      0.86         4\n",
            "        80058884.0       0.33      0.25      0.29         4\n",
            "        80058886.0       0.00      0.00      0.00         1\n",
            "        80058897.0       0.33      1.00      0.50         1\n",
            "        80058898.0       1.00      0.75      0.86         4\n",
            "        80058903.0       1.00      1.00      1.00         1\n",
            "        80058907.0       1.00      1.00      1.00         3\n",
            "        80058909.0       1.00      0.50      0.67         2\n",
            "        80058914.0       1.00      1.00      1.00         3\n",
            "        80058915.0       0.00      0.00      0.00         1\n",
            "        80058928.0       1.00      0.67      0.80         3\n",
            "        80058959.0       1.00      1.00      1.00         1\n",
            "        80058961.0       0.00      0.00      0.00         3\n",
            "        80058963.0       0.00      0.00      0.00         1\n",
            "        80058980.0       1.00      1.00      1.00         1\n",
            "        80058983.0       0.67      1.00      0.80         2\n",
            "        80058997.0       1.00      1.00      1.00         1\n",
            "        80059007.0       1.00      1.00      1.00         1\n",
            "        80059030.0       1.00      1.00      1.00         2\n",
            "        80059036.0       1.00      0.50      0.67         2\n",
            "        80059040.0       0.00      0.00      0.00         1\n",
            "        80059054.0       0.00      0.00      0.00         1\n",
            "        80059055.0       1.00      1.00      1.00         1\n",
            "        80059058.0       1.00      1.00      1.00         1\n",
            "        80059063.0       0.67      0.80      0.73         5\n",
            "        80059082.0       0.67      1.00      0.80         2\n",
            "        80059111.0       1.00      1.00      1.00         1\n",
            "        80059175.0       0.00      0.00      0.00         1\n",
            "        80059183.0       0.00      0.00      0.00         1\n",
            "        80059195.0       1.00      0.92      0.96        13\n",
            "        80059196.0       0.80      1.00      0.89         8\n",
            "        80059198.0       1.00      1.00      1.00         1\n",
            "        80059238.0       1.00      0.67      0.80         3\n",
            "        80059240.0       1.00      0.50      0.67         2\n",
            "        80059244.0       1.00      0.50      0.67         2\n",
            "        80059254.0       0.50      1.00      0.67         1\n",
            "        80059260.0       1.00      1.00      1.00         1\n",
            "        80059262.0       1.00      1.00      1.00         1\n",
            "        80059306.0       0.80      0.94      0.86        17\n",
            "        80059307.0       1.00      1.00      1.00         2\n",
            "        80059327.0       1.00      1.00      1.00         2\n",
            "        80059354.0       1.00      1.00      1.00         3\n",
            "        80059360.0       1.00      1.00      1.00         8\n",
            "        80059368.0       0.00      0.00      0.00         2\n",
            "        80059391.0       0.75      1.00      0.86         3\n",
            "        80059480.0       1.00      0.75      0.86         4\n",
            "        80059498.0       1.00      1.00      1.00         3\n",
            "        80059557.0       0.91      1.00      0.95        10\n",
            "        80059598.0       0.00      0.00      0.00         1\n",
            "        80059612.0       1.00      0.50      0.67         2\n",
            "        80059621.0       1.00      0.50      0.67         2\n",
            "        80059673.0       0.00      0.00      0.00         1\n",
            "        80059691.0       0.33      0.50      0.40         2\n",
            "        80059696.0       1.00      1.00      1.00         1\n",
            "        80059736.0       0.50      0.33      0.40         3\n",
            "        80059757.0       1.00      1.00      1.00         1\n",
            "        80059770.0       1.00      1.00      1.00         1\n",
            "        80059788.0       1.00      1.00      1.00         2\n",
            "        80059815.0       1.00      0.33      0.50         3\n",
            "        80059835.0       0.86      1.00      0.92         6\n",
            "        80059838.0       0.00      0.00      0.00         1\n",
            "        80059852.0       1.00      1.00      1.00         1\n",
            "        80059880.0       1.00      1.00      1.00         1\n",
            "        80059909.0       1.00      1.00      1.00         2\n",
            "        80059945.0       1.00      1.00      1.00         4\n",
            "        80059950.0       0.33      1.00      0.50         1\n",
            "        80059966.0       0.00      0.00      0.00         2\n",
            "        80059973.0       0.50      0.50      0.50         2\n",
            "        80059975.0       1.00      1.00      1.00         2\n",
            "        80059993.0       1.00      1.00      1.00         1\n",
            "        80060002.0       0.00      0.00      0.00         1\n",
            "        80060034.0       1.00      1.00      1.00         2\n",
            "        80060037.0       0.00      0.00      0.00         1\n",
            "        80060075.0       1.00      1.00      1.00         2\n",
            "        80060076.0       1.00      1.00      1.00         1\n",
            "        80060103.0       1.00      1.00      1.00         1\n",
            "        80060142.0       1.00      1.00      1.00         2\n",
            "        80060187.0       0.00      0.00      0.00         1\n",
            "        80060192.0       1.00      1.00      1.00         1\n",
            "        80060230.0       1.00      0.60      0.75         5\n",
            "        80060236.0       1.00      0.50      0.67         2\n",
            "        80060260.0       0.25      0.50      0.33         2\n",
            "        80060267.0       1.00      0.89      0.94         9\n",
            "        80060333.0       1.00      0.50      0.67         2\n",
            "        80060426.0       0.00      0.00      0.00         1\n",
            "        80060439.0       1.00      1.00      1.00         1\n",
            "        80060479.0       0.33      0.50      0.40         2\n",
            "        80060480.0       1.00      1.00      1.00         1\n",
            "        80060513.0       0.00      0.00      0.00         1\n",
            "        80060524.0       1.00      1.00      1.00         1\n",
            "        80060576.0       1.00      1.00      1.00         3\n",
            "        80060647.0       0.67      1.00      0.80         2\n",
            "        80060704.0       1.00      0.50      0.67         2\n",
            "        80060890.0       1.00      1.00      1.00         1\n",
            "        80060905.0       0.00      0.00      0.00         1\n",
            "        80061048.0       1.00      1.00      1.00         1\n",
            "        80801909.0       1.00      0.80      0.89         5\n",
            "        80801934.0       0.00      0.00      0.00         2\n",
            "        80801969.0       0.86      0.86      0.86         7\n",
            "        80801975.0       1.00      0.88      0.93         8\n",
            "        80801982.0       1.00      0.50      0.67         2\n",
            "        80802001.0       0.00      0.00      0.00         1\n",
            "        80802006.0       0.88      0.94      0.91        16\n",
            "        80802013.0       1.00      0.67      0.80         6\n",
            "        80802037.0       1.00      1.00      1.00         1\n",
            "        80802080.0       0.75      1.00      0.86         3\n",
            "        80802089.0       0.78      0.70      0.74        10\n",
            "        80802101.0       0.00      0.00      0.00         1\n",
            "        80802114.0       1.00      1.00      1.00         3\n",
            "        80802120.0       1.00      1.00      1.00         2\n",
            "        80802146.0       1.00      1.00      1.00         7\n",
            "\n",
            "          accuracy                           0.77      1463\n",
            "         macro avg       0.64      0.61      0.61      1463\n",
            "      weighted avg       0.76      0.77      0.75      1463\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0330\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1605 samples from 107 relevant classes. (N=1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 1501 samples from 61 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.821\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   0330.Y12100.004.02       1.00      1.00      1.00         1\n",
            "   0330.Y12100.004.03       0.80      1.00      0.89         4\n",
            "   0330.Y12100.004.05       1.00      1.00      1.00         1\n",
            "   0330.Y12100.004.08       1.00      1.00      1.00         3\n",
            "   0330.Y12100.004.12       1.00      0.80      0.89         5\n",
            "   0330.Y12100.004.16       1.00      0.50      0.67         2\n",
            "   0330.Y12100.004.19       0.00      0.00      0.00         1\n",
            "   0330.Y12100.004.20       0.50      0.50      0.50         2\n",
            "      0330.Y12100.005       0.88      0.58      0.70        12\n",
            "   0330.Y12100.008.01       0.50      1.00      0.67         1\n",
            "   0330.Y12210.009.01       1.00      1.00      1.00         1\n",
            "   0330.Y12210.009.02       1.00      1.00      1.00         2\n",
            "      0330.Y12210.111       1.00      0.33      0.50         3\n",
            "0330.Y12807.001.02.02       1.00      1.00      1.00         4\n",
            "           1033010000       0.83      0.83      0.83         6\n",
            "           1033010094       1.00      0.50      0.67         2\n",
            "           1033010095       0.00      0.00      0.00         1\n",
            "           1033010100       0.33      1.00      0.50         1\n",
            "           1033010200       0.76      0.87      0.81        15\n",
            "           1033010201       0.00      0.00      0.00         1\n",
            "           1033010500       0.67      1.00      0.80         8\n",
            "           1033010600       0.00      0.00      0.00         1\n",
            "           1033010690       1.00      1.00      1.00         1\n",
            "           1033010693       0.78      0.70      0.74        10\n",
            "           1033010694       0.91      0.93      0.92        44\n",
            "           1033010695       0.91      0.91      0.91        11\n",
            "           1033016000       1.00      1.00      1.00         2\n",
            "           1033016300       1.00      1.00      1.00         4\n",
            "           1033016700       1.00      1.00      1.00         3\n",
            "           1033017000       1.00      1.00      1.00         2\n",
            "           1033017100       1.00      0.67      0.80         3\n",
            "           1033017110       1.00      0.50      0.67         2\n",
            "           1033017120       0.50      1.00      0.67         1\n",
            "           1033017200       0.40      0.50      0.44         4\n",
            "           1033017210       0.50      1.00      0.67         1\n",
            "           1033017300       1.00      1.00      1.00         2\n",
            "           1033017400       0.75      1.00      0.86         3\n",
            "           1033017500       0.00      0.00      0.00         1\n",
            "           1033017510       1.00      0.50      0.67         4\n",
            "           1033017520       1.00      1.00      1.00         8\n",
            "           1033020100       1.00      1.00      1.00         3\n",
            "           1033020830       1.00      1.00      1.00         1\n",
            "           1033022400       0.67      1.00      0.80         2\n",
            "           1033030091       1.00      1.00      1.00        14\n",
            "           1033030200       1.00      1.00      1.00         4\n",
            "           1033030201       0.83      0.83      0.83        12\n",
            "           1033030202       0.85      0.92      0.88        25\n",
            "           1033030210       0.67      0.67      0.67         3\n",
            "           1033030220       0.33      0.67      0.44         3\n",
            "           1033030230       1.00      0.57      0.73         7\n",
            "           1033030240       1.00      0.40      0.57         5\n",
            "           1033030300       0.00      0.00      0.00         3\n",
            "           1033030400       0.25      0.33      0.29         3\n",
            "           1033030401       0.00      0.00      0.00         1\n",
            "           1033030410       0.00      0.00      0.00         2\n",
            "           1033030830       0.50      1.00      0.67         1\n",
            "           1033038100       0.60      0.75      0.67         4\n",
            "           1033038200       1.00      0.88      0.93         8\n",
            "           1033038300       0.80      1.00      0.89         8\n",
            "           1033038400       0.50      1.00      0.67         1\n",
            "           1033050200       0.80      1.00      0.89         8\n",
            "\n",
            "             accuracy                           0.82       301\n",
            "            macro avg       0.72      0.73      0.70       301\n",
            "         weighted avg       0.83      0.82      0.81       301\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0362\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 563 samples from 36 relevant classes. (N=1)\n",
            "Reduced to 523 samples from 17 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.838\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0015030020       1.00      0.67      0.80         3\n",
            "        0015030030       0.00      0.00      0.00         3\n",
            "        0015030050       0.73      0.90      0.81        30\n",
            "        0015030065       0.50      0.25      0.33         4\n",
            "        0015030070       1.00      1.00      1.00         1\n",
            "        0015076001       1.00      0.50      0.67         4\n",
            "        0015097101       0.00      0.00      0.00         2\n",
            "        0015098103       0.82      0.90      0.86        10\n",
            "0362.G62000.088.55       0.60      0.75      0.67         4\n",
            "0362.G62000.092.58       1.00      1.00      1.00         2\n",
            "0362.G62000.093.58       1.00      1.00      1.00         1\n",
            "0362.G62000.095.01       1.00      1.00      1.00         1\n",
            "0362.G62000.096.58       1.00      1.00      1.00        28\n",
            "0362.G62000.999.30       1.00      0.80      0.89         5\n",
            "0362.G63000.082.55       1.00      1.00      1.00         3\n",
            "0362.T15109.069.02       1.00      1.00      1.00         2\n",
            "0362.T15109.070.02       0.50      1.00      0.67         2\n",
            "\n",
            "          accuracy                           0.84       105\n",
            "         macro avg       0.77      0.75      0.75       105\n",
            "      weighted avg       0.81      0.84      0.82       105\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0377\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 173 samples from 23 relevant classes. (N=1)\n",
            "Reduced to 153 samples from 10 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.710\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0077025030       1.00      1.00      1.00         1\n",
            "           0077030010       1.00      1.00      1.00         3\n",
            "           0077080122       0.50      0.50      0.50         6\n",
            "           0077100231       1.00      1.00      1.00         2\n",
            "   0377.G77000.072.65       1.00      0.40      0.57         5\n",
            "   0377.G77000.073.59       0.60      1.00      0.75         3\n",
            "   0377.G77000.096.56       0.50      1.00      0.67         3\n",
            "   0377.G77000.999.55       0.67      0.40      0.50         5\n",
            "0377.O77952.005.14.02       1.00      1.00      1.00         1\n",
            "      0377.W77305.005       1.00      1.00      1.00         2\n",
            "\n",
            "             accuracy                           0.71        31\n",
            "            macro avg       0.83      0.83      0.80        31\n",
            "         weighted avg       0.76      0.71      0.70        31\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0601\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 2245 samples from 276 relevant classes. (N=1)\n",
            "Reduced to 1913 samples from 62 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.802\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0090022015       0.86      1.00      0.92         6\n",
            "        0090024002       1.00      1.00      1.00         1\n",
            "        0090024009       1.00      1.00      1.00         2\n",
            "        0090026700       0.00      0.00      0.00         2\n",
            "        0090026810       0.00      0.00      0.00         3\n",
            "        0090030010       0.00      0.00      0.00         1\n",
            "        0090030020       0.46      0.67      0.55         9\n",
            "        0090032010       0.84      1.00      0.91        21\n",
            "        0090032042       0.83      0.77      0.80        13\n",
            "        0090034032       0.73      0.73      0.73        11\n",
            "        0090034035       0.81      0.76      0.79        17\n",
            "        0090034038       1.00      0.50      0.67         2\n",
            "        0090034041       0.00      0.00      0.00         3\n",
            "        0090036150       0.67      0.83      0.74        12\n",
            "        0090036190       1.00      1.00      1.00         1\n",
            "        0090043002       0.94      1.00      0.97        15\n",
            "        0090082001       1.00      1.00      1.00         5\n",
            "        0090095270       0.86      0.86      0.86        21\n",
            "        0090095275       0.88      0.88      0.88         8\n",
            "        0090096101       1.00      1.00      1.00         2\n",
            "        0090096307       1.00      1.00      1.00         1\n",
            "        0090096312       1.00      1.00      1.00         1\n",
            "        0090096702       1.00      0.71      0.83         7\n",
            "        0090098310       0.50      0.50      0.50         2\n",
            "        0090098341       0.50      0.50      0.50         2\n",
            "        0090098376       0.00      0.00      0.00         1\n",
            "        0090098377       1.00      0.50      0.67         2\n",
            "        0090098378       0.50      1.00      0.67         1\n",
            "        0090098379       0.00      0.00      0.00         1\n",
            "        0090098385       1.00      1.00      1.00         5\n",
            "        0090098397       1.00      0.80      0.89         5\n",
            "        0090098398       1.00      1.00      1.00        10\n",
            "        0090098399       1.00      1.00      1.00         5\n",
            "        0090098411       0.75      0.67      0.71         9\n",
            "        0090098414       0.93      0.88      0.90        16\n",
            "        0090098417       0.92      1.00      0.96        12\n",
            "        0090098424       0.88      0.93      0.90        15\n",
            "        0090098429       1.00      0.94      0.97        18\n",
            "        0090098434       0.56      0.64      0.60        14\n",
            "        0090098437       0.57      0.71      0.63        17\n",
            "        0090098480       1.00      0.33      0.50         3\n",
            "        0090098483       1.00      1.00      1.00         4\n",
            "        0090098487       0.50      0.17      0.25         6\n",
            "        0090098489       1.00      1.00      1.00         3\n",
            "0601.B95800.001.14       1.00      1.00      1.00         3\n",
            "0601.R15H11.001.90       0.00      0.00      0.00         1\n",
            "0601.R29G14.001.90       0.00      0.00      0.00         1\n",
            "0601.R29G24.001.90       0.00      0.00      0.00         1\n",
            "0601.R29H11.001.90       0.33      0.75      0.46         4\n",
            "0601.R29H14.001.90       0.75      0.75      0.75         8\n",
            "0601.R29H17.001.90       0.00      0.00      0.00         1\n",
            "0601.R29H24.001.90       1.00      0.75      0.86         4\n",
            "0601.R29H34.001.90       0.70      0.88      0.78         8\n",
            "0601.R29H37.001.90       1.00      0.50      0.67         2\n",
            "0601.R29I11.001.90       1.00      0.33      0.50         3\n",
            "0601.R29I14.001.90       0.00      0.00      0.00         2\n",
            "0601.R29I24.001.90       1.00      0.50      0.67         2\n",
            "0601.R29I34.001.90       0.75      1.00      0.86         6\n",
            "0601.Y93006.001.90       1.00      1.00      1.00         2\n",
            "0601.Y93080.002.01       1.00      1.00      1.00         5\n",
            "0601.Y96G06.001.90       1.00      1.00      1.00         9\n",
            "0601.Y96I06.001.90       1.00      1.00      1.00         6\n",
            "\n",
            "          accuracy                           0.80       383\n",
            "         macro avg       0.71      0.67      0.67       383\n",
            "      weighted avg       0.80      0.80      0.79       383\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0801\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 4789 samples from 568 relevant classes. (N=1)\n",
            "Reduced to 4015 samples from 163 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.767\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0000010240       0.00      0.00      0.00         1\n",
            "           0000011440       0.50      1.00      0.67         1\n",
            "           0000012140       0.50      1.00      0.67         1\n",
            "           0000018010       0.75      1.00      0.86         3\n",
            "           0000040340       0.00      0.00      0.00         2\n",
            "           0000040440       0.50      1.00      0.67         1\n",
            "           0000040540       0.00      0.00      0.00         1\n",
            "           0000041340       1.00      1.00      1.00         1\n",
            "           0000041640       0.00      0.00      0.00         1\n",
            "           0000042040       0.20      0.33      0.25         3\n",
            "           0000042240       0.00      0.00      0.00         1\n",
            "           0000045841       0.00      0.00      0.00         1\n",
            "           0000047010       0.00      0.00      0.00         4\n",
            "           0000048010       1.00      1.00      1.00         1\n",
            "           0000053240       0.00      0.00      0.00         2\n",
            "           0000053940       1.00      1.00      1.00         1\n",
            "           0000081840       0.00      0.00      0.00         1\n",
            "           0000088052       0.50      0.67      0.57         3\n",
            "           0000088111       0.33      0.33      0.33         3\n",
            "           0000089010       0.40      0.67      0.50         3\n",
            "           0000091000       0.58      0.88      0.70        32\n",
            "           0000091010       0.20      0.33      0.25         3\n",
            "           0000091510       0.33      1.00      0.50         1\n",
            "           0000091511       1.00      0.50      0.67         8\n",
            "           0000091519       0.25      0.50      0.33         2\n",
            "           0000091541       0.27      0.44      0.33         9\n",
            "           0000091549       0.33      0.25      0.29         4\n",
            "           0000091550       0.00      0.00      0.00         1\n",
            "           0000091551       0.36      0.44      0.40         9\n",
            "           0000091559       0.50      0.25      0.33         4\n",
            "           0000091570       0.67      0.67      0.67         3\n",
            "           0000091571       0.75      0.38      0.50         8\n",
            "           0000091579       0.50      0.33      0.40         3\n",
            "           0000091580       0.00      0.00      0.00         4\n",
            "           0000091581       0.45      0.56      0.50         9\n",
            "           0000091589       1.00      1.00      1.00         2\n",
            "           0000091700       0.00      0.00      0.00         1\n",
            "           0000092695       0.67      0.50      0.57         4\n",
            "           0000092696       0.88      0.78      0.82         9\n",
            "           0000094000       0.50      0.40      0.44        10\n",
            "           0000094910       1.00      1.00      1.00         5\n",
            "           0000096200       1.00      1.00      1.00         1\n",
            "           0000096400       1.00      1.00      1.00         1\n",
            "           0000099206       1.00      1.00      1.00         1\n",
            "           0000099301       1.00      1.00      1.00         6\n",
            "           0000099970       1.00      1.00      1.00        10\n",
            "      0801.A41051.990       1.00      0.50      0.67         2\n",
            "      0801.C41001.100       0.00      0.00      0.00         4\n",
            "      0801.C41001.110       0.00      0.00      0.00         3\n",
            "      0801.C41001.120       0.33      0.33      0.33         3\n",
            "   0801.C41001.130.01       0.50      1.00      0.67         2\n",
            "      0801.C43015.150       0.00      0.00      0.00         1\n",
            "      0801.C91050.400       1.00      1.00      1.00         1\n",
            "      0801.C94102.200       1.00      1.00      1.00         4\n",
            "      0801.C94102.220       1.00      1.00      1.00         1\n",
            "      0801.C94102.230       1.00      1.00      1.00         4\n",
            "      0801.C94102.240       1.00      1.00      1.00         1\n",
            "      0801.C95001.119       0.88      0.93      0.90        15\n",
            "      0801.C95001.120       0.00      0.00      0.00         2\n",
            "      0801.C95001.402       0.91      1.00      0.95        10\n",
            "      0801.C95001.417       1.00      1.00      1.00         4\n",
            "      0801.C95001.420       1.00      0.62      0.77         8\n",
            "      0801.C95001.421       1.00      1.00      1.00         2\n",
            "      0801.C95001.423       0.00      0.00      0.00         3\n",
            "      0801.C95001.424       1.00      0.67      0.80         3\n",
            "      0801.C95001.431       1.00      1.00      1.00         4\n",
            "      0801.C95001.510       1.00      1.00      1.00         2\n",
            "      0801.C95001.511       1.00      1.00      1.00         2\n",
            "      0801.C95001.512       1.00      0.33      0.50         3\n",
            "      0801.C95001.513       0.33      0.50      0.40         4\n",
            "      0801.C95001.514       1.00      1.00      1.00         2\n",
            "      0801.C95001.515       1.00      1.00      1.00         2\n",
            "      0801.C95001.530       1.00      0.67      0.80         3\n",
            "      0801.C95001.539       1.00      1.00      1.00         4\n",
            "      0801.C95001.590       0.75      1.00      0.86         3\n",
            "      0801.C95001.591       1.00      1.00      1.00         5\n",
            "      0801.C95001.592       1.00      1.00      1.00         6\n",
            "      0801.C95001.593       1.00      1.00      1.00         4\n",
            "      0801.C95001.701       0.00      0.00      0.00         1\n",
            "      0801.C95001.702       1.00      1.00      1.00         5\n",
            "      0801.C95001.704       1.00      1.00      1.00         4\n",
            "      0801.C95001.801       0.67      0.86      0.75         7\n",
            "      0801.C95001.804       0.50      0.25      0.33         4\n",
            "      0801.C95001.806       0.67      1.00      0.80         4\n",
            "      0801.C95001.808       0.00      0.00      0.00         1\n",
            "      0801.C95001.819       0.50      0.50      0.50         2\n",
            "      0801.C95001.820       0.00      0.00      0.00         1\n",
            "      0801.C95001.822       0.50      1.00      0.67         1\n",
            "      0801.C95001.828       1.00      0.75      0.86         4\n",
            "   0801.C95001.913.01       1.00      1.00      1.00         5\n",
            "      0801.C95001.914       0.83      1.00      0.91         5\n",
            "   0801.C95001.923.01       1.00      1.00      1.00         7\n",
            "      0801.C95001.931       1.00      1.00      1.00         8\n",
            "   0801.C95001.933.08       1.00      1.00      1.00         9\n",
            "      0801.C95001.941       1.00      1.00      1.00         6\n",
            "      0801.C95001.945       0.98      0.99      0.99       304\n",
            "      0801.C95001.990       1.00      1.00      1.00         6\n",
            "0801.C96402.042.01.02       1.00      1.00      1.00         6\n",
            "      0801.S10101.119       1.00      1.00      1.00         1\n",
            "      0801.S10102.119       0.00      0.00      0.00         1\n",
            "      0801.S10103.121       1.00      0.67      0.80         3\n",
            "      0801.S10203.110       0.00      0.00      0.00         1\n",
            "      0801.S10203.114       0.50      1.00      0.67         2\n",
            "      0801.S10204.114       0.00      0.00      0.00         2\n",
            "      0801.S10302.102       0.00      0.00      0.00         1\n",
            "      0801.S10302.103       0.00      0.00      0.00         1\n",
            "      0801.S10303.101       0.67      0.67      0.67         3\n",
            "      0801.S10303.102       0.00      0.00      0.00         1\n",
            "      0801.S10303.103       0.33      1.00      0.50         1\n",
            "      0801.S10303.104       0.50      0.33      0.40         3\n",
            "      0801.S40101.407       0.00      0.00      0.00         1\n",
            "      0801.S40101.412       0.00      0.00      0.00         1\n",
            "      0801.S40101.413       0.00      0.00      0.00         1\n",
            "      0801.S40102.415       0.00      0.00      0.00         1\n",
            "      0801.S40102.416       0.50      1.00      0.67         1\n",
            "      0801.S40102.420       1.00      1.00      1.00         3\n",
            "      0801.S40103.402       0.00      0.00      0.00         3\n",
            "      0801.S40103.403       0.20      0.33      0.25         3\n",
            "      0801.S40103.405       0.20      0.20      0.20         5\n",
            "      0801.S40103.406       1.00      0.50      0.67         2\n",
            "      0801.S40103.407       0.50      0.50      0.50         2\n",
            "      0801.S40103.413       0.00      0.00      0.00         1\n",
            "      0801.S40103.416       0.00      0.00      0.00         2\n",
            "      0801.S40103.418       0.00      0.00      0.00         1\n",
            "      0801.S40103.420       0.00      0.00      0.00         2\n",
            "      0801.S40103.458       0.33      0.50      0.40         2\n",
            "   0801.S40107.403.02       1.00      0.50      0.67         2\n",
            "   0801.S40107.413.01       0.00      0.00      0.00         1\n",
            "   0801.S40107.413.03       0.00      0.00      0.00         2\n",
            "      0801.S43236.990       0.00      0.00      0.00         1\n",
            "      0801.S50103.538       0.00      0.00      0.00         1\n",
            "      0801.S50103.539       0.00      0.00      0.00         1\n",
            "      0801.S50203.501       0.00      0.00      0.00         1\n",
            "      0801.S50203.502       1.00      1.00      1.00         1\n",
            "      0801.S50203.506       0.00      0.00      0.00         1\n",
            "      0801.S50203.509       0.00      0.00      0.00         1\n",
            "      0801.S50203.517       0.67      1.00      0.80         2\n",
            "      0801.S50203.518       0.00      0.00      0.00         2\n",
            "      0801.S50203.529       0.00      0.00      0.00         1\n",
            "      0801.S50203.530       0.50      0.33      0.40         3\n",
            "      0801.S50203.531       0.00      0.00      0.00         1\n",
            "   0801.S50206.506.02       0.00      0.00      0.00         1\n",
            "      0801.S50301.532       0.00      0.00      0.00         1\n",
            "      0801.S50302.532       0.00      0.00      0.00         3\n",
            "      0801.S50303.532       0.40      1.00      0.57         4\n",
            "      0801.S50304.532       0.00      0.00      0.00         2\n",
            "      0801.S70101.700       1.00      1.00      1.00         4\n",
            "      0801.S70101.702       0.50      1.00      0.67         1\n",
            "      0801.S70102.700       0.00      0.00      0.00         1\n",
            "      0801.S70102.702       0.33      1.00      0.50         1\n",
            "      0801.S70103.702       0.00      0.00      0.00         1\n",
            "      0801.S70104.702       0.00      0.00      0.00         2\n",
            "      0801.S80103.800       0.00      0.00      0.00         1\n",
            "      0801.S80103.801       0.00      0.00      0.00         1\n",
            "      0801.S80103.806       1.00      0.50      0.67         2\n",
            "      0801.S80104.800       0.00      0.00      0.00         1\n",
            "      0801.S81096.900       0.00      0.00      0.00         4\n",
            "      0801.S94103.990       0.00      0.00      0.00         1\n",
            "      0801.W91002.990       1.00      1.00      1.00         1\n",
            "      0801.Z52027.990       0.00      0.00      0.00         1\n",
            "           2080116600       0.00      0.00      0.00         1\n",
            "           2080122300       0.00      0.00      0.00         3\n",
            "           2080181000       1.00      1.00      1.00         2\n",
            "\n",
            "             accuracy                           0.77       803\n",
            "            macro avg       0.47      0.50      0.47       803\n",
            "         weighted avg       0.75      0.77      0.75       803\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0806\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 229 samples from 20 relevant classes. (N=1)\n",
            "Reduced to 204 samples from 8 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.805\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000068000       0.89      0.92      0.91        26\n",
            "     0000068100       1.00      1.00      1.00         2\n",
            "     0000069000       0.75      0.60      0.67         5\n",
            "0806.A68009.950       0.50      1.00      0.67         1\n",
            "0806.S60000.010       0.50      0.50      0.50         2\n",
            "0806.S60000.500       0.00      0.00      0.00         1\n",
            "0806.S60000.800       0.67      0.67      0.67         3\n",
            "0806.S61016.990       0.00      0.00      0.00         1\n",
            "\n",
            "       accuracy                           0.80        41\n",
            "      macro avg       0.54      0.59      0.55        41\n",
            "   weighted avg       0.79      0.80      0.79        41\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0811\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 53 samples from 10 relevant classes. (N=1)\n",
            "Reduced to 36 samples from 3 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.750\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000010111       1.00      0.33      0.50         3\n",
            "     0000010511       0.60      1.00      0.75         3\n",
            "0811.C95002.105       1.00      1.00      1.00         2\n",
            "\n",
            "       accuracy                           0.75         8\n",
            "      macro avg       0.87      0.78      0.75         8\n",
            "   weighted avg       0.85      0.75      0.72         8\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0821\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1069 samples from 105 relevant classes. (N=1)\n",
            "Reduced to 972 samples from 50 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.903\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000010721       1.00      0.50      0.67         2\n",
            "     0000010921       0.67      0.67      0.67         3\n",
            "     0000011421       1.00      1.00      1.00         1\n",
            "     0000050221       0.00      0.00      0.00         1\n",
            "     0000050321       0.67      1.00      0.80         2\n",
            "     0000050421       0.00      0.00      0.00         1\n",
            "     0000050721       1.00      1.00      1.00         1\n",
            "     0000050961       1.00      1.00      1.00         1\n",
            "     0000052221       0.00      0.00      0.00         2\n",
            "     0000052421       0.50      0.50      0.50         2\n",
            "     0000054221       0.50      1.00      0.67         2\n",
            "     0000058061       1.00      1.00      1.00         1\n",
            "     0000098000       0.91      0.91      0.91        11\n",
            "     0000098010       0.88      0.88      0.88         8\n",
            "     0000098300       1.00      1.00      1.00         3\n",
            "     0000098900       0.83      0.93      0.88        27\n",
            "     0000099083       0.91      0.77      0.83        13\n",
            "     0000099121       1.00      1.00      1.00         5\n",
            "0821.C95001.110       1.00      1.00      1.00         3\n",
            "0821.C95001.114       1.00      1.00      1.00         2\n",
            "0821.C95001.907       1.00      1.00      1.00         1\n",
            "0821.C95001.944       1.00      0.88      0.93         8\n",
            "0821.C95001.945       1.00      1.00      1.00         4\n",
            "0821.C95002.107       0.50      0.50      0.50         4\n",
            "0821.C95002.108       0.00      0.00      0.00         1\n",
            "0821.C95002.109       0.80      1.00      0.89         4\n",
            "0821.C95002.112       1.00      1.00      1.00         4\n",
            "0821.C95002.113       1.00      1.00      1.00         4\n",
            "0821.C95002.501       1.00      1.00      1.00         3\n",
            "0821.C95002.502       1.00      1.00      1.00         4\n",
            "0821.C95002.503       1.00      1.00      1.00         4\n",
            "0821.C95002.504       1.00      1.00      1.00         4\n",
            "0821.C95002.505       0.86      1.00      0.92         6\n",
            "0821.C95002.506       1.00      0.67      0.80         3\n",
            "0821.C95002.507       1.00      1.00      1.00         2\n",
            "0821.C95002.518       1.00      1.00      1.00         4\n",
            "0821.C95002.519       1.00      1.00      1.00         3\n",
            "0821.C95002.520       1.00      1.00      1.00         3\n",
            "0821.C95002.521       1.00      1.00      1.00         4\n",
            "0821.C95002.522       1.00      1.00      1.00         3\n",
            "0821.C95002.523       1.00      1.00      1.00         3\n",
            "0821.C95002.524       1.00      1.00      1.00         4\n",
            "0821.C95002.525       0.60      1.00      0.75         3\n",
            "0821.C95002.527       1.00      1.00      1.00         4\n",
            "0821.C95002.528       1.00      1.00      1.00         4\n",
            "0821.C95002.529       1.00      1.00      1.00         4\n",
            "0821.C95002.530       1.00      1.00      1.00         4\n",
            "0821.C98001.990       1.00      1.00      1.00         2\n",
            "0821.W98000.700       1.00      1.00      1.00         2\n",
            "0821.W98000.800       1.00      1.00      1.00         1\n",
            "\n",
            "       accuracy                           0.90       195\n",
            "      macro avg       0.85      0.86      0.85       195\n",
            "   weighted avg       0.89      0.90      0.89       195\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 2101\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1321 samples from 137 relevant classes. (N=1)\n",
            "Reduced to 1158 samples from 42 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.741\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0000001000       0.67      0.80      0.73         5\n",
            "           0000002000       0.40      0.25      0.31         8\n",
            "           0000002004       0.73      0.69      0.71        16\n",
            "           0000002200       1.00      1.00      1.00         2\n",
            "           0000002401       0.62      0.62      0.62         8\n",
            "           0000002404       0.00      0.00      0.00         1\n",
            "           0000002502       0.60      0.60      0.60         5\n",
            "           0000003000       1.00      0.75      0.86        12\n",
            "           0000005100       1.00      0.25      0.40         4\n",
            "           0000005101       1.00      0.33      0.50         3\n",
            "           0000005206       1.00      0.25      0.40         4\n",
            "           0000006102       1.00      0.71      0.83         7\n",
            "           0000006104       0.00      0.00      0.00         4\n",
            "           0000006106       0.70      0.88      0.78         8\n",
            "           0000006200       0.60      1.00      0.75         3\n",
            "           0000006202       0.81      0.81      0.81        21\n",
            "           0000006203       0.44      0.65      0.52        17\n",
            "           0000006204       0.50      0.50      0.50         4\n",
            "           0000006205       0.50      0.50      0.50         2\n",
            "           0000006302       0.67      0.73      0.70        11\n",
            "           0000006401       1.00      1.00      1.00         1\n",
            "           0000009101       0.91      1.00      0.95        10\n",
            "           0000009103       0.93      1.00      0.96        13\n",
            "           0000009110       0.88      1.00      0.93         7\n",
            "           0000009303       0.75      1.00      0.86         6\n",
            "           0000009401       1.00      1.00      1.00         2\n",
            "2101.C20200.001.62.03       0.67      0.67      0.67         3\n",
            "2101.C64100.011.20.00       1.00      1.00      1.00         2\n",
            "2101.C78200.800.52.06       0.67      1.00      0.80         2\n",
            "2101.C78200.930.52.06       0.80      1.00      0.89         4\n",
            "2101.C78300.800.52.06       0.67      1.00      0.80         2\n",
            "2101.C78300.930.52.06       0.86      0.86      0.86         7\n",
            "2101.C83232.001.61.02       0.00      0.00      0.00         1\n",
            "2101.C83239.001.61.02       0.75      1.00      0.86         3\n",
            "2101.C86008.191.61.06       0.00      0.00      0.00         1\n",
            "2101.C86008.360.61.06       1.00      0.67      0.80         3\n",
            "2101.C86008.396.61.06       0.92      1.00      0.96        12\n",
            "2101.C86008.791.61.06       0.50      1.00      0.67         1\n",
            "2101.C88005.010.51.01       0.00      0.00      0.00         1\n",
            "2101.C92000.001.62.03       0.50      0.50      0.50         2\n",
            "2101.C92192.002.62.03       1.00      1.00      1.00         2\n",
            "2101.Z80004.001.20.00       1.00      0.50      0.67         2\n",
            "\n",
            "             accuracy                           0.74       232\n",
            "            macro avg       0.69      0.68      0.66       232\n",
            "         weighted avg       0.75      0.74      0.73       232\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 2111\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 624 samples from 57 relevant classes. (N=1)\n",
            "Reduced to 551 samples from 19 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.829\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000007100       0.83      0.98      0.90        54\n",
            "     0000007111       0.50      1.00      0.67         1\n",
            "     0000007112       0.00      0.00      0.00         1\n",
            "     0000007114       0.00      0.00      0.00         2\n",
            "     0000007116       0.00      0.00      0.00         1\n",
            "     0000007156       0.00      0.00      0.00         2\n",
            "     0000007160       1.00      0.50      0.67         2\n",
            "     0000007175       0.83      0.94      0.88        16\n",
            "     0000007800       0.00      0.00      0.00         3\n",
            "2111.C02006.001       1.00      1.00      1.00         3\n",
            "2111.C40073.001       0.00      0.00      0.00         2\n",
            "2111.C50121.001       1.00      1.00      1.00         2\n",
            "2111.C60074.001       0.00      0.00      0.00         2\n",
            "2111.C70055.001       1.00      1.00      1.00         1\n",
            "2111.C80015.001       0.00      0.00      0.00         1\n",
            "2111.C80028.001       1.00      0.83      0.91         6\n",
            "2111.C80057.001       1.00      1.00      1.00         8\n",
            "2111.C80067.001       1.00      0.50      0.67         2\n",
            "2111.C90003.001       1.00      1.00      1.00         2\n",
            "\n",
            "       accuracy                           0.83       111\n",
            "      macro avg       0.53      0.51      0.51       111\n",
            "   weighted avg       0.76      0.83      0.79       111\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 3104\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 172 samples from 21 relevant classes. (N=1)\n",
            "Reduced to 155 samples from 10 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.613\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  3104C20000       0.60      0.75      0.67         4\n",
            "  3104C21000       0.29      0.40      0.33         5\n",
            "  3104C22000       0.89      0.80      0.84        10\n",
            "  3104C23000       0.00      0.00      0.00         3\n",
            "  3104C53000       1.00      1.00      1.00         1\n",
            "  3104C55200       1.00      1.00      1.00         1\n",
            "  3104C56000       0.00      0.00      0.00         1\n",
            " 802909914.0       1.00      0.50      0.67         2\n",
            " 802909920.0       0.67      1.00      0.80         2\n",
            " 802909926.0       0.33      0.50      0.40         2\n",
            "\n",
            "    accuracy                           0.61        31\n",
            "   macro avg       0.58      0.59      0.57        31\n",
            "weighted avg       0.60      0.61      0.60        31\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 3401\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 62 samples from 1 relevant classes. (N=1)\n",
            "Reduced to 62 samples from 1 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 1.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  2340141000       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        13\n",
            "   macro avg       1.00      1.00      1.00        13\n",
            "weighted avg       1.00      1.00      1.00        13\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 3420\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 279 samples from 20 relevant classes. (N=1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 254 samples from 7 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.745\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  3420C10000       0.82      0.75      0.78        12\n",
            " 802000424.0       1.00      0.50      0.67         8\n",
            " 802000425.0       1.00      1.00      1.00         2\n",
            " 802000426.0       0.67      1.00      0.80        10\n",
            " 802015306.0       0.82      0.82      0.82        11\n",
            " 802015320.0       0.43      0.43      0.43         7\n",
            " 802015324.0       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.75        51\n",
            "   macro avg       0.82      0.79      0.79        51\n",
            "weighted avg       0.77      0.75      0.74        51\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 9301\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 23 samples from 9 relevant classes. (N=1)\n",
            "Reduced to 13 samples from 2 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  0077020010       0.50      1.00      0.67         1\n",
            "  2930110500       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.75      0.75      0.67         3\n",
            "weighted avg       0.83      0.67      0.67         3\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 9310\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 187 samples from 61 relevant classes. (N=1)\n",
            "Reduced to 97 samples from 12 relevant classes. (N=5)\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.800\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  2931032001       1.00      1.00      1.00         3\n",
            "  2931032230       1.00      1.00      1.00         1\n",
            "  2931032240       1.00      0.50      0.67         2\n",
            "  2931032350       0.00      0.00      0.00         2\n",
            "  2931032351       0.60      1.00      0.75         3\n",
            "  2931032500       0.00      0.00      0.00         1\n",
            "  2931032630       1.00      1.00      1.00         1\n",
            "  2931032740       1.00      1.00      1.00         1\n",
            "  2931035320       1.00      1.00      1.00         2\n",
            "  293103CD05       1.00      1.00      1.00         1\n",
            "  293103L203       0.67      1.00      0.80         2\n",
            "  293105A004       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.73      0.79      0.74        20\n",
            "weighted avg       0.73      0.80      0.74        20\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 9351\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 168 samples from 21 relevant classes. (N=1)\n",
            "Reduced to 154 samples from 14 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.839\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  2935114006       1.00      1.00      1.00         3\n",
            "  2935116100       0.86      0.75      0.80         8\n",
            "  2935116200       0.67      1.00      0.80         2\n",
            "  2935116210       0.50      0.50      0.50         2\n",
            "  2935116230       1.00      1.00      1.00         1\n",
            "  2935116410       1.00      1.00      1.00         1\n",
            "  2935116910       0.00      0.00      0.00         2\n",
            "  2935116920       1.00      1.00      1.00         2\n",
            "  2935158005       0.50      1.00      0.67         2\n",
            "  2935158008       1.00      1.00      1.00         1\n",
            "  2935158010       1.00      1.00      1.00         3\n",
            "  2935158012       1.00      1.00      1.00         2\n",
            "  2935158016       1.00      1.00      1.00         1\n",
            "  2935158020       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.84        31\n",
            "   macro avg       0.82      0.88      0.84        31\n",
            "weighted avg       0.81      0.84      0.82        31\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 9370\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 104 samples from 18 relevant classes. (N=1)\n",
            "Reduced to 89 samples from 7 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  2937030000       1.00      1.00      1.00         2\n",
            "  2937033400       1.00      1.00      1.00         3\n",
            "  2937039200       1.00      1.00      1.00         1\n",
            "  2937052000       0.83      1.00      0.91         5\n",
            "  2937053000       1.00      0.50      0.67         2\n",
            "  2937059000       1.00      1.00      1.00         2\n",
            "  2937060000       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.89        18\n",
            "   macro avg       0.93      0.88      0.89        18\n",
            "weighted avg       0.90      0.89      0.88        18\n",
            "\n",
            "--------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for company_code in clf_bukr.classes_:\n",
        "    print(f\"Company Code: {str(company_code).zfill(4)}\" + \"\\n\")\n",
        "    # reduce dataframe to entries for fixed company code\n",
        "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
        "    \n",
        "    print(\"Cost Element classification:\")\n",
        "    # reduce to relevant samples\n",
        "    df_ce = reduce_to_relevant(df_cost_elem, \"Target\", min_num_samples=local_min_num)\n",
        "    \n",
        "    if df_ce.shape[0] == 0:  # do not proceed if df is empty\n",
        "        print(20 * \"-\" + \"\\n\")\n",
        "        continue\n",
        "    train_com_model('Target')\n",
        "    \n",
        "    print(20 * \"-\" + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "XGB_2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "092bf18aecc55aef496ffbc6cc353c2203dc3161741cf495c00395ce4a3c9d87"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('anaconda3': virtualenv)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}