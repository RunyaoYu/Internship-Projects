{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniper Account Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
    "import pickle\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import dill as pkl\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "TODAY = date.today().strftime(\"%Y%m%d\")\n",
    "CLIENT = \"Uniper\"\n",
    "MIN_NUM_OF_SAMPLES = 5\n",
    "COUNTRY = 'DE'  # possible choices: DE, UK, AT, SE, UBX\n",
    "SAVE_CLFS = True\n",
    "PREDICTIONS_EXCEL = False\n",
    "RES_DIR = Path(f\"./retraining_october21/{COUNTRY.lower()}\")\n",
    "SCAN_ID_COL = \"gl_document_scan_id\"  # document identifier col used when reducing\n",
    "                                     # global df to relevant examples for attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RES_DIR):\n",
    "    os.makedirs(RES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load training_utils.py\n",
    "\"\"\"This file contains helper functionality to train/evaluate models\n",
    "   and create reports\"\"\"\n",
    "\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
    "\n",
    "\n",
    "def reduce_to_relevant(df: DataFrame, col: str, min_num_samples: int) -> DataFrame:\n",
    "    \"\"\"Reduces df to instances with values in col that appear at least\n",
    "       min_num_samples times and returns reduced df.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        col (str): Name of the column that holds the feature of interest.\n",
    "        min_num_samples (int): Minimum number of times a value has to appear\n",
    "            in column 'col'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Reduced DataFrame containing only those values in 'col'\n",
    "            that appear more than 'min_num_samples' times.\n",
    "\n",
    "    \"\"\"\n",
    "    # find rows for values that appear at least min_num_samples times\n",
    "    relevant = [x for x in df[col].value_counts().index\n",
    "                if df[col].value_counts()[x] >= min_num_samples]\n",
    "    # create boolean mask\n",
    "    mask = [(x in relevant) for x in df[col]]\n",
    "\n",
    "    print(\n",
    "        f\"Reduced to {len(df[mask])} samples from {len(relevant)} relevant classes. (N={min_num_samples})\"\n",
    "    )\n",
    "\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def get_reduced_df(\n",
    "        df: DataFrame,\n",
    "        feature_col: str,\n",
    "        scan_id_col: str,\n",
    "        min_num_samples: int) -> DataFrame:\n",
    "    \"\"\"Drops duplicates and reduces df to relevant examples that appear\n",
    "       at least min_num_samples times and are unambiguous.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        feature_col (str): Name of the column that holds the feature of interest.\n",
    "        scan_id_col (str): Name of the column that holds the unique\n",
    "            document identifier.\n",
    "        min_num_samples (int): Minimum number of times a value has to appear\n",
    "            in column 'feature_col'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Reduced DataFrame containing only those values in 'feature_col'\n",
    "            that appear more than 'min_num_samples' times and are unambiguous.\n",
    "\n",
    "    \"\"\"\n",
    "    # keep only documents with unambiguous value for this col\n",
    "    df_ = df.drop_duplicates(subset=[scan_id_col, feature_col])\\\n",
    "            .groupby(scan_id_col)\\\n",
    "            .filter(lambda x: len(x) == 1)\n",
    "\n",
    "    return reduce_to_relevant(df_, feature_col, min_num_samples)\n",
    "\n",
    "\n",
    "def split_for_target_col(df, col, test_size=0.2, random_state=666):\n",
    "    \"\"\"\n",
    "    Performs train test split with specified col as target variable.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    return train_test_split(\n",
    "        df,\n",
    "        df[col],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_top_n_results_with_confs(\n",
    "        clazzes: Sequence[str],\n",
    "        probs: List[float],\n",
    "        n: int = 1) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Constructs list of (class, proba) tuples for top n results.\n",
    "\n",
    "    Args:\n",
    "        clazzes (Sequence[str]): Sequence of class names as stored in\n",
    "            clf.classes_ attribute of sklearn classifier.\n",
    "        probs (List[float]): List with probabilities for each class in clazzes.\n",
    "        n (int): Number of most probable results to return. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: List of (class, proba) tuples for top n results.\n",
    "\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        zip(clazzes, probs),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:n]\n",
    "\n",
    "\n",
    "def get_results_for_target(\n",
    "        target_clf: Any,\n",
    "        df: DataFrame) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Computes predictions with provided classifier on DataFrame df.\n",
    "\n",
    "    Args:\n",
    "        target_clf (Any): Sklearn classifier that offers 'predict_proba()'.\n",
    "        df (DataFrame): Input DataFrame as expected by 'target_clf'.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: List of (class, proba) tuples for\n",
    "            instances in df.\n",
    "\n",
    "    \"\"\"\n",
    "    probs = target_clf.predict_proba(df)\n",
    "    clazzes = target_clf.classes_\n",
    "    results = []\n",
    "    for prob_list in probs:\n",
    "        results.append(_get_top_n_results_with_confs(clazzes, prob_list)[0])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _get_text_col_from_df(df: DataFrame):\n",
    "    return df['text']\n",
    "\n",
    "\n",
    "def save_clf_to_disk(\n",
    "        clf,\n",
    "        attribute_name: str,\n",
    "        folder: Path,\n",
    "        date: str,\n",
    "        client: str,\n",
    "        country: str,\n",
    "        min_num_samples: int,\n",
    "        add_zip=True) -> None:\n",
    "\n",
    "    # safe to pkl with full info in file name\n",
    "    pkl_path = folder / f\"{date}_{client}_clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl\"\n",
    "    with open(pkl_path, 'wb') as file:\n",
    "        pkl.dump(clf, file)\n",
    "\n",
    "    if add_zip:\n",
    "        # create zip with non changing name for easy deployment\n",
    "        zip_path = folder / f\"clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl.zip\"\n",
    "        with ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as file:\n",
    "            # second argument avoids recreation of folder structure in zip-archive\n",
    "            file.write(pkl_path, pkl_path.parts[-1])\n",
    "\n",
    "\n",
    "def enrich_kw_list(kw_list):\n",
    "    \"\"\"Enriches keywords by replacing street names with possible synonyms. Since we demand ALL keywords to\n",
    "       be found for a positive match, this function returns a list of keyword lists, one for every possible\n",
    "       synonym.\n",
    "    \n",
    "    \"\"\"\n",
    "    new_kw_lists = [kw_list]\n",
    "    street_synonyms = [\"str.\", \"strasse\", \"straße\"]\n",
    "    for street_syn in street_synonyms:\n",
    "        remaining_syns = [x for x in street_synonyms if x != street_syn]\n",
    "        if any(street_syn in kw for kw in kw_list):\n",
    "            new_kw_lists += [[kw.replace(street_syn, synonym) for kw in kw_list] for synonym in remaining_syns]\n",
    "    return new_kw_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained on Friday 04. February 2022 with package versions: \n",
      "\n",
      "scikit-learn: 1.0.2\n",
      "dill: 0.3.4\n"
     ]
    }
   ],
   "source": [
    "tday = date.today().strftime(\"%A %d. %B %Y\") \n",
    "print(f\"Models trained on {tday} with package versions: \\n\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"dill: {pkl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts used to restrict ground truth later\n",
    "country_group_to_countries = {\n",
    "    \"DE\": [\"DE\"],\n",
    "    \"SE\": [\"SE\"],\n",
    "    \"AT\": [\"AT\"],\n",
    "    \"UK\": [\"GB\"],\n",
    "    \"UBX\": [\"BE\", \"NL\", \"LU\"]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/texts_all_rt202110.pkl\", \"rb\") as file:\n",
    "    df_lume = pickle.load(file)\n",
    "df_lume.drop_duplicates(inplace=True)\n",
    "print(df_lume.shape)\n",
    "\n",
    "df_ground_truth = pd.read_excel(\"C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/Uniper_GT_09_21.xlsx\")\n",
    "len(df_ground_truth)\n",
    "\n",
    "df_ground_truth.dropna(subset=[SCAN_ID_COL], inplace=True)\n",
    "df_ground_truth[SCAN_ID_COL] = df_ground_truth[SCAN_ID_COL].apply(lambda x: x.lower())\n",
    "\n",
    "len(df_ground_truth[SCAN_ID_COL].unique())\n",
    "df_merged = df_lume.merge(df_ground_truth, left_on=[\"filename\"], right_on=[SCAN_ID_COL], how=\"inner\")\n",
    "\n",
    "df_merged.drop_duplicates(inplace=True)\n",
    "df_merged.shape\n",
    "\n",
    "# add left-hand zeros to gl_legal_entity_id, gl_accounts_id, gl_vendor_id\n",
    "df_merged['gl_legal_entity_id'] = df_merged['gl_legal_entity_id'].apply(lambda x: str(int(x)).zfill(4))\n",
    "df_merged['gl_accounts_id'] = df_merged['gl_accounts_id'].apply(lambda x: str(int(x)).zfill(10))\n",
    "df_merged['gl_vendor_id'] = df_merged['gl_vendor_id'].apply(lambda x: str(int(x)).zfill(10))\n",
    "\n",
    "# restict to respective country(group)\n",
    "df = df_merged[\n",
    "    df_merged['le_country_id'].isin(country_group_to_countries[COUNTRY])\n",
    "]\n",
    "\n",
    "df['le_country_id'].value_counts()\n",
    "\n",
    "df['gl_posting_id'] = df['gl_posting_id'].apply(lambda x: str(int(x)))\n",
    "\n",
    "dft = get_reduced_df(df, 'gl_posting_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
    "dft[dft['gl_posting_id'] == '50']\n",
    "df[['gl_legal_entity_id', 'gl_accounts_id', 'gl_cost_center_id', 'gl_wbs_element_id', 'gl_vendor_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished remapping, writing gl_document_scan_id of mapped invoices to excel file\n"
     ]
    }
   ],
   "source": [
    "with open('C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/sc21v8.json', 'r') as file:\n",
    "    mapping_json = json.load(file)\n",
    "\n",
    "enrich_kw_list([\"a\", \"b\", \"heinestrasse\"])\n",
    "\n",
    "# apply mapping if df contains any of the relevant legeal entity ids\n",
    "\n",
    "remapped_docs = []\n",
    "\n",
    "if any([key in df['gl_legal_entity_id'].unique() for key in mapping_json.keys()]):\n",
    "    \n",
    "    #print(\"Starting to apply remapping script...\")\n",
    "    \n",
    "    type_to_col = {\n",
    "        'psp': 'gl_wbs_element_id',\n",
    "        'cc': 'gl_cost_center_id',\n",
    "        None: ['gl_cost_center_id', 'gl_order_id', 'gl_wbs_element_id']\n",
    "    }\n",
    "\n",
    "    # from collections import defaultdict\n",
    "    # old_vals = defaultdict(list)\n",
    "    # new_vals = defaultdict(list)\n",
    "\n",
    "    for legal_entity, mapping_list in mapping_json.items():\n",
    "        for mapping in mapping_list:\n",
    "\n",
    "            gl_acc, old_value, old_type, new_value, new_type, gl_vend, keywords = mapping\n",
    "\n",
    "            if old_value is None:\n",
    "                continue\n",
    "                \n",
    "            # added 20211021\n",
    "            if old_type is None and new_type is None:\n",
    "                continue\n",
    "\n",
    "            # fill up with left hand zeros\n",
    "            # gl account auch mit 0 füllen???\n",
    "            if old_type == 'cc':\n",
    "                old_value = old_value.zfill(10)\n",
    "            if new_type == 'cc':\n",
    "                new_value = new_value.zfill(10)\n",
    "            if gl_vend is not None:\n",
    "                gl_vend = gl_vend.zfill(10)\n",
    "\n",
    "            old_col = type_to_col[old_type]\n",
    "            new_col = type_to_col[new_type]\n",
    "\n",
    "            # create filter mask\n",
    "            if old_value == '*':\n",
    "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
    "                    & (df['gl_accounts_id'] == gl_acc)\n",
    "            else:\n",
    "                #print(f\"legal entity {legal_entity}, mapping {mapping}\")\n",
    "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
    "                    & (df['gl_accounts_id'] == gl_acc) \\\n",
    "                    & (df[old_col] == old_value)\n",
    "\n",
    "            if gl_vend is not None:\n",
    "                mask = mask & (df['gl_vendor_id'] == gl_vend)\n",
    "\n",
    "            if keywords:\n",
    "                kw_lists = enrich_kw_list(keywords)\n",
    "                kw_mask = df['text'].apply(\n",
    "                    lambda text: any([all([kw in text for kw in kws]) for kws in kw_lists])\n",
    "                )\n",
    "                mask = mask & kw_mask\n",
    "\n",
    "            # set new values, override old ones if necessary\n",
    "            if not df.loc[mask].empty:\n",
    "                #print(f'found value {old_value} of type {old_type} for legal entity {legal_entity}')\n",
    "                if old_col == new_col:\n",
    "    #                 print('old_col == new_col')\n",
    "                    df.loc[mask, new_col] = new_value\n",
    "    #                 old_vals[legal_entity].append(old_value)\n",
    "    #                 new_vals[legal_entity].append(new_value)\n",
    "                else:\n",
    "    #                 print('old_col != new_col')\n",
    "                    df.loc[mask, old_col] = np.nan\n",
    "                    df.loc[mask, new_col] = new_value\n",
    "                \n",
    "                if isinstance(df.loc[mask, SCAN_ID_COL], str):\n",
    "                    remapped_docs.append(df.loc[mask, SCAN_ID_COL])\n",
    "                else:\n",
    "                    remapped_docs += list(df.loc[mask, SCAN_ID_COL])\n",
    "                \n",
    "    print(f\"finished remapping, writing {SCAN_ID_COL} of mapped invoices to excel file\")\n",
    "    pd.DataFrame.from_dict({SCAN_ID_COL: list(set(remapped_docs))})\\\n",
    "            .to_excel(RES_DIR / f\"{TODAY}_{CLIENT}_remapped_invoices.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runyao's update\n",
    "\n",
    "def get_certain_class_after_vec_country(df_lume, label):\n",
    "    label = label\n",
    "    df_attr = get_reduced_df(df_lume, label, SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
    "    x_train, x_test, y_train, y_test = split_for_target_col_stratified(df_attr, label)\n",
    "    vectorizer = TfidfVectorizer(max_features=20000, max_df=0.75, sublinear_tf=True,)\n",
    "    X_train = x_train['text']\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test = x_test['text']\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    return x_train, x_test, y_train, y_test, X_train_vec, X_test_vec\n",
    "\n",
    "\n",
    "def split_for_target_col_stratified(df, col, test_size=0.2, random_state=42,):\n",
    "    return train_test_split(df,\n",
    "        df[col],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df[col]\n",
    "    )\n",
    "\n",
    "\n",
    "def train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test):\n",
    "\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_train_pred = model.predict(X_train_vec)\n",
    "    y_test_pred = model.predict(X_test_vec)\n",
    "\n",
    "    print(model.best_params_)\n",
    "    print(model.best_score_)\n",
    "    print(\"Training Accuracy: {:.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
    "    print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test, y_test_pred)))\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # confusion matix\n",
    "    conf_train = confusion_matrix(y_train, y_train_pred)\n",
    "    conf_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
    "    sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
    "    ax1.set(xlabel=\"predicted label\")\n",
    "    ax1.set(ylabel=\"actual label\")\n",
    "    #ax1.set_xticklabels(['0','1'])\n",
    "    #ax1.set_yticklabels(['0','1'])\n",
    "    ax1.set(title=\"Confusion Matrix for training set\")\n",
    "\n",
    "    sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
    "    ax2.set(xlabel=\"predicted label\")\n",
    "    ax2.set(ylabel=\"actual label\")\n",
    "    #ax2.set_xticklabels(['0','1'])\n",
    "    #ax2.set_yticklabels(['0','1'])\n",
    "    ax2.set(title=\"Confusion Matrix for test set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_accounts_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 28168 samples from 176 relevant classes. (N=5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>gl_instance_id</th>\n",
       "      <th>gl_fiscal_year</th>\n",
       "      <th>gl_header_id</th>\n",
       "      <th>gl_line_nr</th>\n",
       "      <th>gl_reference_document_nr</th>\n",
       "      <th>gl_document_date</th>\n",
       "      <th>gl_doc_currency</th>\n",
       "      <th>gl_vendor_id</th>\n",
       "      <th>...</th>\n",
       "      <th>gl_original_document_source</th>\n",
       "      <th>gl_original_document_nr</th>\n",
       "      <th>gl_reference_internal_key_1</th>\n",
       "      <th>gl_document_type_key</th>\n",
       "      <th>gl_debit_credit</th>\n",
       "      <th>multiple_vendors</th>\n",
       "      <th>gl_approver</th>\n",
       "      <th>gl_document_scan_id</th>\n",
       "      <th>multiple_scans</th>\n",
       "      <th>multiple_postings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d3a2c37f81eda88f70ad52d38ce1c</td>\n",
       "      <td>gedruckt auf „steinbeis innojet premiui  ausge...</td>\n",
       "      <td>100</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16620731012651</td>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>EUR</td>\n",
       "      <td>0002000582</td>\n",
       "      <td>...</td>\n",
       "      <td>BKPF</td>\n",
       "      <td>3.200000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100VA</td>\n",
       "      <td>Debit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A40185</td>\n",
       "      <td>000d3a2c37f81eda88f70ad52d38ce1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000d3a2c37f81eda88f70adcd822ee1c</td>\n",
       "      <td>el: { n 1 )) } ad l dds conferencing &amp; caterin...</td>\n",
       "      <td>100</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>DDS-19/20-1010-E</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>EUR</td>\n",
       "      <td>0002449868</td>\n",
       "      <td>...</td>\n",
       "      <td>BKPF</td>\n",
       "      <td>3.200000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100VA</td>\n",
       "      <td>Debit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A40185</td>\n",
       "      <td>000d3a2c37f81eda88f70adcd822ee1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d3a2c37f81eda88f70ab0ea394e1c</td>\n",
       "      <td>zimmermann hauschild notare  lubmin-brandov ga...</td>\n",
       "      <td>100</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Z2856/0/1-2019</td>\n",
       "      <td>2019-12-11</td>\n",
       "      <td>EUR</td>\n",
       "      <td>0002053354</td>\n",
       "      <td>...</td>\n",
       "      <td>BKPF</td>\n",
       "      <td>3.200000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100VA</td>\n",
       "      <td>Debit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A40185</td>\n",
       "      <td>000d3a2c37f81eda88f70ab0ea394e1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000d3a2c37f81eea8cd4c4aa57e474ed</td>\n",
       "      <td>rechnung  uniper global commodities se rechnun...</td>\n",
       "      <td>100</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90063731-993703</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>EUR</td>\n",
       "      <td>0001000092</td>\n",
       "      <td>...</td>\n",
       "      <td>BKPF</td>\n",
       "      <td>3.200000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100VA</td>\n",
       "      <td>Debit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A40185</td>\n",
       "      <td>000d3a2c37f81eea8cd4c4aa57e474ed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000d3a2c37f81eda88f70aafcad7ee1c</td>\n",
       "      <td>= as fri it til vinr  fre  „&gt;  bundesnetzagent...</td>\n",
       "      <td>100</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3.200000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.12.2019</td>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>EUR</td>\n",
       "      <td>0001257969</td>\n",
       "      <td>...</td>\n",
       "      <td>BKPF</td>\n",
       "      <td>3.200000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100VA</td>\n",
       "      <td>Debit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M21419</td>\n",
       "      <td>000d3a2c37f81eda88f70aafcad7ee1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  000d3a2c37f81eda88f70ad52d38ce1c   \n",
       "1  000d3a2c37f81eda88f70adcd822ee1c   \n",
       "2  000d3a2c37f81eda88f70ab0ea394e1c   \n",
       "3  000d3a2c37f81eea8cd4c4aa57e474ed   \n",
       "4  000d3a2c37f81eda88f70aafcad7ee1c   \n",
       "\n",
       "                                                text gl_instance_id  \\\n",
       "0  gedruckt auf „steinbeis innojet premiui  ausge...            100   \n",
       "1  el: { n 1 )) } ad l dds conferencing & caterin...            100   \n",
       "2  zimmermann hauschild notare  lubmin-brandov ga...            100   \n",
       "3  rechnung  uniper global commodities se rechnun...            100   \n",
       "4  = as fri it til vinr  fre  „>  bundesnetzagent...            100   \n",
       "\n",
       "   gl_fiscal_year  gl_header_id  gl_line_nr gl_reference_document_nr  \\\n",
       "0          2020.0  3.200000e+09         2.0           16620731012651   \n",
       "1          2020.0  3.200000e+09         2.0         DDS-19/20-1010-E   \n",
       "2          2020.0  3.200000e+09         2.0           Z2856/0/1-2019   \n",
       "3          2020.0  3.200000e+09         2.0          90063731-993703   \n",
       "4          2020.0  3.200000e+09         2.0               13.12.2019   \n",
       "\n",
       "  gl_document_date gl_doc_currency gl_vendor_id  ...  \\\n",
       "0       2019-12-09             EUR   0002000582  ...   \n",
       "1       2019-12-10             EUR   0002449868  ...   \n",
       "2       2019-12-11             EUR   0002053354  ...   \n",
       "3       2020-01-08             EUR   0001000092  ...   \n",
       "4       2019-12-09             EUR   0001257969  ...   \n",
       "\n",
       "  gl_original_document_source gl_original_document_nr  \\\n",
       "0                        BKPF            3.200000e+17   \n",
       "1                        BKPF            3.200000e+17   \n",
       "2                        BKPF            3.200000e+17   \n",
       "3                        BKPF            3.200000e+17   \n",
       "4                        BKPF            3.200000e+17   \n",
       "\n",
       "  gl_reference_internal_key_1 gl_document_type_key gl_debit_credit  \\\n",
       "0                         NaN                100VA           Debit   \n",
       "1                         NaN                100VA           Debit   \n",
       "2                         NaN                100VA           Debit   \n",
       "3                         NaN                100VA           Debit   \n",
       "4                         NaN                100VA           Debit   \n",
       "\n",
       "   multiple_vendors  gl_approver               gl_document_scan_id  \\\n",
       "0               NaN       A40185  000d3a2c37f81eda88f70ad52d38ce1c   \n",
       "1               NaN       A40185  000d3a2c37f81eda88f70adcd822ee1c   \n",
       "2               NaN       A40185  000d3a2c37f81eda88f70ab0ea394e1c   \n",
       "3               NaN       A40185  000d3a2c37f81eea8cd4c4aa57e474ed   \n",
       "4               NaN       M21419  000d3a2c37f81eda88f70aafcad7ee1c   \n",
       "\n",
       "  multiple_scans multiple_postings  \n",
       "0            NaN               NaN  \n",
       "1            NaN               NaN  \n",
       "2            NaN               NaN  \n",
       "3            NaN               NaN  \n",
       "4            NaN               NaN  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gl_id = get_reduced_df(df, 'gl_accounts_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 28168 samples from 176 relevant classes. (N=5)\n",
      "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n"
     ]
    }
   ],
   "source": [
    "label = 'gl_accounts_id' # define label here\n",
    "x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_gl_id, label)\n",
    "\n",
    "lr = LogisticRegression(random_state=1) #'C':[1, 0.95, 0.9]\n",
    "parameters = {'penalty': ['none', 'elasticnet'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "model = GridSearchCV(lr, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
    "#model.get_params().keys()\n",
    "train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_legal_entity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'gl_legal_entity_id' # define label here\n",
    "x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_gl_id, label)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "parameters = {'penalty': ['none', 'l2', 'elasticnet'], 'C':[1, 0.95, 0.9], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "model = GridSearchCV(lr, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
    "#model.get_params().keys()\n",
    "train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  start training for attribute: gl_legal_entity_id  ---------- \n",
      "\n",
      "Reduced to 33631 samples from 22 relevant classes. (N=5)\n",
      "Accuracy is 0.9736881224914523.\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0037       1.00      0.98      0.99        58\n",
      "        0065       1.00      1.00      1.00       358\n",
      "        0301       0.96      0.99      0.97      1839\n",
      "        0303       0.99      1.00      0.99      1706\n",
      "        0330       0.94      0.97      0.95       374\n",
      "        0362       1.00      0.94      0.97       113\n",
      "        0377       1.00      0.93      0.96        28\n",
      "        0601       1.00      0.98      0.99       442\n",
      "        0801       0.98      0.96      0.97       945\n",
      "        0806       0.98      0.89      0.93        47\n",
      "        0811       1.00      0.47      0.64        17\n",
      "        0821       0.98      0.98      0.98       211\n",
      "        2101       0.98      0.99      0.99       262\n",
      "        2111       0.99      0.97      0.98       118\n",
      "        3104       0.80      0.43      0.56        37\n",
      "        3401       0.91      0.83      0.87        12\n",
      "        3420       1.00      0.93      0.96        56\n",
      "        9301       0.50      0.33      0.40         3\n",
      "        9310       0.82      0.60      0.69        47\n",
      "        9351       0.85      0.77      0.81        30\n",
      "        9370       0.90      0.75      0.82        24\n",
      "\n",
      "    accuracy                           0.97      6727\n",
      "   macro avg       0.93      0.84      0.88      6727\n",
      "weighted avg       0.97      0.97      0.97      6727\n",
      "\n",
      "----------  created histogram  ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\pandas\\core\\indexing.py:691: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created word report  ---------- \n",
      "\n",
      "----------  saved clf to pkl and zip  ---------- \n",
      "\n",
      "----------  finished training for attribute: gl_legal_entity_id  ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clf_and_create_reports(\n",
    "    df,\n",
    "    'gl_legal_entity_id',\n",
    "    SCAN_ID_COL,\n",
    "    RES_DIR,\n",
    "    TODAY,\n",
    "    CLIENT,\n",
    "    COUNTRY,\n",
    "    MIN_NUM_OF_SAMPLES,\n",
    "    histogram=True,\n",
    "    word_report=True,\n",
    "    predictions_excel=PREDICTIONS_EXCEL,\n",
    "    save_clf=SAVE_CLFS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_tax_code_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'gl_tax_code_id' # define label here\n",
    "x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_gl_id, label)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "parameters = {'penalty': ['none', 'l2', 'elasticnet'], 'C':[1, 0.95, 0.9], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "model = GridSearchCV(lr, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
    "#model.get_params().keys()\n",
    "train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  start training for attribute: gl_tax_code_id  ---------- \n",
      "\n",
      "Reduced to 32127 samples from 31 relevant classes. (N=5)\n",
      "Accuracy is 0.9097416744475568.\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0W       1.00      0.86      0.92        14\n",
      "          11       0.97      0.89      0.93        38\n",
      "          51       0.90      0.99      0.94      4166\n",
      "          52       0.95      0.65      0.77       655\n",
      "          53       0.81      0.52      0.64       157\n",
      "          54       0.00      0.00      0.00         1\n",
      "          56       0.75      0.38      0.50        16\n",
      "          80       0.96      0.98      0.97       525\n",
      "          99       0.96      0.78      0.86       529\n",
      "          A1       1.00      0.83      0.91         6\n",
      "          B2       0.00      0.00      0.00         0\n",
      "          C2       0.84      0.83      0.84        59\n",
      "          C3       1.00      0.52      0.68        27\n",
      "          C6       1.00      1.00      1.00        32\n",
      "          C8       0.00      0.00      0.00         4\n",
      "          CB       0.67      0.29      0.40         7\n",
      "          CH       0.88      0.83      0.85        46\n",
      "          CN       0.00      0.00      0.00         1\n",
      "          E1       0.92      0.61      0.73        18\n",
      "          E2       0.62      0.50      0.56        10\n",
      "          HK       0.83      0.91      0.87        11\n",
      "          HM       1.00      1.00      1.00         3\n",
      "          HN       1.00      0.67      0.80         3\n",
      "          HW       0.00      0.00      0.00         2\n",
      "          J1       0.83      1.00      0.91         5\n",
      "          JA       1.00      1.00      1.00         4\n",
      "          N0       1.00      0.14      0.25         7\n",
      "          N1       0.40      0.25      0.31        16\n",
      "          ND       0.00      0.00      0.00         5\n",
      "          R1       0.87      0.93      0.90        59\n",
      "\n",
      "    accuracy                           0.91      6426\n",
      "   macro avg       0.71      0.58      0.62      6426\n",
      "weighted avg       0.91      0.91      0.90      6426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created histogram  ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\pandas\\core\\indexing.py:691: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created word report  ---------- \n",
      "\n",
      "----------  saved clf to pkl and zip  ---------- \n",
      "\n",
      "----------  finished training for attribute: gl_tax_code_id  ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clf_and_create_reports(\n",
    "    df,\n",
    "    'gl_tax_code_id',\n",
    "    SCAN_ID_COL,\n",
    "    RES_DIR,\n",
    "    TODAY,\n",
    "    CLIENT,\n",
    "    COUNTRY,\n",
    "    MIN_NUM_OF_SAMPLES,\n",
    "    histogram=True,\n",
    "    word_report=True,\n",
    "    predictions_excel=PREDICTIONS_EXCEL,\n",
    "    save_clf=SAVE_CLFS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_vendor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'gl_vendor_id' # define label here\n",
    "x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_gl_id, label)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "parameters = {'penalty': ['none', 'l2', 'elasticnet'], 'C':[1, 0.95, 0.9], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "model = GridSearchCV(lr, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
    "#model.get_params().keys()\n",
    "train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  start training for attribute: gl_vendor_id  ---------- \n",
      "\n",
      "Reduced to 28139 samples from 876 relevant classes. (N=5)\n",
      "Accuracy is 0.9792110874200426.\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  0001000007       1.00      0.80      0.89        10\n",
      "  0001000008       1.00      1.00      1.00         3\n",
      "  0001000074       1.00      1.00      1.00        65\n",
      "  0001000075       1.00      1.00      1.00         4\n",
      "  0001000085       1.00      1.00      1.00        29\n",
      "  0001000089       1.00      1.00      1.00       333\n",
      "  0001000092       1.00      1.00      1.00       100\n",
      "  0001000275       1.00      1.00      1.00        22\n",
      "  0001000281       1.00      1.00      1.00         6\n",
      "  0001000282       1.00      1.00      1.00        11\n",
      "  0001000283       1.00      1.00      1.00        12\n",
      "  0001000284       1.00      1.00      1.00        32\n",
      "  0001000305       1.00      1.00      1.00         4\n",
      "  0001000538       1.00      1.00      1.00         1\n",
      "  0001001026       0.89      0.94      0.92        18\n",
      "  0001001264       1.00      1.00      1.00         4\n",
      "  0001001346       1.00      1.00      1.00         1\n",
      "  0001100007       1.00      1.00      1.00         1\n",
      "  0001100101       1.00      1.00      1.00        10\n",
      "  0001101192       1.00      1.00      1.00        30\n",
      "  0001201450       1.00      1.00      1.00         1\n",
      "  0001201623       1.00      1.00      1.00         1\n",
      "  0001201677       1.00      1.00      1.00         2\n",
      "  0001201694       1.00      1.00      1.00         4\n",
      "  0001201891       0.50      0.50      0.50         2\n",
      "  0001201938       1.00      1.00      1.00         4\n",
      "  0001201939       1.00      1.00      1.00         1\n",
      "  0001202008       1.00      1.00      1.00         1\n",
      "  0001202088       1.00      1.00      1.00         6\n",
      "  0001202173       1.00      1.00      1.00         1\n",
      "  0001202295       1.00      1.00      1.00         1\n",
      "  0001202480       1.00      1.00      1.00         1\n",
      "  0001202577       1.00      0.92      0.96        13\n",
      "  0001202584       1.00      1.00      1.00         5\n",
      "  0001202908       0.33      0.20      0.25         5\n",
      "  0001204449       1.00      1.00      1.00         2\n",
      "  0001204462       1.00      1.00      1.00         2\n",
      "  0001205125       1.00      1.00      1.00         2\n",
      "  0001205764       1.00      0.67      0.80         3\n",
      "  0001205946       1.00      1.00      1.00         1\n",
      "  0001205947       1.00      1.00      1.00         2\n",
      "  0001206116       1.00      1.00      1.00         2\n",
      "  0001206190       1.00      1.00      1.00         3\n",
      "  0001206249       1.00      1.00      1.00         7\n",
      "  0001206340       1.00      1.00      1.00         4\n",
      "  0001206343       1.00      1.00      1.00         3\n",
      "  0001206378       0.67      0.80      0.73         5\n",
      "  0001206758       1.00      1.00      1.00         1\n",
      "  0001207427       1.00      1.00      1.00         1\n",
      "  0001211924       1.00      0.67      0.80         3\n",
      "  0001213502       1.00      1.00      1.00         4\n",
      "  0001216927       1.00      1.00      1.00         1\n",
      "  0001217006       1.00      1.00      1.00         2\n",
      "  0001252116       1.00      1.00      1.00         8\n",
      "  0001252410       1.00      1.00      1.00         4\n",
      "  0001252550       1.00      1.00      1.00         3\n",
      "  0001252721       1.00      1.00      1.00         5\n",
      "  0001254504       0.00      0.00      0.00         0\n",
      "  0001254508       1.00      1.00      1.00         1\n",
      "  0001256737       0.73      0.85      0.79        13\n",
      "  0001258734       0.77      1.00      0.87        17\n",
      "  0001259486       0.00      0.00      0.00         1\n",
      "  0001259516       1.00      1.00      1.00         1\n",
      "  0002000036       1.00      1.00      1.00        49\n",
      "  0002000040       0.00      0.00      0.00         3\n",
      "  0002000046       0.00      0.00      0.00         3\n",
      "  0002000051       1.00      1.00      1.00         1\n",
      "  0002000095       1.00      0.50      0.67         2\n",
      "  0002000169       1.00      1.00      1.00         1\n",
      "  0002000243       1.00      1.00      1.00         4\n",
      "  0002000551       1.00      1.00      1.00        15\n",
      "  0002000562       1.00      1.00      1.00        29\n",
      "  0002000566       1.00      1.00      1.00         5\n",
      "  0002000568       1.00      1.00      1.00        19\n",
      "  0002000569       1.00      1.00      1.00         2\n",
      "  0002000575       1.00      1.00      1.00         4\n",
      "  0002000577       1.00      1.00      1.00         4\n",
      "  0002000580       1.00      1.00      1.00         1\n",
      "  0002000582       1.00      1.00      1.00       161\n",
      "  0002000587       1.00      1.00      1.00        80\n",
      "  0002000596       1.00      1.00      1.00        62\n",
      "  0002000598       1.00      1.00      1.00         5\n",
      "  0002000602       1.00      1.00      1.00        58\n",
      "  0002000612       1.00      1.00      1.00         2\n",
      "  0002000618       1.00      1.00      1.00        15\n",
      "  0002000619       1.00      1.00      1.00         3\n",
      "  0002000629       1.00      1.00      1.00         1\n",
      "  0002000636       1.00      1.00      1.00         2\n",
      "  0002000671       1.00      1.00      1.00         1\n",
      "  0002000677       0.71      0.93      0.81        45\n",
      "  0002000694       1.00      1.00      1.00         8\n",
      "  0002000698       1.00      1.00      1.00         2\n",
      "  0002000700       1.00      1.00      1.00         6\n",
      "  0002000740       1.00      1.00      1.00        10\n",
      "  0002000743       1.00      1.00      1.00        10\n",
      "  0002000751       1.00      1.00      1.00        10\n",
      "  0002000989       1.00      1.00      1.00        11\n",
      "  0002000990       1.00      1.00      1.00         1\n",
      "  0002001129       1.00      1.00      1.00         6\n",
      "  0002001147       1.00      1.00      1.00         3\n",
      "  0002001185       1.00      1.00      1.00         4\n",
      "  0002001335       1.00      1.00      1.00         2\n",
      "  0002001345       1.00      1.00      1.00         1\n",
      "  0002001356       1.00      1.00      1.00         3\n",
      "  0002001453       1.00      1.00      1.00         5\n",
      "  0002001611       1.00      1.00      1.00         1\n",
      "  0002001645       1.00      1.00      1.00         2\n",
      "  0002001873       1.00      1.00      1.00         3\n",
      "  0002001927       1.00      1.00      1.00         6\n",
      "  0002001932       1.00      1.00      1.00         2\n",
      "  0002001939       1.00      1.00      1.00         4\n",
      "  0002001946       1.00      1.00      1.00         4\n",
      "  0002002008       1.00      1.00      1.00         1\n",
      "  0002002016       1.00      1.00      1.00         4\n",
      "  0002002026       1.00      1.00      1.00        25\n",
      "  0002002029       1.00      1.00      1.00         3\n",
      "  0002002041       1.00      1.00      1.00         5\n",
      "  0002002042       1.00      1.00      1.00         2\n",
      "  0002002065       1.00      1.00      1.00         1\n",
      "  0002002070       1.00      1.00      1.00         6\n",
      "  0002002122       1.00      1.00      1.00         4\n",
      "  0002002226       1.00      1.00      1.00        18\n",
      "  0002002334       1.00      1.00      1.00         1\n",
      "  0002002626       1.00      1.00      1.00         2\n",
      "  0002002893       1.00      1.00      1.00        17\n",
      "  0002002901       1.00      1.00      1.00         1\n",
      "  0002002921       1.00      0.92      0.96        39\n",
      "  0002002922       1.00      1.00      1.00         1\n",
      "  0002002957       1.00      1.00      1.00         2\n",
      "  0002002974       1.00      1.00      1.00        12\n",
      "  0002002997       1.00      1.00      1.00        57\n",
      "  0002003033       1.00      1.00      1.00         1\n",
      "  0002003129       1.00      1.00      1.00         1\n",
      "  0002003132       1.00      1.00      1.00         3\n",
      "  0002003135       1.00      1.00      1.00         4\n",
      "  0002003166       0.64      0.64      0.64        14\n",
      "  0002003167       1.00      1.00      1.00         1\n",
      "  0002003183       1.00      1.00      1.00         1\n",
      "  0002003188       1.00      1.00      1.00         1\n",
      "  0002003280       1.00      1.00      1.00         1\n",
      "  0002003439       1.00      1.00      1.00         1\n",
      "  0002003634       1.00      1.00      1.00         3\n",
      "  0002003637       1.00      1.00      1.00         1\n",
      "  0002003639       0.83      1.00      0.91         5\n",
      "  0002003818       1.00      1.00      1.00         2\n",
      "  0002003869       1.00      1.00      1.00         4\n",
      "  0002003918       1.00      1.00      1.00         2\n",
      "  0002003945       1.00      1.00      1.00         4\n",
      "  0002004103       1.00      1.00      1.00         3\n",
      "  0002004181       0.97      1.00      0.98        29\n",
      "  0002004211       1.00      1.00      1.00         3\n",
      "  0002004264       1.00      1.00      1.00         3\n",
      "  0002004343       1.00      1.00      1.00         4\n",
      "  0002004373       1.00      1.00      1.00         2\n",
      "  0002004387       1.00      0.86      0.92         7\n",
      "  0002004430       1.00      1.00      1.00         9\n",
      "  0002004477       1.00      1.00      1.00         3\n",
      "  0002004499       1.00      1.00      1.00         2\n",
      "  0002004647       1.00      1.00      1.00         1\n",
      "  0002004856       1.00      1.00      1.00         3\n",
      "  0002005048       1.00      1.00      1.00         5\n",
      "  0002005148       1.00      1.00      1.00         5\n",
      "  0002005181       1.00      1.00      1.00        23\n",
      "  0002005230       1.00      1.00      1.00         4\n",
      "  0002005384       1.00      1.00      1.00         1\n",
      "  0002005414       1.00      1.00      1.00        12\n",
      "  0002005457       1.00      1.00      1.00         6\n",
      "  0002005462       1.00      1.00      1.00         6\n",
      "  0002005643       1.00      1.00      1.00         3\n",
      "  0002005729       0.00      0.00      0.00         1\n",
      "  0002005895       1.00      1.00      1.00         4\n",
      "  0002005976       1.00      1.00      1.00         3\n",
      "  0002005978       1.00      1.00      1.00         1\n",
      "  0002005996       1.00      1.00      1.00        15\n",
      "  0002006044       1.00      1.00      1.00         1\n",
      "  0002006131       1.00      1.00      1.00         4\n",
      "  0002006228       0.62      1.00      0.77         5\n",
      "  0002006339       1.00      0.50      0.67         4\n",
      "  0002006357       1.00      1.00      1.00         2\n",
      "  0002006580       1.00      1.00      1.00         6\n",
      "  0002006836       0.67      1.00      0.80         2\n",
      "  0002010650       1.00      1.00      1.00         1\n",
      "  0002010803       0.86      1.00      0.92         6\n",
      "  0002011186       1.00      1.00      1.00         5\n",
      "  0002012376       1.00      1.00      1.00         2\n",
      "  0002012619       1.00      1.00      1.00         3\n",
      "  0002012671       1.00      1.00      1.00         1\n",
      "  0002012716       1.00      1.00      1.00         2\n",
      "  0002014696       1.00      1.00      1.00         6\n",
      "  0002017072       1.00      1.00      1.00         1\n",
      "  0002017079       0.80      1.00      0.89         4\n",
      "  0002018329       1.00      1.00      1.00         1\n",
      "  0002018618       1.00      1.00      1.00         5\n",
      "  0002019030       1.00      1.00      1.00         1\n",
      "  0002019232       1.00      1.00      1.00        17\n",
      "  0002019256       1.00      1.00      1.00         1\n",
      "  0002019467       1.00      1.00      1.00         4\n",
      "  0002019901       1.00      1.00      1.00         3\n",
      "  0002019934       1.00      1.00      1.00         2\n",
      "  0002019976       1.00      1.00      1.00         1\n",
      "  0002020412       1.00      1.00      1.00         1\n",
      "  0002020830       1.00      1.00      1.00         8\n",
      "  0002021352       1.00      1.00      1.00         1\n",
      "  0002021374       1.00      1.00      1.00         2\n",
      "  0002021474       1.00      1.00      1.00         2\n",
      "  0002021608       1.00      1.00      1.00        14\n",
      "  0002021631       1.00      1.00      1.00        16\n",
      "  0002021773       1.00      1.00      1.00         3\n",
      "  0002022321       1.00      1.00      1.00         1\n",
      "  0002022564       1.00      1.00      1.00         1\n",
      "  0002022962       0.67      1.00      0.80         2\n",
      "  0002023086       1.00      1.00      1.00         3\n",
      "  0002023190       1.00      1.00      1.00         1\n",
      "  0002023201       1.00      1.00      1.00         2\n",
      "  0002023492       1.00      1.00      1.00         2\n",
      "  0002023504       1.00      1.00      1.00         3\n",
      "  0002023866       1.00      1.00      1.00         3\n",
      "  0002024269       1.00      1.00      1.00         2\n",
      "  0002024282       1.00      1.00      1.00         7\n",
      "  0002024295       1.00      1.00      1.00         2\n",
      "  0002024310       1.00      1.00      1.00         4\n",
      "  0002024669       1.00      1.00      1.00         3\n",
      "  0002024736       1.00      1.00      1.00         2\n",
      "  0002024957       1.00      1.00      1.00        13\n",
      "  0002025129       1.00      1.00      1.00         2\n",
      "  0002025220       1.00      1.00      1.00         1\n",
      "  0002025303       1.00      1.00      1.00         1\n",
      "  0002025642       1.00      1.00      1.00         1\n",
      "  0002025644       1.00      1.00      1.00         2\n",
      "  0002025664       1.00      1.00      1.00         6\n",
      "  0002025753       1.00      1.00      1.00         1\n",
      "  0002025754       1.00      0.75      0.86         4\n",
      "  0002025827       1.00      1.00      1.00         1\n",
      "  0002025830       1.00      1.00      1.00         1\n",
      "  0002025930       1.00      1.00      1.00         5\n",
      "  0002025977       1.00      1.00      1.00         2\n",
      "  0002026088       0.00      0.00      0.00         0\n",
      "  0002026111       1.00      1.00      1.00         3\n",
      "  0002026299       1.00      1.00      1.00         6\n",
      "  0002026310       1.00      1.00      1.00        22\n",
      "  0002026385       1.00      1.00      1.00         2\n",
      "  0002026395       1.00      1.00      1.00         1\n",
      "  0002026426       0.75      1.00      0.86         3\n",
      "  0002026460       1.00      1.00      1.00        27\n",
      "  0002026521       1.00      1.00      1.00         2\n",
      "  0002026532       1.00      1.00      1.00         5\n",
      "  0002026630       1.00      1.00      1.00         2\n",
      "  0002026708       1.00      1.00      1.00         1\n",
      "  0002026724       1.00      1.00      1.00         1\n",
      "  0002026809       1.00      1.00      1.00         1\n",
      "  0002026881       0.67      0.29      0.40        14\n",
      "  0002026910       1.00      1.00      1.00         5\n",
      "  0002026995       1.00      1.00      1.00        15\n",
      "  0002027112       1.00      0.83      0.91         6\n",
      "  0002027119       1.00      1.00      1.00         3\n",
      "  0002027187       1.00      1.00      1.00         7\n",
      "  0002027256       1.00      1.00      1.00         8\n",
      "  0002027395       1.00      1.00      1.00         2\n",
      "  0002027457       1.00      1.00      1.00         2\n",
      "  0002027460       1.00      1.00      1.00         7\n",
      "  0002027486       1.00      1.00      1.00         4\n",
      "  0002027622       1.00      1.00      1.00         2\n",
      "  0002027837       1.00      1.00      1.00         4\n",
      "  0002027943       0.00      0.00      0.00         2\n",
      "  0002028112       1.00      1.00      1.00        87\n",
      "  0002028199       1.00      1.00      1.00         2\n",
      "  0002028521       1.00      1.00      1.00         6\n",
      "  0002028596       1.00      1.00      1.00         7\n",
      "  0002028690       1.00      1.00      1.00         4\n",
      "  0002028707       1.00      1.00      1.00         1\n",
      "  0002028836       1.00      1.00      1.00         1\n",
      "  0002028841       1.00      1.00      1.00         4\n",
      "  0002029283       1.00      1.00      1.00        20\n",
      "  0002029682       0.00      0.00      0.00         1\n",
      "  0002029716       1.00      1.00      1.00         2\n",
      "  0002029727       1.00      1.00      1.00         1\n",
      "  0002029757       1.00      1.00      1.00        11\n",
      "  0002029764       0.86      0.75      0.80         8\n",
      "  0002029765       1.00      1.00      1.00         3\n",
      "  0002029796       1.00      1.00      1.00        15\n",
      "  0002029811       1.00      1.00      1.00         2\n",
      "  0002029903       1.00      1.00      1.00         1\n",
      "  0002029909       1.00      1.00      1.00         1\n",
      "  0002029955       1.00      0.67      0.80         3\n",
      "  0002029987       1.00      1.00      1.00         2\n",
      "  0002029996       1.00      1.00      1.00         2\n",
      "  0002029998       1.00      1.00      1.00         4\n",
      "  0002029999       1.00      1.00      1.00        10\n",
      "  0002030026       1.00      1.00      1.00         1\n",
      "  0002030075       1.00      1.00      1.00         3\n",
      "  0002030084       1.00      1.00      1.00         3\n",
      "  0002030130       1.00      1.00      1.00         5\n",
      "  0002030270       1.00      1.00      1.00         1\n",
      "  0002030357       1.00      1.00      1.00         1\n",
      "  0002030491       1.00      1.00      1.00         1\n",
      "  0002030502       1.00      1.00      1.00         7\n",
      "  0002030814       1.00      1.00      1.00         3\n",
      "  0002031248       1.00      1.00      1.00         2\n",
      "  0002031254       1.00      1.00      1.00         2\n",
      "  0002031289       1.00      1.00      1.00         2\n",
      "  0002031457       1.00      1.00      1.00        12\n",
      "  0002032691       1.00      1.00      1.00         2\n",
      "  0002032797       1.00      1.00      1.00         2\n",
      "  0002033599       1.00      1.00      1.00         4\n",
      "  0002041949       0.00      0.00      0.00         0\n",
      "  0002042336       1.00      1.00      1.00         1\n",
      "  0002042648       1.00      1.00      1.00         1\n",
      "  0002042814       1.00      1.00      1.00        89\n",
      "  0002044525       1.00      1.00      1.00         1\n",
      "  0002044737       1.00      1.00      1.00         1\n",
      "  0002044888       1.00      1.00      1.00        34\n",
      "  0002044941       1.00      1.00      1.00         1\n",
      "  0002045444       0.00      0.00      0.00         2\n",
      "  0002046469       0.96      1.00      0.98        53\n",
      "  0002046740       1.00      1.00      1.00         2\n",
      "  0002046900       1.00      1.00      1.00         2\n",
      "  0002047093       0.00      0.00      0.00         1\n",
      "  0002047330       1.00      1.00      1.00         1\n",
      "  0002048667       1.00      1.00      1.00         1\n",
      "  0002051243       1.00      1.00      1.00        34\n",
      "  0002051345       0.75      1.00      0.86         3\n",
      "  0002052025       1.00      1.00      1.00         1\n",
      "  0002052947       1.00      1.00      1.00         4\n",
      "  0002053073       1.00      1.00      1.00         3\n",
      "  0002053354       1.00      1.00      1.00        20\n",
      "  0002054103       1.00      1.00      1.00         3\n",
      "  0002054723       1.00      1.00      1.00         2\n",
      "  0002054727       1.00      1.00      1.00         2\n",
      "  0002055107       1.00      1.00      1.00         1\n",
      "  0002055413       0.00      0.00      0.00         1\n",
      "  0002055746       1.00      1.00      1.00         3\n",
      "  0002055804       1.00      1.00      1.00         1\n",
      "  0002055917       1.00      1.00      1.00         1\n",
      "  0002056379       1.00      1.00      1.00         2\n",
      "  0002056518       1.00      1.00      1.00        15\n",
      "  0002056533       1.00      1.00      1.00         1\n",
      "  0002056792       1.00      1.00      1.00         1\n",
      "  0002056848       1.00      1.00      1.00         1\n",
      "  0002056858       0.50      0.50      0.50         2\n",
      "  0002056872       1.00      1.00      1.00        46\n",
      "  0002056875       1.00      1.00      1.00         2\n",
      "  0002056902       1.00      1.00      1.00         8\n",
      "  0002056986       1.00      1.00      1.00         1\n",
      "  0002056990       1.00      1.00      1.00         5\n",
      "  0002057000       1.00      1.00      1.00         1\n",
      "  0002057052       1.00      1.00      1.00         5\n",
      "  0002057069       1.00      1.00      1.00         2\n",
      "  0002057127       1.00      1.00      1.00         2\n",
      "  0002057254       1.00      1.00      1.00         8\n",
      "  0002057263       1.00      1.00      1.00         1\n",
      "  0002057295       1.00      1.00      1.00         1\n",
      "  0002057307       1.00      1.00      1.00         1\n",
      "  0002057394       1.00      1.00      1.00         4\n",
      "  0002057424       1.00      1.00      1.00        14\n",
      "  0002057444       1.00      1.00      1.00         2\n",
      "  0002057456       1.00      1.00      1.00         2\n",
      "  0002057472       1.00      1.00      1.00         4\n",
      "  0002057515       1.00      1.00      1.00         2\n",
      "  0002057577       1.00      1.00      1.00        14\n",
      "  0002057584       1.00      1.00      1.00         1\n",
      "  0002057610       1.00      1.00      1.00         1\n",
      "  0002057626       1.00      1.00      1.00        10\n",
      "  0002057654       1.00      1.00      1.00         5\n",
      "  0002057759       1.00      1.00      1.00        17\n",
      "  0002057783       1.00      1.00      1.00         3\n",
      "  0002057785       1.00      1.00      1.00         1\n",
      "  0002057883       1.00      1.00      1.00        30\n",
      "  0002057906       1.00      1.00      1.00         6\n",
      "  0002057970       1.00      1.00      1.00         7\n",
      "  0002057986       1.00      1.00      1.00         4\n",
      "  0002058022       1.00      1.00      1.00         5\n",
      "  0002058111       1.00      1.00      1.00         1\n",
      "  0002058328       1.00      1.00      1.00         1\n",
      "  0002058393       1.00      1.00      1.00        15\n",
      "  0002058423       0.40      0.67      0.50         3\n",
      "  0002058502       1.00      1.00      1.00        19\n",
      "  0002058664       1.00      1.00      1.00         1\n",
      "  0002058702       1.00      1.00      1.00         1\n",
      "  0002058722       1.00      1.00      1.00        14\n",
      "  0002058955       1.00      1.00      1.00       112\n",
      "  0002058993       1.00      1.00      1.00         6\n",
      "  0002059046       1.00      1.00      1.00         3\n",
      "  0002059095       1.00      1.00      1.00         5\n",
      "  0002059226       1.00      1.00      1.00         4\n",
      "  0002059365       1.00      1.00      1.00         2\n",
      "  0002059405       0.00      0.00      0.00         2\n",
      "  0002059432       1.00      1.00      1.00         2\n",
      "  0002059643       1.00      1.00      1.00         2\n",
      "  0002059744       1.00      1.00      1.00         1\n",
      "  0002059748       1.00      1.00      1.00         2\n",
      "  0002059797       1.00      0.33      0.50         3\n",
      "  0002059849       1.00      1.00      1.00         3\n",
      "  0002060295       1.00      1.00      1.00         2\n",
      "  0002060711       0.00      0.00      0.00         0\n",
      "  0002061259       1.00      1.00      1.00         1\n",
      "  0002061276       1.00      1.00      1.00         5\n",
      "  0002061394       1.00      1.00      1.00         1\n",
      "  0002061516       1.00      1.00      1.00         5\n",
      "  0002062749       1.00      1.00      1.00         2\n",
      "  0002062771       1.00      1.00      1.00         6\n",
      "  0002063013       1.00      1.00      1.00         1\n",
      "  0002063027       1.00      1.00      1.00         3\n",
      "  0002063438       1.00      1.00      1.00         7\n",
      "  0002063453       1.00      1.00      1.00        29\n",
      "  0002063481       1.00      1.00      1.00         2\n",
      "  0002063539       1.00      1.00      1.00         1\n",
      "  0002063577       1.00      1.00      1.00         9\n",
      "  0002063607       1.00      1.00      1.00         5\n",
      "  0002063634       1.00      1.00      1.00         5\n",
      "  0002063640       1.00      1.00      1.00         1\n",
      "  0002063657       1.00      1.00      1.00         7\n",
      "  0002063662       1.00      1.00      1.00         1\n",
      "  0002063734       1.00      1.00      1.00         4\n",
      "  0002063751       1.00      1.00      1.00         1\n",
      "  0002063760       1.00      1.00      1.00         2\n",
      "  0002063830       1.00      1.00      1.00         5\n",
      "  0002063849       1.00      1.00      1.00        77\n",
      "  0002063850       1.00      1.00      1.00         1\n",
      "  0002063851       1.00      1.00      1.00         4\n",
      "  0002063859       1.00      1.00      1.00         1\n",
      "  0002063863       0.88      1.00      0.93         7\n",
      "  0002064011       1.00      1.00      1.00        13\n",
      "  0002064049       1.00      1.00      1.00         1\n",
      "  0002064153       1.00      1.00      1.00         2\n",
      "  0002064201       1.00      1.00      1.00         8\n",
      "  0002064213       1.00      1.00      1.00         2\n",
      "  0002064305       1.00      1.00      1.00         2\n",
      "  0002064310       1.00      1.00      1.00         8\n",
      "  0002064476       1.00      1.00      1.00         3\n",
      "  0002064594       1.00      1.00      1.00         1\n",
      "  0002064702       1.00      1.00      1.00         1\n",
      "  0002065569       1.00      1.00      1.00         2\n",
      "  0002065573       1.00      1.00      1.00         5\n",
      "  0002065642       1.00      1.00      1.00         2\n",
      "  0002065737       1.00      1.00      1.00        17\n",
      "  0002066707       1.00      1.00      1.00         5\n",
      "  0002067588       1.00      1.00      1.00         1\n",
      "  0002067616       1.00      1.00      1.00         3\n",
      "  0002068393       1.00      1.00      1.00         2\n",
      "  0002068417       1.00      1.00      1.00         2\n",
      "  0002068444       1.00      1.00      1.00        10\n",
      "  0002068537       1.00      1.00      1.00         2\n",
      "  0002068551       0.25      1.00      0.40         1\n",
      "  0002068617       1.00      1.00      1.00         2\n",
      "  0002068640       1.00      1.00      1.00         1\n",
      "  0002068674       1.00      1.00      1.00         1\n",
      "  0002068704       1.00      1.00      1.00         2\n",
      "  0002068743       1.00      1.00      1.00         2\n",
      "  0002068756       1.00      1.00      1.00         2\n",
      "  0002068799       1.00      1.00      1.00         5\n",
      "  0002068801       1.00      1.00      1.00         1\n",
      "  0002068802       1.00      1.00      1.00         2\n",
      "  0002068808       0.50      1.00      0.67         1\n",
      "  0002068983       1.00      1.00      1.00         4\n",
      "  0002069044       1.00      1.00      1.00         1\n",
      "  0002069238       1.00      1.00      1.00         2\n",
      "  0002069694       1.00      1.00      1.00         4\n",
      "  0002069699       1.00      1.00      1.00         8\n",
      "  0002069710       1.00      1.00      1.00        86\n",
      "  0002069823       1.00      1.00      1.00         1\n",
      "  0002069867       1.00      1.00      1.00         8\n",
      "  0002069888       1.00      1.00      1.00         2\n",
      "  0002072082       1.00      1.00      1.00         1\n",
      "  0002076469       1.00      1.00      1.00         6\n",
      "  0002076576       0.80      0.80      0.80         5\n",
      "  0002076643       1.00      1.00      1.00         3\n",
      "  0002083872       1.00      1.00      1.00         1\n",
      "  0002091277       1.00      1.00      1.00         2\n",
      "  0002091689       1.00      1.00      1.00         3\n",
      "  0002091721       0.00      0.00      0.00         2\n",
      "  0002140933       1.00      1.00      1.00         1\n",
      "  0002141216       1.00      1.00      1.00         2\n",
      "  0002141403       1.00      1.00      1.00        13\n",
      "  0002141571       1.00      1.00      1.00         3\n",
      "  0002143365       1.00      1.00      1.00         5\n",
      "  0002144771       0.00      0.00      0.00         0\n",
      "  0002144938       1.00      1.00      1.00         1\n",
      "  0002144998       1.00      1.00      1.00         3\n",
      "  0002145919       1.00      1.00      1.00         1\n",
      "  0002146888       1.00      1.00      1.00         2\n",
      "  0002149142       1.00      1.00      1.00         6\n",
      "  0002149621       1.00      1.00      1.00         2\n",
      "  0002150095       1.00      1.00      1.00         1\n",
      "  0002150689       1.00      1.00      1.00         6\n",
      "  0002150930       1.00      1.00      1.00         1\n",
      "  0002155828       1.00      1.00      1.00         3\n",
      "  0002157040       1.00      1.00      1.00         1\n",
      "  0002158924       1.00      1.00      1.00         3\n",
      "  0002159079       1.00      1.00      1.00         2\n",
      "  0002159174       1.00      0.50      0.67         2\n",
      "  0002159842       1.00      1.00      1.00         2\n",
      "  0002160852       1.00      1.00      1.00         4\n",
      "  0002178847       0.00      0.00      0.00         0\n",
      "  0002189615       1.00      1.00      1.00         5\n",
      "  0002189874       1.00      1.00      1.00         2\n",
      "  0002195862       1.00      1.00      1.00        19\n",
      "  0002199845       1.00      1.00      1.00        21\n",
      "  0002200319       1.00      1.00      1.00        13\n",
      "  0002238567       1.00      1.00      1.00         2\n",
      "  0002241002       1.00      1.00      1.00         3\n",
      "  0002242575       1.00      1.00      1.00         1\n",
      "  0002245205       1.00      1.00      1.00         2\n",
      "  0002253796       1.00      1.00      1.00         1\n",
      "  0002279219       1.00      1.00      1.00         1\n",
      "  0002286589       1.00      1.00      1.00         1\n",
      "  0002290743       1.00      1.00      1.00        12\n",
      "  0002292157       1.00      1.00      1.00        56\n",
      "  0002293365       1.00      1.00      1.00         4\n",
      "  0002295277       1.00      1.00      1.00         2\n",
      "  0002296204       1.00      1.00      1.00         2\n",
      "  0002298308       1.00      1.00      1.00         2\n",
      "  0002299657       1.00      1.00      1.00         1\n",
      "  0002301207       1.00      1.00      1.00         3\n",
      "  0002303465       1.00      1.00      1.00         3\n",
      "  0002307522       1.00      1.00      1.00         3\n",
      "  0002322013       1.00      1.00      1.00         1\n",
      "  0002324076       1.00      1.00      1.00         1\n",
      "  0002324172       1.00      1.00      1.00         2\n",
      "  0002351397       1.00      1.00      1.00         5\n",
      "  0002352000       1.00      1.00      1.00         3\n",
      "  0002352296       1.00      1.00      1.00         9\n",
      "  0002353307       1.00      1.00      1.00         2\n",
      "  0002353687       1.00      1.00      1.00         1\n",
      "  0002355985       1.00      1.00      1.00         2\n",
      "  0002356696       1.00      1.00      1.00         2\n",
      "  0002357039       1.00      1.00      1.00         1\n",
      "  0002357590       1.00      1.00      1.00         4\n",
      "  0002358680       1.00      1.00      1.00         8\n",
      "  0002359515       1.00      1.00      1.00         2\n",
      "  0002359550       1.00      1.00      1.00         1\n",
      "  0002359672       1.00      1.00      1.00         5\n",
      "  0002360053       1.00      1.00      1.00         8\n",
      "  0002360593       1.00      1.00      1.00        15\n",
      "  0002360993       1.00      1.00      1.00         1\n",
      "  0002362734       1.00      1.00      1.00         1\n",
      "  0002364151       1.00      1.00      1.00         1\n",
      "  0002364350       1.00      1.00      1.00         5\n",
      "  0002366559       1.00      1.00      1.00        11\n",
      "  0002371107       1.00      1.00      1.00         2\n",
      "  0002373572       1.00      1.00      1.00         3\n",
      "  0002375047       1.00      1.00      1.00         7\n",
      "  0002377788       1.00      1.00      1.00         3\n",
      "  0002379782       1.00      1.00      1.00         1\n",
      "  0002382154       1.00      1.00      1.00         3\n",
      "  0002383851       1.00      1.00      1.00         1\n",
      "  0002384542       1.00      1.00      1.00        18\n",
      "  0002385386       0.89      1.00      0.94        17\n",
      "  0002387879       1.00      1.00      1.00         2\n",
      "  0002388355       1.00      1.00      1.00         7\n",
      "  0002388408       1.00      1.00      1.00         1\n",
      "  0002390126       1.00      1.00      1.00         2\n",
      "  0002393329       1.00      1.00      1.00        16\n",
      "  0002393381       0.00      0.00      0.00         1\n",
      "  0002393409       1.00      1.00      1.00         2\n",
      "  0002393451       1.00      1.00      1.00         7\n",
      "  0002393493       1.00      1.00      1.00         1\n",
      "  0002393551       1.00      1.00      1.00         4\n",
      "  0002393579       1.00      1.00      1.00         7\n",
      "  0002395083       1.00      1.00      1.00         5\n",
      "  0002396306       0.62      0.57      0.59        14\n",
      "  0002396407       0.50      0.33      0.40         3\n",
      "  0002396412       0.98      1.00      0.99        42\n",
      "  0002398267       1.00      1.00      1.00         4\n",
      "  0002398314       0.67      1.00      0.80         6\n",
      "  0002398448       1.00      1.00      1.00         6\n",
      "  0002401474       1.00      1.00      1.00         4\n",
      "  0002402277       1.00      1.00      1.00         2\n",
      "  0002402393       1.00      1.00      1.00         4\n",
      "  0002402653       1.00      1.00      1.00         2\n",
      "  0002403210       1.00      1.00      1.00         4\n",
      "  0002403516       1.00      1.00      1.00         3\n",
      "  0002404145       0.67      1.00      0.80         2\n",
      "  0002405509       1.00      1.00      1.00         3\n",
      "  0002405561       1.00      1.00      1.00         2\n",
      "  0002405693       1.00      1.00      1.00         1\n",
      "  0002406081       1.00      1.00      1.00         3\n",
      "  0002406744       1.00      1.00      1.00         1\n",
      "  0002407329       1.00      1.00      1.00         1\n",
      "  0002407594       1.00      1.00      1.00         3\n",
      "  0002409173       0.67      1.00      0.80         2\n",
      "  0002411088       1.00      1.00      1.00         1\n",
      "  0002413260       1.00      1.00      1.00         2\n",
      "  0002413261       1.00      1.00      1.00         1\n",
      "  0002413545       1.00      1.00      1.00        26\n",
      "  0002414558       1.00      1.00      1.00         1\n",
      "  0002416356       1.00      1.00      1.00         1\n",
      "  0002418013       1.00      1.00      1.00         2\n",
      "  0002419595       1.00      1.00      1.00         4\n",
      "  0002420430       1.00      1.00      1.00         3\n",
      "  0002420651       1.00      1.00      1.00         7\n",
      "  0002421327       0.00      0.00      0.00         3\n",
      "  0002421571       0.86      1.00      0.92         6\n",
      "  0002421798       1.00      1.00      1.00         1\n",
      "  0002422467       1.00      1.00      1.00         1\n",
      "  0002423034       1.00      1.00      1.00         1\n",
      "  0002423432       1.00      1.00      1.00         3\n",
      "  0002425804       1.00      1.00      1.00         1\n",
      "  0002425999       1.00      1.00      1.00         2\n",
      "  0002426119       1.00      1.00      1.00        36\n",
      "  0002426184       1.00      1.00      1.00         6\n",
      "  0002426533       0.00      0.00      0.00         1\n",
      "  0002426575       1.00      1.00      1.00         2\n",
      "  0002426756       1.00      1.00      1.00         2\n",
      "  0002427212       1.00      1.00      1.00         6\n",
      "  0002432017       1.00      1.00      1.00         1\n",
      "  0002434600       1.00      1.00      1.00         3\n",
      "  0002435084       1.00      1.00      1.00         1\n",
      "  0002435120       1.00      1.00      1.00         1\n",
      "  0002435478       1.00      1.00      1.00         8\n",
      "  0002440212       1.00      1.00      1.00         1\n",
      "  0002440835       1.00      1.00      1.00         1\n",
      "  0002444354       0.00      0.00      0.00         2\n",
      "  0002444511       1.00      1.00      1.00         4\n",
      "  0002445204       1.00      1.00      1.00         2\n",
      "  0002446013       1.00      1.00      1.00         5\n",
      "  0002447455       1.00      1.00      1.00         2\n",
      "  0002448351       1.00      1.00      1.00         2\n",
      "  0002448947       1.00      1.00      1.00         2\n",
      "  0002449134       1.00      1.00      1.00         5\n",
      "  0002449333       1.00      1.00      1.00         3\n",
      "  0002449396       1.00      1.00      1.00         5\n",
      "  0002449868       1.00      1.00      1.00         4\n",
      "  0002450049       1.00      1.00      1.00         5\n",
      "  0002452595       1.00      1.00      1.00        11\n",
      "  0002453228       1.00      1.00      1.00         3\n",
      "  0002454997       1.00      1.00      1.00         1\n",
      "  0002457072       1.00      1.00      1.00        19\n",
      "  0002458806       1.00      1.00      1.00         3\n",
      "  0002460681       1.00      1.00      1.00         2\n",
      "  0002461545       1.00      1.00      1.00         1\n",
      "  0002461849       1.00      1.00      1.00         1\n",
      "  0002462249       1.00      1.00      1.00         1\n",
      "  0002463077       1.00      1.00      1.00         1\n",
      "  0002464934       1.00      1.00      1.00         4\n",
      "  0002464961       1.00      1.00      1.00         1\n",
      "  0002465596       1.00      1.00      1.00         6\n",
      "  0002469533       1.00      1.00      1.00         2\n",
      "  0002470441       1.00      1.00      1.00         2\n",
      "  0002472520       1.00      0.38      0.55         8\n",
      "  0002472874       1.00      1.00      1.00         3\n",
      "  0002472966       1.00      1.00      1.00         3\n",
      "  0002473321       1.00      1.00      1.00         2\n",
      "  0002473435       1.00      1.00      1.00         3\n",
      "  0002473453       1.00      1.00      1.00         4\n",
      "  0002473586       0.80      1.00      0.89         4\n",
      "  0002473828       1.00      1.00      1.00         3\n",
      "  0002473954       1.00      1.00      1.00         6\n",
      "  0002474062       1.00      1.00      1.00         1\n",
      "  0002474259       0.56      0.42      0.48        12\n",
      "  0002474364       0.66      0.83      0.74        30\n",
      "  0002474621       1.00      0.80      0.89         5\n",
      "  0002474704       1.00      1.00      1.00         1\n",
      "  0002474824       1.00      1.00      1.00         6\n",
      "  0002474840       1.00      0.75      0.86         4\n",
      "  0002474861       1.00      1.00      1.00         2\n",
      "  0002474956       1.00      1.00      1.00        15\n",
      "  0002474980       1.00      1.00      1.00         7\n",
      "  0002475016       1.00      1.00      1.00         1\n",
      "  0002475023       0.00      0.00      0.00         1\n",
      "  0002475036       0.98      1.00      0.99        80\n",
      "  0002475288       0.88      1.00      0.93         7\n",
      "  0002475319       1.00      0.50      0.67         2\n",
      "  0002475410       1.00      1.00      1.00         8\n",
      "  0002475438       1.00      1.00      1.00         1\n",
      "  0002475633       0.00      0.00      0.00         0\n",
      "  0002475657       1.00      1.00      1.00         1\n",
      "  0002475691       1.00      1.00      1.00         1\n",
      "  0002475705       1.00      1.00      1.00         3\n",
      "  0002475868       1.00      1.00      1.00         1\n",
      "  0002476038       1.00      1.00      1.00         3\n",
      "  0002476104       1.00      1.00      1.00         2\n",
      "  0002476185       1.00      1.00      1.00         2\n",
      "  0002476232       1.00      1.00      1.00         8\n",
      "  0002476508       1.00      1.00      1.00         1\n",
      "  0002476607       1.00      1.00      1.00         7\n",
      "  0002476629       1.00      1.00      1.00         2\n",
      "  0002476704       1.00      1.00      1.00         9\n",
      "  0002476727       1.00      1.00      1.00        10\n",
      "  0002476733       1.00      1.00      1.00         3\n",
      "  0002477023       1.00      1.00      1.00         6\n",
      "  0002477093       1.00      1.00      1.00         4\n",
      "  0002477115       1.00      1.00      1.00         2\n",
      "  0002477235       1.00      1.00      1.00         3\n",
      "  0002477329       1.00      1.00      1.00         1\n",
      "  0002477353       1.00      0.50      0.67         2\n",
      "  0002477375       1.00      1.00      1.00         4\n",
      "  0002477408       1.00      1.00      1.00         1\n",
      "  0002477420       1.00      1.00      1.00         2\n",
      "  0002477445       1.00      1.00      1.00         1\n",
      "  0002477527       1.00      1.00      1.00         9\n",
      "  0002477582       1.00      1.00      1.00         1\n",
      "  0002477669       1.00      1.00      1.00         2\n",
      "  0002477694       1.00      1.00      1.00         1\n",
      "  0002477703       1.00      1.00      1.00         2\n",
      "  0002477827       1.00      0.50      0.67         2\n",
      "  0002477839       1.00      1.00      1.00         2\n",
      "  0002477840       1.00      1.00      1.00         3\n",
      "  0002477848       1.00      1.00      1.00         1\n",
      "  0002477849       1.00      1.00      1.00       421\n",
      "  0002477850       0.98      1.00      0.99       147\n",
      "  0002477856       1.00      1.00      1.00         4\n",
      "  0002477862       1.00      1.00      1.00        28\n",
      "  0002477864       1.00      1.00      1.00         3\n",
      "  0002477869       1.00      1.00      1.00         2\n",
      "  0002478003       0.75      1.00      0.86         3\n",
      "  0002478013       1.00      1.00      1.00         2\n",
      "  0002478033       1.00      0.57      0.73         7\n",
      "  0002478043       1.00      1.00      1.00         2\n",
      "  0002478119       1.00      1.00      1.00         1\n",
      "  0002478133       1.00      1.00      1.00         8\n",
      "  0002478226       1.00      1.00      1.00         1\n",
      "  0002478308       1.00      1.00      1.00        11\n",
      "  0002478415       1.00      1.00      1.00         2\n",
      "  0002478580       1.00      1.00      1.00         2\n",
      "  0002478645       1.00      1.00      1.00         5\n",
      "  0002479006       1.00      1.00      1.00         1\n",
      "  0002479025       1.00      1.00      1.00         1\n",
      "  0002479187       1.00      1.00      1.00         3\n",
      "  0002479211       1.00      1.00      1.00         3\n",
      "  0002479214       1.00      1.00      1.00         1\n",
      "  0002479226       1.00      1.00      1.00         4\n",
      "  0002479611       1.00      1.00      1.00         1\n",
      "  0002479613       1.00      1.00      1.00         1\n",
      "  0002479739       1.00      1.00      1.00         1\n",
      "  0002479768       1.00      1.00      1.00         1\n",
      "  0002479788       1.00      1.00      1.00         1\n",
      "  0002479839       0.86      1.00      0.92         6\n",
      "  0002479847       1.00      1.00      1.00         1\n",
      "  0002479931       1.00      1.00      1.00         1\n",
      "  0002479964       1.00      1.00      1.00         1\n",
      "  0002480020       1.00      1.00      1.00         4\n",
      "  0002480039       1.00      1.00      1.00         2\n",
      "  0002480050       1.00      1.00      1.00         2\n",
      "  0002480118       1.00      1.00      1.00         5\n",
      "  0002480284       1.00      1.00      1.00         1\n",
      "  0002480410       1.00      1.00      1.00         3\n",
      "  0002480505       1.00      1.00      1.00         1\n",
      "  0002480557       1.00      1.00      1.00         1\n",
      "  0002480598       1.00      1.00      1.00         1\n",
      "  0002480652       1.00      1.00      1.00         5\n",
      "  0002480665       1.00      1.00      1.00         1\n",
      "  0002480722       1.00      1.00      1.00         2\n",
      "  0002480733       1.00      1.00      1.00         2\n",
      "  0002480769       1.00      1.00      1.00         1\n",
      "  0002480806       1.00      1.00      1.00         8\n",
      "  0002480875       1.00      1.00      1.00         1\n",
      "  0002480890       1.00      1.00      1.00        33\n",
      "  0002480917       1.00      0.88      0.93         8\n",
      "  0002481029       1.00      1.00      1.00        13\n",
      "  0002481263       1.00      1.00      1.00         2\n",
      "  0002481391       0.00      0.00      0.00         4\n",
      "  0002481537       0.00      0.00      0.00         1\n",
      "  0002481726       1.00      0.50      0.67         2\n",
      "  0002481770       1.00      1.00      1.00         4\n",
      "  0002481932       1.00      1.00      1.00         3\n",
      "  0002481983       1.00      1.00      1.00         3\n",
      "  0002482161       1.00      1.00      1.00         1\n",
      "  0002482343       1.00      1.00      1.00         4\n",
      "  0002482373       1.00      1.00      1.00         4\n",
      "  0002482375       1.00      1.00      1.00         1\n",
      "  0002482472       1.00      1.00      1.00         1\n",
      "  0002482886       1.00      1.00      1.00         1\n",
      "  0002482910       1.00      1.00      1.00         2\n",
      "  0002482931       1.00      1.00      1.00         2\n",
      "  0002483050       1.00      1.00      1.00         3\n",
      "  0002483232       1.00      1.00      1.00         3\n",
      "  0002483234       1.00      1.00      1.00         7\n",
      "  0002483393       1.00      1.00      1.00         7\n",
      "  0002483463       1.00      1.00      1.00         2\n",
      "  0002484000       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.98      5628\n",
      "   macro avg       0.95      0.95      0.95      5628\n",
      "weighted avg       0.98      0.98      0.98      5628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created histogram  ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\pandas\\core\\indexing.py:691: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created word report  ---------- \n",
      "\n",
      "----------  saved clf to pkl and zip  ---------- \n",
      "\n",
      "----------  finished training for attribute: gl_vendor_id  ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clf_and_create_reports(\n",
    "    df,\n",
    "    'gl_vendor_id',\n",
    "    SCAN_ID_COL,\n",
    "    RES_DIR,\n",
    "    TODAY,\n",
    "    CLIENT,\n",
    "    COUNTRY,\n",
    "    MIN_NUM_OF_SAMPLES,\n",
    "    histogram=True,\n",
    "    word_report=True,\n",
    "    predictions_excel=PREDICTIONS_EXCEL,\n",
    "    save_clf=SAVE_CLFS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train cost element classifiers per company code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-dcc3b2c8755c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_kostenstelle['Target'] = df_kostenstelle['gl_cost_center_id']\n",
      "<ipython-input-57-dcc3b2c8755c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_psp['Target'] = df_psp['gl_wbs_element_id']\n",
      "<ipython-input-57-dcc3b2c8755c>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_auftrag['Target'] = df_auftrag['gl_order_id']\n"
     ]
    }
   ],
   "source": [
    "# Prepare cost element column\n",
    "\n",
    "df_kostenstelle = df[df['gl_cost_center_id'].notna()]\n",
    "df_kostenstelle['Target'] = df_kostenstelle['gl_cost_center_id']\n",
    "\n",
    "df_psp = df[df['gl_wbs_element_id'].notna()]\n",
    "df_psp['Target'] = df_psp['gl_wbs_element_id']\n",
    "\n",
    "df_auftrag = df[df['gl_order_id'].notna()]\n",
    "df_auftrag['Target'] = df_auftrag['gl_order_id']\n",
    "\n",
    "df_pka = pd.concat([df_auftrag,df_kostenstelle, df_psp], axis=0)\n",
    "df_pka['Target'] = df_pka['Target'].apply(lambda x: str(x))\n",
    "\n",
    "df_pka.dropna(subset=['text'], inplace=True)\n",
    "df_pka.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "len(df_pka),len(df_pka['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31656, 31656)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RES_DIR / f\"{TODAY}_{CLIENT}_clf_gl_legal_entity_id_{COUNTRY.lower()}_N_{MIN_NUM_OF_SAMPLES}.pkl\", \"rb\") as file:\n",
    "    clf_bukr = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers (always uses N=1 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company code 0037: 279 entries for 6 classes\n",
      "company code 0065: 1721 entries for 174 classes\n",
      "company code 0301: 7176 entries for 782 classes\n",
      "company code 0303: 8809 entries for 1207 classes\n",
      "company code 0330: 1605 entries for 107 classes\n",
      "company code 0362: 563 entries for 36 classes\n",
      "company code 0377: 173 entries for 23 classes\n",
      "company code 0601: 2245 entries for 276 classes\n",
      "company code 0801: 4789 entries for 568 classes\n",
      "company code 0806: 229 entries for 20 classes\n",
      "company code 0811: 53 entries for 10 classes\n",
      "company code 0812: 5 entries for 1 classes\n",
      "company code 0821: 1069 entries for 105 classes\n",
      "company code 2101: 1321 entries for 137 classes\n",
      "company code 2111: 624 entries for 57 classes\n",
      "company code 3104: 172 entries for 21 classes\n",
      "company code 3401: 62 entries for 1 classes\n",
      "company code 3420: 279 entries for 20 classes\n",
      "company code 9301: 23 entries for 9 classes\n",
      "company code 9310: 187 entries for 61 classes\n",
      "company code 9351: 168 entries for 21 classes\n",
      "company code 9370: 104 entries for 18 classes\n"
     ]
    }
   ],
   "source": [
    "local_min_num = 1\n",
    "\n",
    "for company_code in clf_bukr.classes_:\n",
    "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
    "    num_classes = len(df_cost_elem[\"Target\"].unique())\n",
    "    #print(f\"company code {str(company_code).zfill(4)}: {len(df_cost_elem)} entries for {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_element_de_0037 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0037_N_1.pkl.zip\n",
      "cost_element_de_0065 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0065_N_1.pkl.zip\n",
      "cost_element_de_0301 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0301_N_1.pkl.zip\n",
      "cost_element_de_0303 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0303_N_1.pkl.zip\n",
      "cost_element_de_0330 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0330_N_1.pkl.zip\n",
      "cost_element_de_0362 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0362_N_1.pkl.zip\n",
      "cost_element_de_0377 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0377_N_1.pkl.zip\n",
      "cost_element_de_0601 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0601_N_1.pkl.zip\n",
      "cost_element_de_0801 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0801_N_1.pkl.zip\n",
      "cost_element_de_0806 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0806_N_1.pkl.zip\n",
      "cost_element_de_0811 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0811_N_1.pkl.zip\n",
      "cost_element_de_0812 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0812_N_1.pkl.zip\n",
      "cost_element_de_0821 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_0821_N_1.pkl.zip\n",
      "cost_element_de_2101 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_2101_N_1.pkl.zip\n",
      "cost_element_de_2111 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_2111_N_1.pkl.zip\n",
      "cost_element_de_3104 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_3104_N_1.pkl.zip\n",
      "cost_element_de_3401 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_3401_N_1.pkl.zip\n",
      "cost_element_de_3420 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_3420_N_1.pkl.zip\n",
      "cost_element_de_9301 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_9301_N_1.pkl.zip\n",
      "cost_element_de_9310 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_9310_N_1.pkl.zip\n",
      "cost_element_de_9351 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_9351_N_1.pkl.zip\n",
      "cost_element_de_9370 = data/models/accountclassification_models/de/clfs_de/clf_pka_cascade_de_company_code_9370_N_1.pkl.zip\n"
     ]
    }
   ],
   "source": [
    "# this is the output used in the config file\n",
    "for company_code in clf_bukr.classes_:\n",
    "    print(f\"cost_element_{COUNTRY.lower()}_{str(company_code).zfill(4)}\" + \" = \"\n",
    "          f\"data/models/accountclassification_models/{COUNTRY.lower()}/clfs_{COUNTRY.lower()}/clf_pka_cascade_{COUNTRY.lower()}_company_code_{str(company_code).zfill(4)}_N_{local_min_num}.pkl.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Code: 0037\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 279 samples from 6 relevant classes. (N=1)\n",
      "Accuracy is 0.9464285714285714.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0065\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 1721 samples from 174 relevant classes. (N=1)\n",
      "Accuracy is 0.7159420289855073.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0301\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 7176 samples from 782 relevant classes. (N=1)\n",
      "Accuracy is 0.7277158774373259.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0303\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 8809 samples from 1207 relevant classes. (N=1)\n",
      "Accuracy is 0.6174801362088536.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0330\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 1605 samples from 107 relevant classes. (N=1)\n",
      "Accuracy is 0.7694704049844237.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0362\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 563 samples from 36 relevant classes. (N=1)\n",
      "Accuracy is 0.8141592920353983.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0377\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 173 samples from 23 relevant classes. (N=1)\n",
      "Accuracy is 0.7428571428571429.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0601\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 2245 samples from 276 relevant classes. (N=1)\n",
      "Accuracy is 0.6614699331848553.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0801\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 4789 samples from 568 relevant classes. (N=1)\n",
      "Accuracy is 0.6722338204592901.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0806\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 229 samples from 20 relevant classes. (N=1)\n",
      "Accuracy is 0.7391304347826086.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0811\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 53 samples from 10 relevant classes. (N=1)\n",
      "Accuracy is 0.8181818181818182.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0812\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 5 samples from 1 relevant classes. (N=1)\n",
      "Accuracy is 1.0.\n",
      "--------------------\n",
      "\n",
      "Company Code: 0821\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 1069 samples from 105 relevant classes. (N=1)\n",
      "Accuracy is 0.7336448598130841.\n",
      "--------------------\n",
      "\n",
      "Company Code: 2101\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 1321 samples from 137 relevant classes. (N=1)\n",
      "Accuracy is 0.5849056603773585.\n",
      "--------------------\n",
      "\n",
      "Company Code: 2111\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 624 samples from 57 relevant classes. (N=1)\n",
      "Accuracy is 0.736.\n",
      "--------------------\n",
      "\n",
      "Company Code: 3104\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 172 samples from 21 relevant classes. (N=1)\n",
      "Accuracy is 0.7142857142857143.\n",
      "--------------------\n",
      "\n",
      "Company Code: 3401\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 62 samples from 1 relevant classes. (N=1)\n",
      "Accuracy is 1.0.\n",
      "--------------------\n",
      "\n",
      "Company Code: 3420\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 279 samples from 20 relevant classes. (N=1)\n",
      "Accuracy is 0.625.\n",
      "--------------------\n",
      "\n",
      "Company Code: 9301\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 23 samples from 9 relevant classes. (N=1)\n",
      "Accuracy is 0.8.\n",
      "--------------------\n",
      "\n",
      "Company Code: 9310\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 187 samples from 61 relevant classes. (N=1)\n",
      "Accuracy is 0.2894736842105263.\n",
      "--------------------\n",
      "\n",
      "Company Code: 9351\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 168 samples from 21 relevant classes. (N=1)\n",
      "Accuracy is 0.8235294117647058.\n",
      "--------------------\n",
      "\n",
      "Company Code: 9370\n",
      "\n",
      "Cost Element classification:\n",
      "Reduced to 104 samples from 18 relevant classes. (N=1)\n",
      "Accuracy is 0.6666666666666666.\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for company_code in clf_bukr.classes_:\n",
    "    print(f\"Company Code: {str(company_code).zfill(4)}\" + \"\\n\")\n",
    "    # reduce dataframe to entries for fixed company code\n",
    "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
    "    \n",
    "    print(\"Cost Element classification:\")\n",
    "    # reduce to relevant samples\n",
    "    df_ce = reduce_to_relevant(df_cost_elem, \"Target\", min_num_samples=local_min_num)\n",
    "    \n",
    "    if df_ce.shape[0] == 0:  # do not proceed if df is empty\n",
    "        print(20 * \"-\" + \"\\n\")\n",
    "        continue\n",
    "    \n",
    "    # perform train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_ce, df_ce['Target'],\n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    "    )\n",
    "    \n",
    "    # train and evaluate model\n",
    "    clf = create_model()\n",
    "    clf.fit(X_train, y_train)\n",
    "    evaluate_model(clf, X_test, y_test, report=False)\n",
    "    \n",
    "    if SAVE_CLFS:\n",
    "        # create filename and subfolder (if necessary)\n",
    "        fname = f\"clf_pka_cascade_{COUNTRY.lower()}_company_code_{str(company_code).zfill(4)}_N_{local_min_num}.pkl\"\n",
    "        clfs_path = RES_DIR / f\"clfs_{COUNTRY.lower()}\"\n",
    "        \n",
    "        if not os.path.exists(clfs_path):\n",
    "            os.makedirs(clfs_path)\n",
    "\n",
    "        with open(clfs_path / fname, 'wb') as file:  \n",
    "            pkl.dump(clf, file)\n",
    "    \n",
    "    print(20 * \"-\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_approver (usually N=1 is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  start training for attribute: gl_approver  ---------- \n",
      "\n",
      "Reduced to 33615 samples from 1011 relevant classes. (N=1)\n",
      "Accuracy is 0.6642867767365759.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      A00548       1.00      0.33      0.50         3\n",
      "      A12946       0.00      0.00      0.00         2\n",
      "      A13019       0.00      0.00      0.00         1\n",
      "      A14870       0.00      0.00      0.00         1\n",
      "       A1550       0.50      0.33      0.40         3\n",
      "      A17018       0.59      0.67      0.63        39\n",
      "      A17056       1.00      0.50      0.67         2\n",
      "      A17563       0.50      0.83      0.62         6\n",
      "      A18091       0.71      0.26      0.38        19\n",
      "      A18160       0.00      0.00      0.00         2\n",
      "      A18496       0.50      1.00      0.67         1\n",
      "      A21819       1.00      1.00      1.00         1\n",
      "      A22646       0.61      0.94      0.74        36\n",
      "      A22989       0.00      0.00      0.00         1\n",
      "      A23331       0.81      0.93      0.87        14\n",
      "      A23661       0.00      0.00      0.00         2\n",
      "      A24501       0.00      0.00      0.00         3\n",
      "      A24738       0.00      0.00      0.00         0\n",
      "      A27123       0.00      0.00      0.00         1\n",
      "      A28999       1.00      1.00      1.00         1\n",
      "      A32216       0.00      0.00      0.00         0\n",
      "      A34652       0.00      0.00      0.00         8\n",
      "      A34679       0.00      0.00      0.00         0\n",
      "      A39850       0.00      0.00      0.00         1\n",
      "      A40185       0.76      0.82      0.78        38\n",
      "      A40430       0.00      0.00      0.00         1\n",
      "      A41268       0.67      0.38      0.48        21\n",
      "       A4493       0.00      0.00      0.00         1\n",
      "      A45474       1.00      0.50      0.67         2\n",
      "      A45532       0.00      0.00      0.00         1\n",
      "      A47182       0.64      0.67      0.65        21\n",
      "      A47495       0.75      0.50      0.60         6\n",
      "      A51143       0.00      0.00      0.00         0\n",
      "       A5264       0.18      0.12      0.14        26\n",
      "       A5298       0.00      0.00      0.00         2\n",
      "      A53092       0.00      0.00      0.00         1\n",
      "       A5556       0.57      0.67      0.62         6\n",
      "       A5682       0.33      0.50      0.40         2\n",
      "       A5691       0.29      0.29      0.29         7\n",
      "       A5718       0.50      1.00      0.67         1\n",
      "       A5728       0.50      0.50      0.50         2\n",
      "       A5806       0.60      1.00      0.75         3\n",
      "       A5809       0.00      0.00      0.00         4\n",
      "       A5853       1.00      1.00      1.00         6\n",
      "       A5861       0.00      0.00      0.00         1\n",
      "       A5877       1.00      1.00      1.00         1\n",
      "       A5882       0.00      0.00      0.00         2\n",
      "       A5912       0.78      0.78      0.78         9\n",
      "       A5925       1.00      0.33      0.50         3\n",
      "       A6008       1.00      0.67      0.80         3\n",
      "       A6009       1.00      1.00      1.00         3\n",
      "       A6022       0.00      0.00      0.00         2\n",
      "       A6026       0.00      0.00      0.00         0\n",
      "       A6029       0.60      0.83      0.70        29\n",
      "       A6030       0.25      1.00      0.40         2\n",
      "       A6034       0.40      0.50      0.44         4\n",
      "       A6042       1.00      0.50      0.67         2\n",
      "       A6088       0.81      0.85      0.83        20\n",
      "       A6090       0.00      0.00      0.00         1\n",
      "       A6092       0.00      0.00      0.00         5\n",
      "       A6093       0.50      0.33      0.40         3\n",
      "       A6101       0.56      0.62      0.59         8\n",
      "       A6108       0.00      0.00      0.00         0\n",
      "       A6115       0.39      0.54      0.45        28\n",
      "       A6120       1.00      0.45      0.62        11\n",
      "       A6232       1.00      1.00      1.00         2\n",
      "       A7774       0.00      0.00      0.00         1\n",
      "       A9072       0.72      0.96      0.83        27\n",
      "     B000010       0.00      0.00      0.00         1\n",
      "      B00125       0.00      0.00      0.00         1\n",
      "      B10008       0.71      0.56      0.63         9\n",
      "      B10163       0.25      1.00      0.40         1\n",
      "      B10174       0.00      0.00      0.00         1\n",
      "      B10273       0.93      0.93      0.93        15\n",
      "      B10746       1.00      1.00      1.00         2\n",
      "      B12048       0.00      0.00      0.00         1\n",
      "       B1299       1.00      0.33      0.50         3\n",
      "      B13592       0.62      0.58      0.60        31\n",
      "      B13971       0.00      0.00      0.00         5\n",
      "      B16874       0.00      0.00      0.00         1\n",
      "      B17281       0.67      0.80      0.73        10\n",
      "      B17542       0.00      0.00      0.00         3\n",
      "      B19315       0.00      0.00      0.00         1\n",
      "      B19506       0.00      0.00      0.00         0\n",
      "      B19600       0.81      0.84      0.82        56\n",
      "      B19892       1.00      0.50      0.67         2\n",
      "       B3266       0.65      1.00      0.79        15\n",
      "       B3399       1.00      1.00      1.00         1\n",
      "       B3407       1.00      1.00      1.00         1\n",
      "       B3415       0.00      0.00      0.00         0\n",
      "       B3456       0.50      0.36      0.42        22\n",
      "       B3535       0.00      0.00      0.00         1\n",
      "       B3556       0.33      0.33      0.33         6\n",
      "       B3589       0.40      0.50      0.44         4\n",
      "       B3592       0.69      0.82      0.75        50\n",
      "       B4731       1.00      1.00      1.00        22\n",
      "       B5077       0.00      0.00      0.00         1\n",
      "       B6569       0.49      0.66      0.56        38\n",
      "       B6800       0.87      0.99      0.93       119\n",
      "       B7796       0.25      0.20      0.22         5\n",
      "       B8630       0.00      0.00      0.00         1\n",
      "       B9489       1.00      0.67      0.80         3\n",
      "       B9843       0.53      0.53      0.53       106\n",
      "       B9862       0.00      0.00      0.00         1\n",
      "     C000034       0.00      0.00      0.00         1\n",
      "       C1092       0.33      0.20      0.25         5\n",
      "      C11450       0.00      0.00      0.00         1\n",
      "      C11925       0.00      0.00      0.00         1\n",
      "      C12017       0.83      1.00      0.91         5\n",
      "      C12398       0.00      0.00      0.00         2\n",
      "      C12452       0.00      0.00      0.00         3\n",
      "      C14466       0.00      0.00      0.00         1\n",
      "      C14743       0.00      0.00      0.00         0\n",
      "      C14921       1.00      0.82      0.90        17\n",
      "      C15016       0.50      0.25      0.33         4\n",
      "      C16016       0.00      0.00      0.00         1\n",
      "      C16490       1.00      0.67      0.80         3\n",
      "      C18120       0.56      0.62      0.59         8\n",
      "      C18930       0.67      0.67      0.67         3\n",
      "      C19265       0.91      0.71      0.80        14\n",
      "      C19877       0.67      1.00      0.80         2\n",
      "      C20195       0.00      0.00      0.00         1\n",
      "      C21152       0.87      0.62      0.72        21\n",
      "      C22795       0.00      0.00      0.00         2\n",
      "      C23869       0.00      0.00      0.00         1\n",
      "      C26483       0.00      0.00      0.00         1\n",
      "      C26682       0.50      0.43      0.46         7\n",
      "      C28692       0.00      0.00      0.00         0\n",
      "      C28965       1.00      1.00      1.00         1\n",
      "      C29328       0.00      0.00      0.00         1\n",
      "       C3945       0.00      0.00      0.00         3\n",
      "       C3972       0.40      0.50      0.44         4\n",
      "       C4051       0.33      0.80      0.47         5\n",
      "       C4052       0.00      0.00      0.00         1\n",
      "       C4065       0.00      0.00      0.00         1\n",
      "       C4078       0.00      0.00      0.00         1\n",
      "       C4094       0.41      0.88      0.56         8\n",
      "       C4097       0.94      0.98      0.96        95\n",
      "       C4123       0.00      0.00      0.00         1\n",
      "       C4132       0.38      0.60      0.46         5\n",
      "       C4138       1.00      0.50      0.67         2\n",
      "       C4144       0.00      0.00      0.00         1\n",
      "       C4181       0.00      0.00      0.00         1\n",
      "       C4182       0.68      0.76      0.72        25\n",
      "       C4216       0.00      0.00      0.00         0\n",
      "       C4250       0.67      0.83      0.74        12\n",
      "       C4264       0.65      0.72      0.69        50\n",
      "       C4266       0.00      0.00      0.00         2\n",
      "       C4278       0.67      0.25      0.36         8\n",
      "       C4284       0.00      0.00      0.00         3\n",
      "       C4293       0.56      0.83      0.67         6\n",
      "       C5278       0.00      0.00      0.00         1\n",
      "       C6118       1.00      0.57      0.73         7\n",
      "       C6129       0.00      0.00      0.00         2\n",
      "       C6138       0.71      0.71      0.71         7\n",
      "       C7896       0.50      1.00      0.67         2\n",
      "       C8857       0.67      0.60      0.63        20\n",
      "       C8892       0.50      0.12      0.20         8\n",
      "       C9067       0.25      1.00      0.40         1\n",
      "       D0043       1.00      1.00      1.00         1\n",
      "      D10650       0.00      0.00      0.00         0\n",
      "      D11171       0.83      1.00      0.91         5\n",
      "      D13436       1.00      0.50      0.67         2\n",
      "      D14837       0.00      0.00      0.00         2\n",
      "      D15238       0.57      0.33      0.42        24\n",
      "      D15783       0.00      0.00      0.00         1\n",
      "      D16278       0.00      0.00      0.00         1\n",
      "      D18003       0.40      0.67      0.50         3\n",
      "      D18584       0.75      0.75      0.75         8\n",
      "      D19202       0.00      0.00      0.00         1\n",
      "      D20953       0.61      0.75      0.67       110\n",
      "      D23367       0.00      0.00      0.00         1\n",
      "      D24879       0.00      0.00      0.00         1\n",
      "      D25786       0.00      0.00      0.00         0\n",
      "      D29217       0.80      0.57      0.67         7\n",
      "      D30401       0.00      0.00      0.00         0\n",
      "       D4410       0.00      0.00      0.00         0\n",
      "       D4435       0.50      1.00      0.67         1\n",
      "       D4650       0.00      0.00      0.00         6\n",
      "       D4662       0.00      0.00      0.00         1\n",
      "       D4671       0.00      0.00      0.00         0\n",
      "       D4677       1.00      1.00      1.00         1\n",
      "       D4685       0.67      1.00      0.80         6\n",
      "       D4701       1.00      0.33      0.50         3\n",
      "       D4704       1.00      1.00      1.00         1\n",
      "       D4776       0.62      0.80      0.70        30\n",
      "       D4780       0.50      0.62      0.56         8\n",
      "       D4784       0.50      0.75      0.60         4\n",
      "       D4789       0.00      0.00      0.00         0\n",
      "       D4812       0.94      0.88      0.91        34\n",
      "       D4826       0.83      0.56      0.67        18\n",
      "       D4836       0.26      0.30      0.28        27\n",
      "       D4838       0.67      0.38      0.48        16\n",
      "       D4840       0.00      0.00      0.00         3\n",
      "       D4893       0.50      1.00      0.67         1\n",
      "       D5226       0.67      1.00      0.80         2\n",
      "       D6177       0.53      0.91      0.67        11\n",
      "       D6179       0.00      0.00      0.00         0\n",
      "       D6578       0.00      0.00      0.00         1\n",
      "       D8107       0.65      0.65      0.65        17\n",
      "       D8466       0.56      0.71      0.63         7\n",
      "       D9376       0.00      0.00      0.00         5\n",
      "       E0034       0.57      0.50      0.53         8\n",
      "      E10461       0.00      0.00      0.00         1\n",
      "       E1381       0.50      0.60      0.55         5\n",
      "      E13819       0.94      0.96      0.95       176\n",
      "      E14684       0.00      0.00      0.00         1\n",
      "      E15472       0.47      1.00      0.64         7\n",
      "      E16434       0.60      0.86      0.71         7\n",
      "      E16665       0.00      0.00      0.00         0\n",
      "      E18674       0.65      0.38      0.48        39\n",
      "       E2059       0.50      0.17      0.25         6\n",
      "       E2223       1.00      0.50      0.67         2\n",
      "       E2240       0.00      0.00      0.00         1\n",
      "       E2274       1.00      0.75      0.86         4\n",
      "       E2324       1.00      0.88      0.93         8\n",
      "       E2332       1.00      1.00      1.00         2\n",
      "       E2339       0.60      0.60      0.60         5\n",
      "       E2350       0.50      0.25      0.33         4\n",
      "       E2864       0.00      0.00      0.00         1\n",
      "       E7969       0.00      0.00      0.00         1\n",
      "       F0789       0.00      0.00      0.00         1\n",
      "      F10521       0.44      0.65      0.52        37\n",
      "      F10767       0.00      0.00      0.00         1\n",
      "      F13612       1.00      0.33      0.50         3\n",
      "      F14809       0.00      0.00      0.00         1\n",
      "       F1987       1.00      1.00      1.00         4\n",
      "       F2012       0.00      0.00      0.00         2\n",
      "       F2048       0.00      0.00      0.00         0\n",
      "       F2070       0.79      0.86      0.82       306\n",
      "       F2078       0.55      0.88      0.68        25\n",
      "       F2164       0.25      0.33      0.29         3\n",
      "       F2172       0.62      0.62      0.62         8\n",
      "       F2180       0.67      0.50      0.57         8\n",
      "       F2203       0.80      0.44      0.57         9\n",
      "       F2208       0.00      0.00      0.00         2\n",
      "       F2209       0.67      0.33      0.44         6\n",
      "       F2217       1.00      1.00      1.00         9\n",
      "       F2219       0.00      0.00      0.00         1\n",
      "       F2234       1.00      1.00      1.00         4\n",
      "       F2235       0.75      0.16      0.26        19\n",
      "       F5705       0.67      1.00      0.80         2\n",
      "       F5853       1.00      0.50      0.67         2\n",
      "       F6919       0.00      0.00      0.00         0\n",
      "       F8244       0.00      0.00      0.00         1\n",
      "       F8601       0.50      0.33      0.40         3\n",
      "      G14395       0.50      0.33      0.40         3\n",
      "      G16876       0.00      0.00      0.00         1\n",
      "      G18155       1.00      0.80      0.89         5\n",
      "      G21081       0.00      0.00      0.00         0\n",
      "      G21953       0.00      0.00      0.00         2\n",
      "       G3196       0.50      1.00      0.67         1\n",
      "       G3490       0.00      0.00      0.00         2\n",
      "       G3533       0.00      0.00      0.00         0\n",
      "       G3539       0.00      0.00      0.00         0\n",
      "       G3561       0.40      1.00      0.57         2\n",
      "       G3562       0.11      0.50      0.18         2\n",
      "       G3563       0.67      0.50      0.57         4\n",
      "       G3639       0.69      0.79      0.73        14\n",
      "       G3643       0.67      1.00      0.80         2\n",
      "       G3697       0.00      0.00      0.00         2\n",
      "       G3736       0.00      0.00      0.00         0\n",
      "       G3760       0.46      0.40      0.43        15\n",
      "       G3769       0.00      0.00      0.00         2\n",
      "       G9328       0.75      1.00      0.86         3\n",
      "      H16086       0.00      0.00      0.00         1\n",
      "      H17134       0.00      0.00      0.00        10\n",
      "      H17532       0.00      0.00      0.00         1\n",
      "      H19020       0.53      0.83      0.65        12\n",
      "      H19236       0.00      0.00      0.00         1\n",
      "      H19428       0.29      0.20      0.24        20\n",
      "      H19707       0.88      0.78      0.82         9\n",
      "      H20208       1.00      0.50      0.67         2\n",
      "      H21293       0.00      0.00      0.00         4\n",
      "      H22131       0.00      0.00      0.00         1\n",
      "      H22822       1.00      1.00      1.00         1\n",
      "      H24928       0.40      1.00      0.57         2\n",
      "      H26975       1.00      1.00      1.00         1\n",
      "       H4746       1.00      1.00      1.00         6\n",
      "       H5010       0.58      0.39      0.47        18\n",
      "       H5096       1.00      0.67      0.80         3\n",
      "       H5128       0.00      0.00      0.00         0\n",
      "       H5144       0.00      0.00      0.00         1\n",
      "       H5151       0.00      0.00      0.00         1\n",
      "       H5195       0.58      0.42      0.48        36\n",
      "       H5241       0.00      0.00      0.00         0\n",
      "       H5266       0.67      1.00      0.80         2\n",
      "       H5267       0.00      0.00      0.00         3\n",
      "       H5298       0.60      0.60      0.60         5\n",
      "       H5308       0.00      0.00      0.00         1\n",
      "       H5398       0.38      0.73      0.50        15\n",
      "       H5400       0.67      0.77      0.71        13\n",
      "       H5402       0.56      1.00      0.71         5\n",
      "       H5427       0.00      0.00      0.00         0\n",
      "       H5428       0.62      0.73      0.67        11\n",
      "       H5516       1.00      0.50      0.67         2\n",
      "       H5559       0.29      0.33      0.31         6\n",
      "       H7477       1.00      0.50      0.67         4\n",
      "       H8029       0.00      0.00      0.00         5\n",
      "       I0374       0.25      0.33      0.29         3\n",
      "       I1210       0.50      0.67      0.57         3\n",
      "       I1228       0.00      0.00      0.00         1\n",
      "       I1229       0.25      0.25      0.25         4\n",
      "      I12603       0.33      0.33      0.33         3\n",
      "      I13145       0.00      0.00      0.00         2\n",
      "       I3400       1.00      0.50      0.67         2\n",
      "       I4899       1.00      0.45      0.62        11\n",
      "       I6618       0.66      0.54      0.59        87\n",
      "       I6672       0.78      0.78      0.78         9\n",
      "       I7102       1.00      0.50      0.67         4\n",
      "      J10007       0.00      0.00      0.00         1\n",
      "       J1557       0.00      0.00      0.00         2\n",
      "      J15640       0.00      0.00      0.00         1\n",
      "      J16975       0.33      0.20      0.25         5\n",
      "       J2080       0.74      1.00      0.85        40\n",
      "      J21410       0.50      0.50      0.50         2\n",
      "      J21750       0.00      0.00      0.00         1\n",
      "      J24230       0.00      0.00      0.00         0\n",
      "      J25718       0.00      0.00      0.00         1\n",
      "      J27411       0.00      0.00      0.00         4\n",
      "      J29934       0.00      0.00      0.00         0\n",
      "      J32166       0.88      0.78      0.82         9\n",
      "      J32636       0.33      0.67      0.44         3\n",
      "      J36034       0.00      0.00      0.00         2\n",
      "      J36268       0.79      0.86      0.82        90\n",
      "      J36391       0.50      0.45      0.47        20\n",
      "      J37145       0.17      0.12      0.14         8\n",
      "      J38252       0.00      0.00      0.00         1\n",
      "      J38365       0.74      0.50      0.60        28\n",
      "      J38986       0.00      0.00      0.00         2\n",
      "       J3902       0.00      0.00      0.00         0\n",
      "       J6620       0.62      0.48      0.54        21\n",
      "       J6730       0.00      0.00      0.00         1\n",
      "       J6735       0.33      1.00      0.50         1\n",
      "       J6750       0.00      0.00      0.00         4\n",
      "       J6758       0.71      0.62      0.67         8\n",
      "       J6764       0.33      0.25      0.29         4\n",
      "       J6772       0.75      0.60      0.67         5\n",
      "       J6777       0.00      0.00      0.00         1\n",
      "       J6818       0.00      0.00      0.00         2\n",
      "       J6820       0.00      0.00      0.00         1\n",
      "       J6839       0.00      0.00      0.00         3\n",
      "       J6867       0.25      0.14      0.18         7\n",
      "       J6871       0.56      1.00      0.71         5\n",
      "       J6872       0.00      0.00      0.00         1\n",
      "       J6876       0.78      1.00      0.88         7\n",
      "       J6877       0.00      0.00      0.00         2\n",
      "       J6878       0.00      0.00      0.00         1\n",
      "       J6880       1.00      1.00      1.00         5\n",
      "       J6886       0.75      1.00      0.86         3\n",
      "       J6944       1.00      0.50      0.67         2\n",
      "       J6956       0.00      0.00      0.00         1\n",
      "       J6961       0.50      0.40      0.44         5\n",
      "       J6967       1.00      0.50      0.67         2\n",
      "       J6968       0.00      0.00      0.00         1\n",
      "       J6995       0.91      0.71      0.80        14\n",
      "       J7021       0.33      0.33      0.33         3\n",
      "       J7045       1.00      1.00      1.00         1\n",
      "       J7048       0.67      1.00      0.80         4\n",
      "       J7084       1.00      1.00      1.00         1\n",
      "       J7115       0.25      0.14      0.18         7\n",
      "       J7117       0.50      1.00      0.67         1\n",
      "       J7135       1.00      0.20      0.33         5\n",
      "       J7137       0.00      0.00      0.00         1\n",
      "       J7145       1.00      0.33      0.50         3\n",
      "       J7146       1.00      0.40      0.57         5\n",
      "       J7150       0.00      0.00      0.00         1\n",
      "       J7170       0.00      0.00      0.00         0\n",
      "       J7212       0.00      0.00      0.00         7\n",
      "       J7246       1.00      0.67      0.80         6\n",
      "       J7251       0.00      0.00      0.00         1\n",
      "       J7254       0.00      0.00      0.00         1\n",
      "       J9728       0.00      0.00      0.00         1\n",
      "       K0494       0.57      1.00      0.73         4\n",
      "      K10911       0.14      0.17      0.15         6\n",
      "       K1111       1.00      0.60      0.75         5\n",
      "      K12328       0.29      0.33      0.31         6\n",
      "      K12679       0.00      0.00      0.00         1\n",
      "      K12780       0.71      0.62      0.67         8\n",
      "      K13463       0.00      0.00      0.00         1\n",
      "      K14389       0.60      0.61      0.60        41\n",
      "      K15041       0.00      0.00      0.00         1\n",
      "      K19542       0.73      0.71      0.72        73\n",
      "      K26575       1.00      1.00      1.00         1\n",
      "      K27165       0.67      0.50      0.57         4\n",
      "      K27210       0.85      0.96      0.90        48\n",
      "       K2729       0.00      0.00      0.00         0\n",
      "       K4305       0.00      0.00      0.00         1\n",
      "       K4346       0.00      0.00      0.00         0\n",
      "       K4347       0.00      0.00      0.00         1\n",
      "       K4365       0.00      0.00      0.00         2\n",
      "       K4384       0.50      0.68      0.58        22\n",
      "       K4385       1.00      0.20      0.33        10\n",
      "       K4389       1.00      0.25      0.40         4\n",
      "       K4399       0.75      0.43      0.55         7\n",
      "       K4405       0.60      0.75      0.67         4\n",
      "       K4411       0.67      0.33      0.44         6\n",
      "       K4438       1.00      1.00      1.00         1\n",
      "       K4484       0.00      0.00      0.00         4\n",
      "       K4508       0.55      0.58      0.56        19\n",
      "       K4600       0.00      0.00      0.00         1\n",
      "       K5039       0.00      0.00      0.00         2\n",
      "       K6434       0.00      0.00      0.00         3\n",
      "       K6472       0.00      0.00      0.00         1\n",
      "       K9944       0.50      1.00      0.67         2\n",
      "     L000023       0.25      0.14      0.18         7\n",
      "       L0236       0.77      0.77      0.77        22\n",
      "       L0516       0.86      0.86      0.86         7\n",
      "      L13213       0.00      0.00      0.00         1\n",
      "      L14756       0.00      0.00      0.00         2\n",
      "      L16115       0.00      0.00      0.00         1\n",
      "      L16178       0.00      0.00      0.00         0\n",
      "      L17155       0.00      0.00      0.00         0\n",
      "      L19039       0.00      0.00      0.00         1\n",
      "      L20251       0.00      0.00      0.00         3\n",
      "       L2082       0.00      0.00      0.00         0\n",
      "       L2101       0.00      0.00      0.00         3\n",
      "       L2107       0.00      0.00      0.00         1\n",
      "       L2111       1.00      0.80      0.89         5\n",
      "       L2131       0.56      0.38      0.45        13\n",
      "       L6384       0.00      0.00      0.00         0\n",
      "     M000027       0.00      0.00      0.00         1\n",
      "     M000114       0.67      0.50      0.57         4\n",
      "      M00211       0.00      0.00      0.00         0\n",
      "      M00482       0.65      0.55      0.59        20\n",
      "       M0128       0.00      0.00      0.00         5\n",
      "       M0206       0.00      0.00      0.00         5\n",
      "      M11186       0.00      0.00      0.00         1\n",
      "      M11435       0.00      0.00      0.00         5\n",
      "      M12168       1.00      0.67      0.80         3\n",
      "      M12174       0.00      0.00      0.00         1\n",
      "       M1460       1.00      0.25      0.40         4\n",
      "      M17260       0.00      0.00      0.00         1\n",
      "      M20271       1.00      1.00      1.00         6\n",
      "      M20347       0.50      0.50      0.50         4\n",
      "      M20625       0.00      0.00      0.00         2\n",
      "       M2079       0.00      0.00      0.00         0\n",
      "      M20917       0.00      0.00      0.00         1\n",
      "      M21419       0.00      0.00      0.00         2\n",
      "       M2201       0.00      0.00      0.00         1\n",
      "      M22274       0.75      0.75      0.75         4\n",
      "      M23623       0.00      0.00      0.00         1\n",
      "      M25375       0.00      0.00      0.00         0\n",
      "      M27354       0.00      0.00      0.00         1\n",
      "      M28733       0.67      1.00      0.80         2\n",
      "      M29322       0.43      0.55      0.48        11\n",
      "      M29871       0.50      0.33      0.40         3\n",
      "      M30153       0.00      0.00      0.00         0\n",
      "      M30566       0.67      0.67      0.67        15\n",
      "      M30983       0.88      1.00      0.93         7\n",
      "      M31091       0.00      0.00      0.00         2\n",
      "      M31519       0.00      0.00      0.00         1\n",
      "      M32215       0.00      0.00      0.00         0\n",
      "      M33691       0.00      0.00      0.00         1\n",
      "      M33729       0.00      0.00      0.00         0\n",
      "      M34353       0.00      0.00      0.00         1\n",
      "      M34561       0.00      0.00      0.00         1\n",
      "       M3643       0.22      0.25      0.24         8\n",
      "      M40921       0.00      0.00      0.00         2\n",
      "      M41790       1.00      1.00      1.00         2\n",
      "      M43650       0.00      0.00      0.00         1\n",
      "      M50704       0.00      0.00      0.00         1\n",
      "      M52010       0.36      0.42      0.38        12\n",
      "      M53531       0.40      0.43      0.41        14\n",
      "      M55865       0.00      0.00      0.00         2\n",
      "      M56430       0.00      0.00      0.00         1\n",
      "      M60343       0.00      0.00      0.00         0\n",
      "      M63641       1.00      0.75      0.86         4\n",
      "      M64219       1.00      1.00      1.00         3\n",
      "      M65970       0.38      0.55      0.44        11\n",
      "      M66453       0.00      0.00      0.00         0\n",
      "      M66612       0.25      0.25      0.25         4\n",
      "       M6814       0.00      0.00      0.00         2\n",
      "       M7357       0.00      0.00      0.00         0\n",
      "       M7780       0.69      0.74      0.72        39\n",
      "       M7893       0.00      0.00      0.00         2\n",
      "       M7934       0.00      0.00      0.00         1\n",
      "       M7951       0.00      0.00      0.00         0\n",
      "       M7971       0.75      0.75      0.75        12\n",
      "       M7984       0.00      0.00      0.00         1\n",
      "       M7989       0.00      0.00      0.00         5\n",
      "       M7993       0.89      0.81      0.85        21\n",
      "       M8006       0.00      0.00      0.00         1\n",
      "       M8011       1.00      1.00      1.00         2\n",
      "       M8013       0.60      0.50      0.55         6\n",
      "       M8095       1.00      0.44      0.62         9\n",
      "       M8099       1.00      1.00      1.00         1\n",
      "       M8138       0.71      0.83      0.77         6\n",
      "       M8161       0.71      0.71      0.71        14\n",
      "       M8181       0.00      0.00      0.00         3\n",
      "       M8228       0.62      0.58      0.60        31\n",
      "       M8235       0.42      0.62      0.50        24\n",
      "       M8239       0.45      0.50      0.48        10\n",
      "       M8247       0.79      0.88      0.83        57\n",
      "       M8276       0.00      0.00      0.00         2\n",
      "       M8338       0.61      0.79      0.69        14\n",
      "       M8339       0.50      0.14      0.22         7\n",
      "       M8357       0.00      0.00      0.00         3\n",
      "       M8359       1.00      0.33      0.50         3\n",
      "       M8360       0.20      0.40      0.27         5\n",
      "       M8361       0.29      0.50      0.36         4\n",
      "       M8369       0.00      0.00      0.00         1\n",
      "       M8430       0.50      0.44      0.47        16\n",
      "       M8432       0.00      0.00      0.00         7\n",
      "       M8433       0.86      0.75      0.80        16\n",
      "       M8438       0.75      0.75      0.75         4\n",
      "       M8440       0.00      0.00      0.00         1\n",
      "       M8443       0.00      0.00      0.00         1\n",
      "       M8485       0.70      0.54      0.61        13\n",
      "       M8503       0.00      0.00      0.00         4\n",
      "       M8506       0.80      0.80      0.80         5\n",
      "       M8507       0.81      0.70      0.75        60\n",
      "       M8553       0.17      0.50      0.25         2\n",
      "       M9534       0.00      0.00      0.00         1\n",
      "       N0453       0.00      0.00      0.00         1\n",
      "       N0455       0.00      0.00      0.00         3\n",
      "      N11742       0.33      1.00      0.50         1\n",
      "      N14371       0.33      0.25      0.29         8\n",
      "       N1632       0.64      0.39      0.48        18\n",
      "       N1682       1.00      0.57      0.73         7\n",
      "       N1690       0.00      0.00      0.00         1\n",
      "       N1693       0.00      0.00      0.00         5\n",
      "       N1785       0.00      0.00      0.00         1\n",
      "       N1846       0.00      0.00      0.00         1\n",
      "       N2040       0.75      0.75      0.75         4\n",
      "       N3485       0.80      0.67      0.73         6\n",
      "       N4027       0.00      0.00      0.00         1\n",
      "       N5747       0.43      0.33      0.38         9\n",
      "       N5804       0.75      0.88      0.81        34\n",
      "       N6028       0.54      0.71      0.61        21\n",
      "       N6178       0.00      0.00      0.00         2\n",
      "       N6877       0.00      0.00      0.00         1\n",
      "       O0052       0.00      0.00      0.00         3\n",
      "       O0555       0.79      0.85      0.81        13\n",
      "       O0567       1.00      0.12      0.22         8\n",
      "       O0568       0.95      0.97      0.96       121\n",
      "       O0598       0.17      0.50      0.25         2\n",
      "       O0638       1.00      1.00      1.00         1\n",
      "       O2704       0.44      1.00      0.62         4\n",
      "      P00180       0.00      0.00      0.00         1\n",
      "       P0637       0.33      0.29      0.31        35\n",
      "       P0704       1.00      0.50      0.67         2\n",
      "       P0888       0.00      0.00      0.00         2\n",
      "      P13587       0.00      0.00      0.00         1\n",
      "      P13596       0.29      0.50      0.36         4\n",
      "      P14057       0.00      0.00      0.00         1\n",
      "      P15727       0.59      0.42      0.49        24\n",
      "      P15963       0.76      0.68      0.72        19\n",
      "      P19080       0.67      0.50      0.57         4\n",
      "      P19596       0.69      0.78      0.74        96\n",
      "      P25148       0.79      0.83      0.81       293\n",
      "       P3217       0.00      0.00      0.00         1\n",
      "       P3253       0.48      0.56      0.51        18\n",
      "       P3271       0.00      0.00      0.00         1\n",
      "       P3276       1.00      0.33      0.50         3\n",
      "       P3278       0.33      0.20      0.25         5\n",
      "       P3326       0.00      0.00      0.00         1\n",
      "       P3327       0.33      1.00      0.50         1\n",
      "       P6242       0.00      0.00      0.00         0\n",
      "       P8919       0.00      0.00      0.00         2\n",
      "       P9698       0.00      0.00      0.00         1\n",
      "     R000025       0.00      0.00      0.00         1\n",
      "      R12705       0.00      0.00      0.00         3\n",
      "      R12826       0.00      0.00      0.00         5\n",
      "      R13348       1.00      0.50      0.67         2\n",
      "      R14030       0.00      0.00      0.00        11\n",
      "      R14374       0.00      0.00      0.00         2\n",
      "      R14824       0.67      0.80      0.73         5\n",
      "       R1514       0.81      1.00      0.89        50\n",
      "      R15449       0.42      1.00      0.59         5\n",
      "      R15701       0.94      0.94      0.94        16\n",
      "      R17568       1.00      0.33      0.50         3\n",
      "      R18376       0.00      0.00      0.00         1\n",
      "      R19859       0.50      1.00      0.67         2\n",
      "      R23765       0.00      0.00      0.00         1\n",
      "      R24658       0.64      1.00      0.78        16\n",
      "      R25302       1.00      1.00      1.00         3\n",
      "      R30653       0.00      0.00      0.00         1\n",
      "      R31183       0.00      0.00      0.00         1\n",
      "      R31829       0.00      0.00      0.00         5\n",
      "       R5543       0.00      0.00      0.00         1\n",
      "       R5551       0.00      0.00      0.00         4\n",
      "       R5594       0.00      0.00      0.00         0\n",
      "       R5672       0.00      0.00      0.00         1\n",
      "       R5679       0.67      0.80      0.73         5\n",
      "       R5756       0.00      0.00      0.00         1\n",
      "       R5760       0.60      1.00      0.75         3\n",
      "       R5764       0.54      0.78      0.64         9\n",
      "       R5766       0.42      0.42      0.42        12\n",
      "       R5769       0.25      0.33      0.29         3\n",
      "       R5793       0.00      0.00      0.00         1\n",
      "       R5798       0.00      0.00      0.00         5\n",
      "       R5863       1.00      0.50      0.67         2\n",
      "       R5865       0.65      1.00      0.79        24\n",
      "       R5866       1.00      1.00      1.00         2\n",
      "       R5867       1.00      0.33      0.50         3\n",
      "       R5875       1.00      0.75      0.86         8\n",
      "       R5877       0.00      0.00      0.00         2\n",
      "       R5915       0.00      0.00      0.00         1\n",
      "       R6553       1.00      1.00      1.00         1\n",
      "       R7773       1.00      0.33      0.50         3\n",
      "       R8131       0.10      0.05      0.06        21\n",
      "       R8306       0.80      0.67      0.73         6\n",
      "       S0010       0.00      0.00      0.00         1\n",
      "      S10875       0.70      0.88      0.78        16\n",
      "      S10937       0.00      0.00      0.00         0\n",
      "       S1131       0.83      0.56      0.67         9\n",
      "      S11329       0.00      0.00      0.00         1\n",
      "       S1138       0.50      0.40      0.44         5\n",
      "      S12806       0.00      0.00      0.00         5\n",
      "      S13969       1.00      1.00      1.00         1\n",
      "      S14203       0.25      0.50      0.33         4\n",
      "      S18379       0.00      0.00      0.00         0\n",
      "      S18479       0.00      0.00      0.00         1\n",
      "      S18678       0.50      0.50      0.50         2\n",
      "      S19531       1.00      1.00      1.00         1\n",
      "      S19943       0.00      0.00      0.00         0\n",
      "      S20871       0.50      0.33      0.40         3\n",
      "      S21041       1.00      0.08      0.14        13\n",
      "      S22338       0.00      0.00      0.00         0\n",
      "      S22977       0.17      0.29      0.21         7\n",
      "      S23962       0.00      0.00      0.00         2\n",
      "      S24012       0.80      0.82      0.81       340\n",
      "      S25140       0.70      0.57      0.63        40\n",
      "      S26041       0.50      0.69      0.58        13\n",
      "      S27798       1.00      1.00      1.00         2\n",
      "      S27969       0.00      0.00      0.00         1\n",
      "      S31601       0.00      0.00      0.00         1\n",
      "      S35845       1.00      0.25      0.40         4\n",
      "      S35968       1.00      1.00      1.00         1\n",
      "      S38580       1.00      1.00      1.00         2\n",
      "      S39237       0.00      0.00      0.00         1\n",
      "      S40003       0.94      0.89      0.91        36\n",
      "      S40484       0.29      0.40      0.33         5\n",
      "      S40751       0.33      0.14      0.20         7\n",
      "      S44760       0.33      0.14      0.20         7\n",
      "      S45329       0.00      0.00      0.00         1\n",
      "      S46026       0.50      1.00      0.67         2\n",
      "      S46396       0.00      0.00      0.00         1\n",
      "      S46801       0.00      0.00      0.00         4\n",
      "      S46948       0.00      0.00      0.00         1\n",
      "      S48665       1.00      1.00      1.00         1\n",
      "       S5438       0.00      0.00      0.00         1\n",
      "       S5449       0.50      0.33      0.40         3\n",
      "       S5766       1.00      1.00      1.00         1\n",
      "       S5792       0.00      0.00      0.00         1\n",
      "       S5793       0.59      0.76      0.67        25\n",
      "       S5813       0.00      0.00      0.00         4\n",
      "       S5842       0.00      0.00      0.00         2\n",
      "       S5846       0.38      0.71      0.50         7\n",
      "       S5850       1.00      1.00      1.00         3\n",
      "       S5888       0.47      1.00      0.64         7\n",
      "       S5904       0.69      0.95      0.80        40\n",
      "       S5928       0.00      0.00      0.00         3\n",
      "       S5961       0.40      0.18      0.25        11\n",
      "       S5966       0.33      1.00      0.50         1\n",
      "       S5999       0.90      0.75      0.82        12\n",
      "       S6063       0.85      0.92      0.88        12\n",
      "       S6103       1.00      0.33      0.50         3\n",
      "       S6123       0.00      0.00      0.00         1\n",
      "       S6545       0.80      0.79      0.80        67\n",
      "       S7354       0.50      1.00      0.67         1\n",
      "       S8955       0.00      0.00      0.00         2\n",
      "      T00151       0.00      0.00      0.00         0\n",
      "      T00159       0.00      0.00      0.00         1\n",
      "       T0927       0.00      0.00      0.00         1\n",
      "      T10829       0.00      0.00      0.00         2\n",
      "      T10851       0.86      0.93      0.89        27\n",
      "      T11781       0.00      0.00      0.00         1\n",
      "      T11849       0.00      0.00      0.00         1\n",
      "      T11863       0.50      0.58      0.54        12\n",
      "      T12228       0.43      0.23      0.30        43\n",
      "      T12709       0.00      0.00      0.00         1\n",
      "      T16272       0.33      0.50      0.40         4\n",
      "      T19732       0.73      0.73      0.73        15\n",
      "      T19897       0.67      0.44      0.53         9\n",
      "      T21407       0.70      0.89      0.78        18\n",
      "      T23843       0.50      0.33      0.40         3\n",
      "      T24625       0.67      0.57      0.62         7\n",
      "       T2781       0.87      0.81      0.84        16\n",
      "       T2907       0.50      1.00      0.67         1\n",
      "       T3360       0.00      0.00      0.00         1\n",
      "       T3392       1.00      1.00      1.00         1\n",
      "       T3396       0.00      0.00      0.00         1\n",
      "       T3412       0.89      0.89      0.89         9\n",
      "       T3416       0.90      1.00      0.95         9\n",
      "       T3431       1.00      1.00      1.00        29\n",
      "       T3447       0.67      0.57      0.62         7\n",
      "       T3477       1.00      1.00      1.00         1\n",
      "       T3478       0.82      0.94      0.88        35\n",
      "       T3495       0.00      0.00      0.00         1\n",
      "       T3519       0.00      0.00      0.00         1\n",
      "       T3520       0.00      0.00      0.00         1\n",
      "       T3521       0.50      1.00      0.67         1\n",
      "       T3522       0.00      0.00      0.00         2\n",
      "       T3556       0.62      0.38      0.48        13\n",
      "       T3559       0.00      0.00      0.00         1\n",
      "       T3601       0.94      1.00      0.97        17\n",
      "       T3603       0.00      0.00      0.00         1\n",
      "       T3611       0.41      0.77      0.54        88\n",
      "       T3612       0.71      0.83      0.77         6\n",
      "       T3627       0.00      0.00      0.00         5\n",
      "       T3635       0.56      0.28      0.37        18\n",
      "       T3638       0.40      0.75      0.52         8\n",
      "       T3762       1.00      0.67      0.80         3\n",
      "       T4406       0.40      0.67      0.50         3\n",
      "       T4674       0.67      0.75      0.71         8\n",
      "       T6523       1.00      1.00      1.00         1\n",
      "       T7410       0.00      0.00      0.00         1\n",
      "       T7690       0.25      0.25      0.25         4\n",
      "       T9439       0.00      0.00      0.00         2\n",
      "       U1456       0.76      0.84      0.80        19\n",
      "       U1520       0.00      0.00      0.00         2\n",
      "       U1522       0.00      0.00      0.00         2\n",
      "       U1571       0.67      0.86      0.75         7\n",
      "       U1581       0.00      0.00      0.00         1\n",
      "       U1610       0.71      0.83      0.77        12\n",
      "       U1611       0.00      0.00      0.00         1\n",
      "       U1618       0.40      0.36      0.38        11\n",
      "       U1622       0.00      0.00      0.00         3\n",
      "       U1661       0.00      0.00      0.00         3\n",
      "       U1664       0.74      0.80      0.77        25\n",
      "       U1665       1.00      0.33      0.50         3\n",
      "       U2118       0.77      0.85      0.81        20\n",
      "       U3409       0.00      0.00      0.00         2\n",
      "       U4303       0.46      0.60      0.52        20\n",
      "       U4480       0.00      0.00      0.00         2\n",
      "       U4576       1.00      0.25      0.40         4\n",
      "       U4646       0.00      0.00      0.00         4\n",
      "       U5168       0.00      0.00      0.00         1\n",
      "       U5625       0.84      0.86      0.85        37\n",
      "       U6615       0.86      1.00      0.92         6\n",
      "       V0008       1.00      0.50      0.67         4\n",
      "       V0410       0.50      0.71      0.59        31\n",
      "       V0854       1.00      1.00      1.00         2\n",
      "       V0862       0.50      0.67      0.57         3\n",
      "       V0883       0.50      0.33      0.40         3\n",
      "       V0905       0.00      0.00      0.00         1\n",
      "       V0906       0.00      0.00      0.00         1\n",
      "       V0907       1.00      0.33      0.50         3\n",
      "       V4797       1.00      0.50      0.67         2\n",
      "       V8115       1.00      1.00      1.00         1\n",
      "      W10013       0.00      0.00      0.00         1\n",
      "      W11394       1.00      0.33      0.50         3\n",
      "       W2770       0.67      1.00      0.80         2\n",
      "       W2771       0.00      0.00      0.00         1\n",
      "       W2784       1.00      0.33      0.50         3\n",
      "       W2794       0.00      0.00      0.00         1\n",
      "       W2923       1.00      0.14      0.25         7\n",
      "       W2947       0.00      0.00      0.00         1\n",
      "       W2949       0.00      0.00      0.00         1\n",
      "       W2962       0.50      0.22      0.31         9\n",
      "       W2963       0.86      1.00      0.92         6\n",
      "       W3095       0.83      1.00      0.91         5\n",
      "       W3347       0.00      0.00      0.00         0\n",
      "       Y3758       0.00      0.00      0.00         0\n",
      "       Y4169       0.00      0.00      0.00         1\n",
      "       Y4254       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.66      6723\n",
      "   macro avg       0.39      0.38      0.36      6723\n",
      "weighted avg       0.65      0.66      0.64      6723\n",
      "\n",
      "----------  created histogram  ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\pandas\\core\\indexing.py:691: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  created word report  ---------- \n",
      "\n",
      "----------  saved clf to pkl and zip  ---------- \n",
      "\n",
      "----------  finished training for attribute: gl_approver  ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clf_and_create_reports(\n",
    "    df,\n",
    "    'gl_approver',\n",
    "    SCAN_ID_COL,\n",
    "    RES_DIR,\n",
    "    TODAY,\n",
    "    CLIENT,\n",
    "    COUNTRY,\n",
    "    min_num_samples=1,\n",
    "    histogram=True,\n",
    "    word_report=True,\n",
    "    predictions_excel=PREDICTIONS_EXCEL,\n",
    "    save_clf=SAVE_CLFS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gl_posting_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  start training for attribute: gl_posting_id  ---------- \n",
      "\n",
      "Reduced to 32957 samples from 2 relevant classes. (N=8)\n",
      "Accuracy is 0.9863470873786407.\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          40       0.99      1.00      0.99      6298\n",
      "          50       0.92      0.76      0.83       294\n",
      "\n",
      "    accuracy                           0.99      6592\n",
      "   macro avg       0.95      0.88      0.91      6592\n",
      "weighted avg       0.99      0.99      0.99      6592\n",
      "\n",
      "----------  created histogram  ---------- \n",
      "\n",
      "----------  created word report  ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djourdain\\Anaconda3\\envs\\uniper_py_38\\lib\\site-packages\\pandas\\core\\indexing.py:691: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  saved clf to pkl and zip  ---------- \n",
      "\n",
      "----------  finished training for attribute: gl_posting_id  ----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_clf_and_create_reports(\n",
    "#     df,\n",
    "#     'gl_posting_id',\n",
    "#     SCAN_ID_COL,\n",
    "#     RES_DIR,\n",
    "#     TODAY,\n",
    "#     CLIENT,\n",
    "#     COUNTRY,\n",
    "#     min_num_samples=8, # set N so that only 40 and 50 appear as classes\n",
    "#     histogram=True,\n",
    "#     word_report=True,\n",
    "#     predictions_excel=PREDICTIONS_EXCEL,\n",
    "#     save_clf=SAVE_CLFS\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba53e05a62b8ced6f0f8ce0c83d2d7f60547f32cea2b339741e51997916c6c74"
  },
  "kernelspec": {
   "display_name": "uniper_py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
