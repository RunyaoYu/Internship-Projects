{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Mlaj48Wr3C"
      },
      "source": [
        "# Uniper Account Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLz8KDbiZFwy",
        "outputId": "64af1a59-1235-463e-a56a-539e215cf785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=d7f1fcdd1dd5412953754403b2b1fa1ef002b85f00c535a89be27007a7a83f51\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ],
      "source": [
        "! pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QJuf6DesWr3E",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from docx import Document\n",
        "import matplotlib.pyplot as plt\n",
        "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
        "import pickle\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "import dill as pkl\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "TODAY = date.today().strftime(\"%Y%m%d\")\n",
        "CLIENT = \"Uniper\"\n",
        "MIN_NUM_OF_SAMPLES = 5\n",
        "COUNTRY = 'DE'  # possible choices: DE, UK, AT, SE, UBX\n",
        "SAVE_CLFS = True\n",
        "PREDICTIONS_EXCEL = False\n",
        "RES_DIR = Path(f\"./retraining_october21/{COUNTRY.lower()}\")\n",
        "SCAN_ID_COL = \"gl_document_scan_id\"  # document identifier col used when reducing\n",
        "                                     # global df to relevant examples for attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTIz8YBpXgI9",
        "outputId": "b7e7bca9-3a4b-4633-f686-b7855be92147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GiE7fS74Wr3G"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(RES_DIR):\n",
        "    os.makedirs(RES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GdHVzayLWr3G"
      },
      "outputs": [],
      "source": [
        "# %load training_utils.py\n",
        "\"\"\"This file contains helper functionality to train/evaluate models\n",
        "   and create reports\"\"\"\n",
        "\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from docx import Document\n",
        "import matplotlib.pyplot as plt\n",
        "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
        "\n",
        "\n",
        "def reduce_to_relevant(df: DataFrame, col: str, min_num_samples: int) -> DataFrame:\n",
        "    \"\"\"Reduces df to instances with values in col that appear at least\n",
        "       min_num_samples times and returns reduced df.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame.\n",
        "        col (str): Name of the column that holds the feature of interest.\n",
        "        min_num_samples (int): Minimum number of times a value has to appear\n",
        "            in column 'col'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Reduced DataFrame containing only those values in 'col'\n",
        "            that appear more than 'min_num_samples' times.\n",
        "\n",
        "    \"\"\"\n",
        "    # find rows for values that appear at least min_num_samples times\n",
        "    relevant = [x for x in df[col].value_counts().index\n",
        "                if df[col].value_counts()[x] >= min_num_samples]\n",
        "    # create boolean mask\n",
        "    mask = [(x in relevant) for x in df[col]]\n",
        "\n",
        "    print(\n",
        "        f\"Reduced to {len(df[mask])} samples from {len(relevant)} relevant classes. (N={min_num_samples})\"\n",
        "    )\n",
        "\n",
        "    return df[mask]\n",
        "\n",
        "\n",
        "def get_reduced_df(\n",
        "        df: DataFrame,\n",
        "        feature_col: str,\n",
        "        scan_id_col: str,\n",
        "        min_num_samples: int) -> DataFrame:\n",
        "    \"\"\"Drops duplicates and reduces df to relevant examples that appear\n",
        "       at least min_num_samples times and are unambiguous.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame.\n",
        "        feature_col (str): Name of the column that holds the feature of interest.\n",
        "        scan_id_col (str): Name of the column that holds the unique\n",
        "            document identifier.\n",
        "        min_num_samples (int): Minimum number of times a value has to appear\n",
        "            in column 'feature_col'.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Reduced DataFrame containing only those values in 'feature_col'\n",
        "            that appear more than 'min_num_samples' times and are unambiguous.\n",
        "\n",
        "    \"\"\"\n",
        "    # keep only documents with unambiguous value for this col\n",
        "    df_ = df.drop_duplicates(subset=[scan_id_col, feature_col])\\\n",
        "            .groupby(scan_id_col)\\\n",
        "            .filter(lambda x: len(x) == 1)\n",
        "\n",
        "    return reduce_to_relevant(df_, feature_col, min_num_samples)\n",
        "\n",
        "\n",
        "def split_for_target_col(df, col, test_size=0.2, random_state=666):\n",
        "    \"\"\"\n",
        "    Performs train test split with specified col as target variable.\n",
        "    Returns: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    return train_test_split(\n",
        "        df,\n",
        "        df[col],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_top_n_results_with_confs(\n",
        "        clazzes: Sequence[str],\n",
        "        probs: List[float],\n",
        "        n: int = 1) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Constructs list of (class, proba) tuples for top n results.\n",
        "\n",
        "    Args:\n",
        "        clazzes (Sequence[str]): Sequence of class names as stored in\n",
        "            clf.classes_ attribute of sklearn classifier.\n",
        "        probs (List[float]): List with probabilities for each class in clazzes.\n",
        "        n (int): Number of most probable results to return. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: List of (class, proba) tuples for top n results.\n",
        "\n",
        "    \"\"\"\n",
        "    return sorted(\n",
        "        zip(clazzes, probs),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )[:n]\n",
        "\n",
        "\n",
        "def get_results_for_target(\n",
        "        target_clf: Any,\n",
        "        df: DataFrame) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Computes predictions with provided classifier on DataFrame df.\n",
        "\n",
        "    Args:\n",
        "        target_clf (Any): Sklearn classifier that offers 'predict_proba()'.\n",
        "        df (DataFrame): Input DataFrame as expected by 'target_clf'.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: List of (class, proba) tuples for\n",
        "            instances in df.\n",
        "\n",
        "    \"\"\"\n",
        "    probs = target_clf.predict_proba(df)\n",
        "    clazzes = target_clf.classes_\n",
        "    results = []\n",
        "    for prob_list in probs:\n",
        "        results.append(_get_top_n_results_with_confs(clazzes, prob_list)[0])\n",
        "    return results\n",
        "\n",
        "\n",
        "def _get_text_col_from_df(df: DataFrame):\n",
        "    return df['text']\n",
        "\n",
        "\n",
        "def save_clf_to_disk(\n",
        "        clf,\n",
        "        attribute_name: str,\n",
        "        folder: Path,\n",
        "        date: str,\n",
        "        client: str,\n",
        "        country: str,\n",
        "        min_num_samples: int,\n",
        "        add_zip=True) -> None:\n",
        "\n",
        "    # safe to pkl with full info in file name\n",
        "    pkl_path = folder / f\"{date}_{client}_clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl\"\n",
        "    with open(pkl_path, 'wb') as file:\n",
        "        pkl.dump(clf, file)\n",
        "\n",
        "    if add_zip:\n",
        "        # create zip with non changing name for easy deployment\n",
        "        zip_path = folder / f\"clf_{attribute_name}_{country.lower()}_N_{min_num_samples}.pkl.zip\"\n",
        "        with ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as file:\n",
        "            # second argument avoids recreation of folder structure in zip-archive\n",
        "            file.write(pkl_path, pkl_path.parts[-1])\n",
        "\n",
        "\n",
        "def enrich_kw_list(kw_list):\n",
        "    \"\"\"Enriches keywords by replacing street names with possible synonyms. Since we demand ALL keywords to\n",
        "       be found for a positive match, this function returns a list of keyword lists, one for every possible\n",
        "       synonym.\n",
        "    \n",
        "    \"\"\"\n",
        "    new_kw_lists = [kw_list]\n",
        "    street_synonyms = [\"str.\", \"strasse\", \"straße\"]\n",
        "    for street_syn in street_synonyms:\n",
        "        remaining_syns = [x for x in street_synonyms if x != street_syn]\n",
        "        if any(street_syn in kw for kw in kw_list):\n",
        "            new_kw_lists += [[kw.replace(street_syn, synonym) for kw in kw_list] for synonym in remaining_syns]\n",
        "    return new_kw_lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sM-OMSYeWr3J"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubr4sfWSWr3J",
        "outputId": "0abfa254-3aac-444f-865d-08296dae02b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models trained on Sunday 06. February 2022 with package versions: \n",
            "\n",
            "scikit-learn: 1.0.2\n",
            "dill: 0.3.4\n"
          ]
        }
      ],
      "source": [
        "tday = date.today().strftime(\"%A %d. %B %Y\") \n",
        "print(f\"Models trained on {tday} with package versions: \\n\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"dill: {pkl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "stzaJKF6Wr3K"
      },
      "outputs": [],
      "source": [
        "# dicts used to restrict ground truth later\n",
        "country_group_to_countries = {\n",
        "    \"DE\": [\"DE\"],\n",
        "    \"SE\": [\"SE\"],\n",
        "    \"AT\": [\"AT\"],\n",
        "    \"UK\": [\"GB\"],\n",
        "    \"UBX\": [\"BE\", \"NL\", \"LU\"]    \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "O02m0SaeWr3K",
        "outputId": "b322e03e-eb6c-4916-e6e3-6240b08c8f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(160704, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 32962 samples from 3 relevant classes. (N=5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6f700f8b-da3e-4f4d-84df-6f283a8a2557\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gl_legal_entity_id</th>\n",
              "      <th>gl_accounts_id</th>\n",
              "      <th>gl_cost_center_id</th>\n",
              "      <th>gl_wbs_element_id</th>\n",
              "      <th>gl_vendor_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734082001</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002000582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734934025</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002449868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734067004</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002053354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734934001</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001000092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0037</td>\n",
              "      <td>1734030000</td>\n",
              "      <td>0000037001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001257969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90275</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734999000</td>\n",
              "      <td>2937052000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002006228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90276</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734940001</td>\n",
              "      <td>2937060000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0001435553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90277</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734934012</td>\n",
              "      <td>2937039200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002037197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90278</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734950002</td>\n",
              "      <td>2937059000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002479202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90279</th>\n",
              "      <td>9370</td>\n",
              "      <td>1734934012</td>\n",
              "      <td>2937039200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0002037197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45775 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f700f8b-da3e-4f4d-84df-6f283a8a2557')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f700f8b-da3e-4f4d-84df-6f283a8a2557 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f700f8b-da3e-4f4d-84df-6f283a8a2557');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      gl_legal_entity_id gl_accounts_id  ... gl_wbs_element_id gl_vendor_id\n",
              "0                   0037     1734082001  ...               NaN   0002000582\n",
              "1                   0037     1734934025  ...               NaN   0002449868\n",
              "2                   0037     1734067004  ...               NaN   0002053354\n",
              "3                   0037     1734934001  ...               NaN   0001000092\n",
              "4                   0037     1734030000  ...               NaN   0001257969\n",
              "...                  ...            ...  ...               ...          ...\n",
              "90275               9370     1734999000  ...               NaN   0002006228\n",
              "90276               9370     1734940001  ...               NaN   0001435553\n",
              "90277               9370     1734934012  ...               NaN   0002037197\n",
              "90278               9370     1734950002  ...               NaN   0002479202\n",
              "90279               9370     1734934012  ...               NaN   0002037197\n",
              "\n",
              "[45775 rows x 5 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pickle_texts = \"/content/drive/MyDrive/KPMG/Multiple_Classification/texts_all_rt202110.pkl\"\n",
        "uniper =  \"/content/drive/MyDrive/KPMG/Multiple_Classification/Uniper_GT_09_21.xlsx\"\n",
        "sc21_json = \"/content/drive/MyDrive/KPMG/Multiple_Classification/sc21v8.json\"\n",
        "\n",
        "with open(pickle_texts, \"rb\") as file:\n",
        "    df_lume = pickle.load(file)\n",
        "df_lume.drop_duplicates(inplace=True)\n",
        "print(df_lume.shape)\n",
        "\n",
        "df_ground_truth = pd.read_excel(uniper)\n",
        "len(df_ground_truth)\n",
        "\n",
        "df_ground_truth.dropna(subset=[SCAN_ID_COL], inplace=True)\n",
        "df_ground_truth[SCAN_ID_COL] = df_ground_truth[SCAN_ID_COL].apply(lambda x: x.lower())\n",
        "\n",
        "len(df_ground_truth[SCAN_ID_COL].unique())\n",
        "df_merged = df_lume.merge(df_ground_truth, left_on=[\"filename\"], right_on=[SCAN_ID_COL], how=\"inner\")\n",
        "\n",
        "df_merged.drop_duplicates(inplace=True)\n",
        "df_merged.shape\n",
        "\n",
        "# add left-hand zeros to gl_legal_entity_id, gl_accounts_id, gl_vendor_id\n",
        "df_merged['gl_legal_entity_id'] = df_merged['gl_legal_entity_id'].apply(lambda x: str(int(x)).zfill(4))\n",
        "df_merged['gl_accounts_id'] = df_merged['gl_accounts_id'].apply(lambda x: str(int(x)).zfill(10))\n",
        "df_merged['gl_vendor_id'] = df_merged['gl_vendor_id'].apply(lambda x: str(int(x)).zfill(10))\n",
        "\n",
        "# restict to respective country(group)\n",
        "df = df_merged[\n",
        "    df_merged['le_country_id'].isin(country_group_to_countries[COUNTRY])\n",
        "]\n",
        "\n",
        "df['le_country_id'].value_counts()\n",
        "\n",
        "df['gl_posting_id'] = df['gl_posting_id'].apply(lambda x: str(int(x)))\n",
        "\n",
        "dft = get_reduced_df(df, 'gl_posting_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
        "dft[dft['gl_posting_id'] == '50']\n",
        "df[['gl_legal_entity_id', 'gl_accounts_id', 'gl_cost_center_id', 'gl_wbs_element_id', 'gl_vendor_id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTdp9GP4Wr3L",
        "outputId": "5e279f70-f4a7-4d79-9e52-991d594659e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(loc, value, pi)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finished remapping, writing gl_document_scan_id of mapped invoices to excel file\n"
          ]
        }
      ],
      "source": [
        "with open(sc21_json, 'r') as file:\n",
        "    mapping_json = json.load(file)\n",
        "\n",
        "enrich_kw_list([\"a\", \"b\", \"heinestrasse\"])\n",
        "\n",
        "# apply mapping if df contains any of the relevant legeal entity ids\n",
        "\n",
        "remapped_docs = []\n",
        "\n",
        "if any([key in df['gl_legal_entity_id'].unique() for key in mapping_json.keys()]):\n",
        "    \n",
        "    #print(\"Starting to apply remapping script...\")\n",
        "    \n",
        "    type_to_col = {\n",
        "        'psp': 'gl_wbs_element_id',\n",
        "        'cc': 'gl_cost_center_id',\n",
        "        None: ['gl_cost_center_id', 'gl_order_id', 'gl_wbs_element_id']\n",
        "    }\n",
        "\n",
        "    # from collections import defaultdict\n",
        "    # old_vals = defaultdict(list)\n",
        "    # new_vals = defaultdict(list)\n",
        "\n",
        "    for legal_entity, mapping_list in mapping_json.items():\n",
        "        for mapping in mapping_list:\n",
        "\n",
        "            gl_acc, old_value, old_type, new_value, new_type, gl_vend, keywords = mapping\n",
        "\n",
        "            if old_value is None:\n",
        "                continue\n",
        "                \n",
        "            # added 20211021\n",
        "            if old_type is None and new_type is None:\n",
        "                continue\n",
        "\n",
        "            # fill up with left hand zeros\n",
        "            # gl account auch mit 0 füllen???\n",
        "            if old_type == 'cc':\n",
        "                old_value = old_value.zfill(10)\n",
        "            if new_type == 'cc':\n",
        "                new_value = new_value.zfill(10)\n",
        "            if gl_vend is not None:\n",
        "                gl_vend = gl_vend.zfill(10)\n",
        "\n",
        "            old_col = type_to_col[old_type]\n",
        "            new_col = type_to_col[new_type]\n",
        "\n",
        "            # create filter mask\n",
        "            if old_value == '*':\n",
        "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
        "                    & (df['gl_accounts_id'] == gl_acc)\n",
        "            else:\n",
        "                #print(f\"legal entity {legal_entity}, mapping {mapping}\")\n",
        "                mask = (df['gl_legal_entity_id'] == legal_entity) \\\n",
        "                    & (df['gl_accounts_id'] == gl_acc) \\\n",
        "                    & (df[old_col] == old_value)\n",
        "\n",
        "            if gl_vend is not None:\n",
        "                mask = mask & (df['gl_vendor_id'] == gl_vend)\n",
        "\n",
        "            if keywords:\n",
        "                kw_lists = enrich_kw_list(keywords)\n",
        "                kw_mask = df['text'].apply(\n",
        "                    lambda text: any([all([kw in text for kw in kws]) for kws in kw_lists])\n",
        "                )\n",
        "                mask = mask & kw_mask\n",
        "\n",
        "            # set new values, override old ones if necessary\n",
        "            if not df.loc[mask].empty:\n",
        "                #print(f'found value {old_value} of type {old_type} for legal entity {legal_entity}')\n",
        "                if old_col == new_col:\n",
        "    #                 print('old_col == new_col')\n",
        "                    df.loc[mask, new_col] = new_value\n",
        "    #                 old_vals[legal_entity].append(old_value)\n",
        "    #                 new_vals[legal_entity].append(new_value)\n",
        "                else:\n",
        "    #                 print('old_col != new_col')\n",
        "                    df.loc[mask, old_col] = np.nan\n",
        "                    df.loc[mask, new_col] = new_value\n",
        "                \n",
        "                if isinstance(df.loc[mask, SCAN_ID_COL], str):\n",
        "                    remapped_docs.append(df.loc[mask, SCAN_ID_COL])\n",
        "                else:\n",
        "                    remapped_docs += list(df.loc[mask, SCAN_ID_COL])\n",
        "                \n",
        "    print(f\"finished remapping, writing {SCAN_ID_COL} of mapped invoices to excel file\")\n",
        "    pd.DataFrame.from_dict({SCAN_ID_COL: list(set(remapped_docs))})\\\n",
        "            .to_excel(RES_DIR / f\"{TODAY}_{CLIENT}_remapped_invoices.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Pz0W_DrWr3M"
      },
      "outputs": [],
      "source": [
        "# Runyao's update\n",
        "def get_certain_class_after_vec_country(df_lume, label):\n",
        "    label = label\n",
        "    df_attr = get_reduced_df(df_lume, label, SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
        "    x_train, x_test, y_train, y_test = split_for_target_col_stratified(df_attr, label)\n",
        "    vectorizer = TfidfVectorizer(max_features=20000, max_df=0.75, sublinear_tf=True,)\n",
        "    X_train = x_train['text']\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test = x_test['text']\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return x_train, x_test, y_train, y_test, X_train_vec, X_test_vec\n",
        "\n",
        "\n",
        "def split_for_target_col_stratified(df, col, test_size=0.2, random_state=42,):\n",
        "    return train_test_split(df,\n",
        "        df[col],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df[col]\n",
        "    )\n",
        "\n",
        "\n",
        "def train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl):\n",
        "\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    y_train_pred = model.predict(X_train_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    if save_pkl == True:\n",
        "      pkl_path = \"/content/retraining_october21/model.pkl\"\n",
        "      with open(pkl_path, \"wb\") as file:\n",
        "        pkl.dump(model, file)\n",
        "\n",
        "    print(model.best_params_)\n",
        "    print(model.best_score_)\n",
        "    print(\"Training Accuracy: {:.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test, y_test_pred)))\n",
        "\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # confusion matix\n",
        "    conf_train = confusion_matrix(y_train, y_train_pred)\n",
        "    conf_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "    fg, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4))\n",
        "    sns.heatmap(conf_train, annot=True, fmt=\"d\", ax=ax1)\n",
        "    ax1.set(xlabel=\"predicted label\")\n",
        "    ax1.set(ylabel=\"actual label\")\n",
        "    #ax1.set_xticklabels(['0','1'])\n",
        "    #ax1.set_yticklabels(['0','1'])\n",
        "    ax1.set(title=\"Confusion Matrix for training set\")\n",
        "\n",
        "    sns.heatmap(conf_test, annot=True, fmt=\"d\", ax=ax2)\n",
        "    ax2.set(xlabel=\"predicted label\")\n",
        "    ax2.set(ylabel=\"actual label\")\n",
        "    #ax2.set_xticklabels(['0','1'])\n",
        "    #ax2.set_yticklabels(['0','1'])\n",
        "    ax2.set(title=\"Confusion Matrix for test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Iw_t-L4Ua9Rb"
      },
      "outputs": [],
      "source": [
        "def train_model(label, save_pkl=False):\n",
        "  x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df, label)\n",
        "\n",
        "  lr = LogisticRegression(random_state=42) \n",
        "  parameters = {'penalty': ['none'], 'C':[1], 'solver': ['lbfgs', 'saga']}\n",
        "  model = GridSearchCV(lr, parameters,  verbose=1, scoring='accuracy', cv=10)#n_jobs=-1,\n",
        "  #model.get_params().keys()\n",
        "  train_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZsQFosDWr3M"
      },
      "source": [
        "# gl_accounts_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Llnvq5Wr3N",
        "outputId": "f067fcd8-fcda-44ea-d45f-1d49c44b0a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 28168 samples from 176 relevant classes. (N=5)\n"
          ]
        }
      ],
      "source": [
        "df_gl_id = get_reduced_df(df, 'gl_accounts_id', SCAN_ID_COL, MIN_NUM_OF_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ckeWuHQrWr3N",
        "outputId": "39c31c5f-ffa5-42c6-dd0a-709b30abbee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 28168 samples from 176 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1, 'penalty': 'none', 'solver': 'saga'}\n",
            "0.8022986210636631\n",
            "Training Accuracy: 0.999\n",
            "Test Accuracy: 0.796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  1161182000       1.00      0.50      0.67         6\n",
            "  1200100001       0.64      0.74      0.69        73\n",
            "  1200200001       1.00      0.27      0.42        15\n",
            "  1200300001       1.00      0.86      0.92        14\n",
            "  1240301000       0.75      0.60      0.67         5\n",
            "  1240304000       0.55      0.44      0.49        39\n",
            "  1261184001       1.00      1.00      1.00         6\n",
            "  1540002001       0.77      0.80      0.79       138\n",
            "  1561171002       0.50      0.50      0.50         2\n",
            "  1600003000       1.00      0.60      0.75         5\n",
            "  1600006003       1.00      1.00      1.00         2\n",
            "  1600199008       1.00      0.83      0.91         6\n",
            "  1600798000       0.78      1.00      0.88         7\n",
            "  1625000001       1.00      1.00      1.00        18\n",
            "  1625014000       0.75      0.86      0.80         7\n",
            "  1625014001       1.00      0.80      0.89        10\n",
            "  1625014002       0.00      0.00      0.00         2\n",
            "  1625030000       0.75      1.00      0.86         6\n",
            "  1625099000       1.00      0.67      0.80         3\n",
            "  1625099005       0.80      0.50      0.62         8\n",
            "  1625099011       0.79      1.00      0.88        11\n",
            "  1701006000       1.00      1.00      1.00         7\n",
            "  1701019000       0.75      0.67      0.71         9\n",
            "  1701019003       0.00      0.00      0.00         2\n",
            "  1701019009       1.00      0.50      0.67         2\n",
            "  1701026000       1.00      0.91      0.95        11\n",
            "  1702000000       0.86      0.93      0.90        87\n",
            "  1702003001       0.94      0.83      0.88        18\n",
            "  1702003002       0.85      0.93      0.89        61\n",
            "  1702004000       0.74      0.59      0.66        44\n",
            "  1702004001       0.50      0.36      0.42        11\n",
            "  1702004005       0.80      0.50      0.62         8\n",
            "  1702099000       0.85      0.85      0.85        20\n",
            "  1702099009       1.00      0.67      0.80         3\n",
            "  1702099031       0.76      0.86      0.81        29\n",
            "  1702100000       0.88      1.00      0.94        15\n",
            "  1702107000       1.00      1.00      1.00         6\n",
            "  1702107001       0.00      0.00      0.00         1\n",
            "  1702108000       0.00      0.00      0.00         2\n",
            "  1702200000       0.97      0.97      0.97        37\n",
            "  1702201000       0.67      0.67      0.67         3\n",
            "  1702599001       1.00      1.00      1.00         2\n",
            "  1702902000       0.87      1.00      0.93        20\n",
            "  1702902001       0.50      0.50      0.50         4\n",
            "  1702999000       1.00      0.33      0.50         3\n",
            "  1703000000       0.93      0.95      0.94       478\n",
            "  1703000001       0.80      0.79      0.80       112\n",
            "  1703000011       0.78      0.86      0.82        71\n",
            "  1703004015       1.00      1.00      1.00         6\n",
            "  1703026000       0.91      1.00      0.95        10\n",
            "  1704000000       0.77      0.95      0.85        21\n",
            "  1704000001       0.69      0.55      0.61        20\n",
            "  1704004000       0.53      0.57      0.55        88\n",
            "  1704006000       0.93      0.87      0.90        15\n",
            "  1704008000       0.88      0.93      0.90        15\n",
            "  1704008003       0.00      0.00      0.00         2\n",
            "  1706002000       0.50      0.36      0.42        11\n",
            "  1706004000       1.00      0.75      0.86         4\n",
            "  1706016000       1.00      1.00      1.00         6\n",
            "  1706020000       0.72      0.74      0.73        35\n",
            "  1706998000       0.80      0.86      0.83       480\n",
            "  1706999000       0.69      0.50      0.58        22\n",
            "  1706999001       0.50      0.29      0.36        21\n",
            "  1707000000       1.00      1.00      1.00        10\n",
            "  1707010000       0.75      0.50      0.60         6\n",
            "  1707010001       0.50      0.67      0.57         3\n",
            "  1707016000       0.76      0.87      0.81        15\n",
            "  1707016001       1.00      0.43      0.60         7\n",
            "  1707031000       0.87      0.97      0.92        34\n",
            "  1707031001       0.00      0.00      0.00         2\n",
            "  1707032000       0.55      0.60      0.58        91\n",
            "  1707034000       0.00      0.00      0.00         2\n",
            "  1707034004       0.30      0.16      0.21        19\n",
            "  1707071000       0.78      0.90      0.84        40\n",
            "  1707072000       1.00      0.83      0.91         6\n",
            "  1707072001       0.67      1.00      0.80         2\n",
            "  1707073000       0.69      0.63      0.66        35\n",
            "  1707074000       0.96      1.00      0.98       201\n",
            "  1707078000       0.87      0.87      0.87        45\n",
            "  1707099000       0.64      0.68      0.66       243\n",
            "  1707099001       0.37      0.26      0.31        38\n",
            "  1707099018       0.89      0.89      0.89        73\n",
            "  1710000004       1.00      1.00      1.00         1\n",
            "  1710000013       0.00      0.00      0.00         1\n",
            "  1710299000       1.00      0.75      0.86         8\n",
            "  1710299002       1.00      1.00      1.00         3\n",
            "  1710299006       0.00      0.00      0.00         1\n",
            "  1710299008       0.86      0.92      0.89        13\n",
            "  1733004000       1.00      1.00      1.00         2\n",
            "  1734030000       0.73      0.77      0.75       110\n",
            "  1734030001       0.67      0.50      0.57         4\n",
            "  1734030007       0.60      0.32      0.42        28\n",
            "  1734036000       0.58      0.69      0.63        75\n",
            "  1734042000       1.00      1.00      1.00         1\n",
            "  1734046000       0.95      0.95      0.95        75\n",
            "  1734046004       0.00      0.00      0.00         2\n",
            "  1734060001       0.00      0.00      0.00         1\n",
            "  1734061004       0.57      0.67      0.62         6\n",
            "  1734067000       0.72      0.86      0.78        78\n",
            "  1734067001       0.00      0.00      0.00         1\n",
            "  1734067004       0.80      0.33      0.47        12\n",
            "  1734069000       0.00      0.00      0.00         2\n",
            "  1734073000       1.00      1.00      1.00         3\n",
            "  1734075000       0.61      0.78      0.68        18\n",
            "  1734079000       0.60      0.60      0.60         5\n",
            "  1734079003       1.00      1.00      1.00         2\n",
            "  1734079004       1.00      1.00      1.00         1\n",
            "  1734080000       0.78      0.68      0.73        63\n",
            "  1734080007       0.75      0.60      0.67         5\n",
            "  1734082000       0.95      0.97      0.96       278\n",
            "  1734082001       0.75      0.27      0.40        11\n",
            "  1734084000       0.00      0.00      0.00         1\n",
            "  1734090000       0.84      0.84      0.84        19\n",
            "  1734090001       0.00      0.00      0.00         1\n",
            "  1734092000       0.85      0.90      0.87       100\n",
            "  1734092001       0.80      1.00      0.89        16\n",
            "  1734092002       0.00      0.00      0.00         2\n",
            "  1734098001       0.40      0.22      0.29         9\n",
            "  1734114000       0.74      0.88      0.80        51\n",
            "  1734116000       0.00      0.00      0.00         3\n",
            "  1734116001       0.00      0.00      0.00         1\n",
            "  1734116002       0.88      0.88      0.88         8\n",
            "  1734116004       0.96      1.00      0.98       116\n",
            "  1734116006       0.89      0.89      0.89         9\n",
            "  1734116013       1.00      0.29      0.44         7\n",
            "  1734120000       0.25      0.50      0.33         2\n",
            "  1734130000       0.70      0.67      0.68        39\n",
            "  1734130007       0.00      0.00      0.00         2\n",
            "  1734132000       1.00      0.83      0.91         6\n",
            "  1734134002       0.60      1.00      0.75         3\n",
            "  1734900300       0.80      0.57      0.67         7\n",
            "  1734900500       1.00      1.00      1.00         2\n",
            "  1734908000       0.74      0.80      0.77       168\n",
            "  1734908003       0.50      0.25      0.33        16\n",
            "  1734910001       0.72      0.66      0.69        47\n",
            "  1734910002       0.47      0.53      0.50        43\n",
            "  1734910003       0.60      0.55      0.57        22\n",
            "  1734910006       0.50      0.12      0.20         8\n",
            "  1734912000       1.00      0.50      0.67         2\n",
            "  1734914000       0.96      0.93      0.95       170\n",
            "  1734934000       1.00      0.25      0.40         4\n",
            "  1734934001       0.71      0.47      0.57        36\n",
            "  1734934003       0.89      0.89      0.89       159\n",
            "  1734934008       0.00      0.00      0.00         1\n",
            "  1734934010       0.00      0.00      0.00         1\n",
            "  1734934012       0.70      0.76      0.73       143\n",
            "  1734934016       1.00      1.00      1.00         6\n",
            "  1734934024       1.00      1.00      1.00         2\n",
            "  1734934025       1.00      0.20      0.33         5\n",
            "  1734934027       0.00      0.00      0.00         1\n",
            "  1734940000       0.80      0.89      0.84       173\n",
            "  1734940001       1.00      0.67      0.80         3\n",
            "  1734940003       0.00      0.00      0.00         5\n",
            "  1734944000       0.76      0.88      0.82        51\n",
            "  1734944001       1.00      0.25      0.40         4\n",
            "  1734946000       1.00      1.00      1.00         2\n",
            "  1734950000       0.86      0.90      0.88        21\n",
            "  1734950001       1.00      0.33      0.50         3\n",
            "  1734950002       0.65      0.62      0.63        71\n",
            "  1734950004       0.30      0.25      0.27        12\n",
            "  1734961001       1.00      0.67      0.80         3\n",
            "  1734961002       1.00      0.90      0.95        10\n",
            "  1734970100       0.83      0.65      0.73        31\n",
            "  1734999000       0.46      0.32      0.38        34\n",
            "  1734999001       0.75      0.67      0.71         9\n",
            "  1734999003       1.00      0.33      0.50         3\n",
            "  1734999004       1.00      0.80      0.89        10\n",
            "  1734999005       0.00      0.00      0.00         2\n",
            "  1734999018       0.00      0.00      0.00         8\n",
            "  1734999027       0.25      1.00      0.40         1\n",
            "  1751098001       0.00      0.00      0.00         1\n",
            "  1751098002       0.83      1.00      0.91         5\n",
            "  1751098004       1.00      1.00      1.00         3\n",
            "  1751098011       0.00      0.00      0.00         1\n",
            "  1911250002       0.55      0.32      0.41        34\n",
            "  1911250003       0.70      0.78      0.74         9\n",
            "\n",
            "    accuracy                           0.80      5634\n",
            "   macro avg       0.68      0.62      0.63      5634\n",
            "weighted avg       0.79      0.80      0.79      5634\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "label = 'gl_accounts_id' # define label here\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgxtGkyNWr3N"
      },
      "source": [
        "# gl_legal_entity_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "QH-kjT8VCtsk",
        "outputId": "5efae8ad-e273-4f82-eec4-725cf5da8e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 28164 samples from 21 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1, 'penalty': 'none', 'solver': 'saga'}\n",
            "0.981802927852088\n",
            "Training Accuracy: 1.000\n",
            "Test Accuracy: 0.985\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        0037       1.00      1.00      1.00        56\n",
            "        0065       1.00      1.00      1.00       299\n",
            "        0301       0.98      0.99      0.98      1574\n",
            "        0303       0.99      1.00      0.99      1216\n",
            "        0330       0.97      0.97      0.97       314\n",
            "        0362       0.99      0.96      0.98       109\n",
            "        0377       1.00      1.00      1.00        32\n",
            "        0601       0.99      0.98      0.99       292\n",
            "        0801       0.99      0.98      0.98       898\n",
            "        0806       1.00      0.98      0.99        42\n",
            "        0811       1.00      0.92      0.96        12\n",
            "        0821       0.99      0.99      0.99       209\n",
            "        2101       1.00      0.98      0.99       260\n",
            "        2111       1.00      1.00      1.00       122\n",
            "        3104       0.97      0.89      0.93        35\n",
            "        3401       0.85      0.92      0.88        12\n",
            "        3420       1.00      1.00      1.00        44\n",
            "        9301       0.33      0.25      0.29         4\n",
            "        9310       0.90      0.85      0.88        41\n",
            "        9351       0.95      1.00      0.97        37\n",
            "        9370       0.96      0.88      0.92        25\n",
            "\n",
            "    accuracy                           0.98      5633\n",
            "   macro avg       0.94      0.93      0.94      5633\n",
            "weighted avg       0.98      0.98      0.98      5633\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "label = 'gl_legal_entity_id' # define label here\n",
        "train_model(label, save_pkl=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cePEkGzjWr3O"
      },
      "source": [
        "# gl_tax_code_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKodEQ09Wr3P",
        "outputId": "b6db9b56-0184-4bf4-f574-97e6ecf1ecff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 27886 samples from 32 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1, 'penalty': 'none', 'solver': 'saga'}\n",
            "0.9185044812899361\n",
            "Training Accuracy: 0.999\n",
            "Test Accuracy: 0.922\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0W       1.00      0.80      0.89        10\n",
            "          11       0.95      0.95      0.95        37\n",
            "          51       0.94      0.96      0.95      3399\n",
            "          52       0.87      0.87      0.87       550\n",
            "          53       0.76      0.73      0.74       183\n",
            "          54       0.00      0.00      0.00         3\n",
            "          56       0.71      0.56      0.63        27\n",
            "          80       0.97      0.97      0.97       484\n",
            "          99       0.90      0.85      0.87       569\n",
            "          A1       1.00      0.86      0.92         7\n",
            "          B2       0.00      0.00      0.00         1\n",
            "          C2       0.89      0.87      0.88        62\n",
            "          C3       1.00      0.75      0.86        20\n",
            "          C6       1.00      1.00      1.00        25\n",
            "          C8       0.75      1.00      0.86         3\n",
            "          CB       0.67      0.57      0.62         7\n",
            "          CH       0.92      0.85      0.89        41\n",
            "          CN       1.00      1.00      1.00         1\n",
            "          E1       0.82      0.60      0.69        15\n",
            "          E2       0.88      0.88      0.88         8\n",
            "          HK       1.00      1.00      1.00        12\n",
            "          HM       0.80      1.00      0.89         4\n",
            "          HN       1.00      1.00      1.00         2\n",
            "          HW       0.00      0.00      0.00         1\n",
            "          I8       1.00      1.00      1.00         1\n",
            "          J1       0.78      1.00      0.88         7\n",
            "          JA       1.00      1.00      1.00         5\n",
            "          N0       0.67      0.40      0.50         5\n",
            "          N1       0.33      0.15      0.21        20\n",
            "          N2       0.00      0.00      0.00         3\n",
            "          ND       0.40      0.33      0.36         6\n",
            "          R1       0.90      0.95      0.93        60\n",
            "\n",
            "    accuracy                           0.92      5578\n",
            "   macro avg       0.75      0.72      0.73      5578\n",
            "weighted avg       0.92      0.92      0.92      5578\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "label = 'gl_tax_code_id' # define label here\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6FTpvn9C2HAl",
        "outputId": "7f5ee236-004a-4ced-c33e-42585c3895e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 23189 samples from 768 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-709b8dc4d421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gl_vendor_id'\u001b[0m \u001b[0;31m# define label here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-bc25ad4e0170>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(label, save_pkl)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#n_jobs=-1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#model.get_params().keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-339f059afee3>\u001b[0m in \u001b[0;36mtrain_statistical_model\u001b[0;34m(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             )\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start_coef_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         )\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0mwarm_start_sag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mis_saga\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"saga\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             )\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mintercept_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mis_saga\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "label = 'gl_vendor_id' # define label here\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxb0uaSAWr3Q"
      },
      "source": [
        "# Train cost element classifiers per company code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oah1S1Wr3Q"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw6ynIIqWr3R",
        "outputId": "a67ca97c-bf18-4e05-b4f2-7617dc0d2bcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(31656, 31656)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare cost element column\n",
        "\n",
        "df_kostenstelle = df[df['gl_cost_center_id'].notna()]\n",
        "df_kostenstelle['Target'] = df_kostenstelle['gl_cost_center_id']\n",
        "\n",
        "df_psp = df[df['gl_wbs_element_id'].notna()]\n",
        "df_psp['Target'] = df_psp['gl_wbs_element_id']\n",
        "\n",
        "df_auftrag = df[df['gl_order_id'].notna()]\n",
        "df_auftrag['Target'] = df_auftrag['gl_order_id']\n",
        "\n",
        "df_pka = pd.concat([df_auftrag,df_kostenstelle, df_psp], axis=0)\n",
        "df_pka['Target'] = df_pka['Target'].apply(lambda x: str(x))\n",
        "\n",
        "df_pka.dropna(subset=['text'], inplace=True)\n",
        "df_pka.drop_duplicates(subset=['text'], inplace=True)\n",
        "\n",
        "len(df_pka),len(df_pka['text'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-GkQOTikWr3R"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/retraining_october21/model.pkl\", \"rb\") as file:\n",
        "    clf_bukr = pkl.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdakO5t9Wr3R"
      },
      "source": [
        "## Train classifiers (always uses N=1 here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqLoduwcWr3S",
        "outputId": "4ce4eb86-fe98-42a1-e30b-27104ac89dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "company code 0037: 279 entries for 6 classes\n",
            "company code 0065: 1721 entries for 174 classes\n",
            "company code 0301: 7176 entries for 782 classes\n",
            "company code 0303: 8809 entries for 1207 classes\n",
            "company code 0330: 1605 entries for 107 classes\n",
            "company code 0362: 563 entries for 36 classes\n",
            "company code 0377: 173 entries for 23 classes\n",
            "company code 0601: 2245 entries for 276 classes\n",
            "company code 0801: 4789 entries for 568 classes\n",
            "company code 0806: 229 entries for 20 classes\n",
            "company code 0811: 53 entries for 10 classes\n",
            "company code 0821: 1069 entries for 105 classes\n",
            "company code 2101: 1321 entries for 137 classes\n",
            "company code 2111: 624 entries for 57 classes\n",
            "company code 3104: 172 entries for 21 classes\n",
            "company code 3401: 62 entries for 1 classes\n",
            "company code 3420: 279 entries for 20 classes\n",
            "company code 9301: 23 entries for 9 classes\n",
            "company code 9310: 187 entries for 61 classes\n",
            "company code 9351: 168 entries for 21 classes\n",
            "company code 9370: 104 entries for 18 classes\n"
          ]
        }
      ],
      "source": [
        "local_min_num = 1\n",
        "\n",
        "for company_code in clf_bukr.classes_:\n",
        "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
        "    num_classes = len(df_cost_elem[\"Target\"].unique())\n",
        "    print(f\"company code {str(company_code).zfill(4)}: {len(df_cost_elem)} entries for {num_classes} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZSVVncmAhNsm"
      },
      "outputs": [],
      "source": [
        "def train_com_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl):\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    y_train_pred = model.predict(X_train_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    if save_pkl == True:\n",
        "      pkl_path = \"/content/retraining_october21/model.pkl\"\n",
        "      with open(pkl_path, \"wb\") as file:\n",
        "        pkl.dump(model, file)\n",
        "\n",
        "    print(\"Training Accuracy: {:.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "    print(\"Test Accuracy: {:.3f}\".format(accuracy_score(y_test, y_test_pred)))\n",
        "    print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qYWAhMJVOW2A"
      },
      "outputs": [],
      "source": [
        "def train_com_model(label, save_pkl=False):\n",
        "  x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_ce, label)\n",
        "  model = LogisticRegression(solver='saga', random_state=42) \n",
        "  train_com_statistical_model(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KksReKi6Wr3S",
        "outputId": "0305b786-a07d-4375-edfb-ca9562c97f05",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company Code: 0037\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 279 samples from 6 relevant classes. (N=1)\n",
            "Reduced to 271 samples from 2 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.940\n",
            "Test Accuracy: 0.945\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  0000037001       0.95      1.00      0.97        52\n",
            "  0000037330       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.95        55\n",
            "   macro avg       0.47      0.50      0.49        55\n",
            "weighted avg       0.89      0.95      0.92        55\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0065\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1721 samples from 174 relevant classes. (N=1)\n",
            "Reduced to 1498 samples from 37 relevant classes. (N=5)\n",
            "Training Accuracy: 0.871\n",
            "Test Accuracy: 0.783\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "0065.1206O3.201.00.10.03       0.77      1.00      0.87        10\n",
            "0065.1206O3.201.00.10.06       1.00      0.50      0.67         2\n",
            "0065.1206OA.203.00.10.02       0.00      0.00      0.00         1\n",
            "0065.1206OA.203.00.10.07       1.00      0.40      0.57         5\n",
            "0065.1206OA.203.00.10.08       0.00      0.00      0.00         4\n",
            "0065.1206OA.203.00.10.14       0.60      1.00      0.75         3\n",
            "0065.1206OA.207.00.10.07       0.67      0.50      0.57         4\n",
            "0065.1206OA.207.00.10.08       0.00      0.00      0.00         1\n",
            "0065.1206OA.208.00.10.07       1.00      1.00      1.00         4\n",
            "0065.1209FA.220.00.97.11       1.00      1.00      1.00        12\n",
            "0065.1602JA.206.00.60.10       0.00      0.00      0.00         1\n",
            "0065.1608FA.200.00.00.10       0.00      0.00      0.00         2\n",
            "0065.1613OA.202.05.69.10       0.00      0.00      0.00         1\n",
            "0065.SALCOM.200.00.10.10       1.00      1.00      1.00         6\n",
            "            5550007411.0       0.00      0.00      0.00         1\n",
            "            5550007412.0       0.00      0.00      0.00         1\n",
            "            5560004423.0       0.00      0.00      0.00         1\n",
            "              K0065_1001       0.00      0.00      0.00         1\n",
            "              K0065_1004       1.00      0.30      0.46        10\n",
            "              K0065_1006       0.00      0.00      0.00         1\n",
            "              K0065_1007       0.90      0.90      0.90        20\n",
            "              K0065_1008       0.72      0.98      0.83        48\n",
            "              K0065_1011       0.82      0.50      0.62        18\n",
            "              K0065_1013       0.83      0.97      0.89        30\n",
            "              K0065_1102       0.00      0.00      0.00         5\n",
            "              K0065_1108       1.00      0.90      0.95        20\n",
            "              K0065_1201       1.00      1.00      1.00         9\n",
            "              K0065_1315       0.00      0.00      0.00         1\n",
            "              K0065_1401       1.00      0.33      0.50         6\n",
            "              K0065_1402       0.54      1.00      0.70        32\n",
            "              K0065_1403       0.83      1.00      0.91         5\n",
            "              K0065_1404       0.86      0.67      0.75         9\n",
            "              K0065_1405       1.00      0.67      0.80         3\n",
            "              K0065_1406       1.00      0.88      0.93         8\n",
            "              K0065_1416       0.00      0.00      0.00         3\n",
            "              K0065_1420       0.71      0.83      0.77         6\n",
            "              K0065_1423       1.00      0.50      0.67         6\n",
            "\n",
            "                accuracy                           0.78       300\n",
            "               macro avg       0.55      0.48      0.49       300\n",
            "            weighted avg       0.76      0.78      0.74       300\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0301\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 7176 samples from 782 relevant classes. (N=1)\n",
            "Reduced to 6173 samples from 238 relevant classes. (N=5)\n",
            "Training Accuracy: 0.761\n",
            "Test Accuracy: 0.681\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "              0010100902       0.87      1.00      0.93        26\n",
            "              0010100950       0.00      0.00      0.00         1\n",
            "              0011100810       0.00      0.00      0.00         3\n",
            "              0012000163       0.67      0.80      0.73        10\n",
            "              0012000355       0.00      0.00      0.00         4\n",
            "              0012000580       0.00      0.00      0.00         1\n",
            "              0012000591       0.00      0.00      0.00         1\n",
            "              0012000603       1.00      1.00      1.00         2\n",
            "              0012006201       0.00      0.00      0.00         2\n",
            "              0022100133       0.00      0.00      0.00         1\n",
            "              0028000020       0.00      0.00      0.00         4\n",
            "              0028000050       0.57      0.50      0.53         8\n",
            "              0041003122       1.00      0.50      0.67         8\n",
            "              0041100723       0.00      0.00      0.00         1\n",
            "              0042004122       0.00      0.00      0.00         2\n",
            "              0042004411       1.00      0.50      0.67         4\n",
            "              0042004441       0.00      0.00      0.00         1\n",
            "              0042004511       0.00      0.00      0.00         1\n",
            "              0042100104       0.00      0.00      0.00         4\n",
            "              0042100111       0.00      0.00      0.00         2\n",
            "              0042100191       1.00      0.25      0.40         4\n",
            "              0042100221       0.00      0.00      0.00         3\n",
            "              0042100231       0.00      0.00      0.00         3\n",
            "              0042100561       0.00      0.00      0.00         1\n",
            "              0042100641       0.00      0.00      0.00         1\n",
            "              0042100811       0.00      0.00      0.00         1\n",
            "              0043005030       0.00      0.00      0.00         4\n",
            "              0043006030       0.00      0.00      0.00         3\n",
            "              0043100231       1.00      0.67      0.80         3\n",
            "              0046005030       0.00      0.00      0.00         1\n",
            "              0046090122       1.00      0.75      0.86         8\n",
            "              0046090511       0.00      0.00      0.00         2\n",
            "              0046100103       0.00      0.00      0.00         1\n",
            "              0046100104       0.75      0.86      0.80         7\n",
            "              0046100111       1.00      0.25      0.40         4\n",
            "              0046100191       0.00      0.00      0.00         5\n",
            "              0046100221       0.67      1.00      0.80         8\n",
            "              0046100231       0.50      0.50      0.50         4\n",
            "              0046100421       0.00      0.00      0.00         1\n",
            "              0046100621       0.53      0.90      0.67        10\n",
            "              0046100731       0.43      0.90      0.58        10\n",
            "              0046100821       0.00      0.00      0.00         1\n",
            "              0047001030       1.00      0.09      0.17        11\n",
            "              0047001122       0.46      0.60      0.52        10\n",
            "              0047001411       0.50      1.00      0.67         8\n",
            "              0047001441       0.00      0.00      0.00         1\n",
            "              0047001511       1.00      1.00      1.00         7\n",
            "              0047100103       0.00      0.00      0.00         2\n",
            "              0047100104       1.00      0.75      0.86         4\n",
            "              0047100111       0.50      0.50      0.50         8\n",
            "              0047100191       0.43      0.43      0.43         7\n",
            "              0047100221       0.00      0.00      0.00         4\n",
            "              0047100231       0.00      0.00      0.00         3\n",
            "              0047100421       0.00      0.00      0.00         2\n",
            "              0047100561       0.00      0.00      0.00         2\n",
            "              0047100621       0.60      0.60      0.60         5\n",
            "              0047100662       0.00      0.00      0.00         1\n",
            "              0047100723       0.00      0.00      0.00         2\n",
            "              0047100831       0.50      0.67      0.57         6\n",
            "              0049000221       0.00      0.00      0.00         2\n",
            "              0049001040       0.80      1.00      0.89         8\n",
            "              0052001040       1.00      0.50      0.67         4\n",
            "              0062020122       0.00      0.00      0.00         2\n",
            "              0062020441       1.00      0.33      0.50         3\n",
            "              0062020451       1.00      1.00      1.00        11\n",
            "              0062020541       0.00      0.00      0.00         1\n",
            "              0062023030       1.00      0.67      0.80         6\n",
            "              0062069031       1.00      1.00      1.00         3\n",
            "              0062100111       0.38      0.67      0.48         9\n",
            "              0062100191       1.00      1.00      1.00         6\n",
            "              0062100221       0.00      0.00      0.00         5\n",
            "              0062100231       0.43      1.00      0.60        31\n",
            "              0062100531       1.00      0.50      0.67         2\n",
            "              0062100723       0.00      0.00      0.00         1\n",
            "              0063041030       0.00      0.00      0.00         3\n",
            "              0063100102       0.00      0.00      0.00         1\n",
            "              0063100104       1.00      0.67      0.80         3\n",
            "              0063100221       0.00      0.00      0.00         1\n",
            "              0063100222       0.00      0.00      0.00         1\n",
            "              0063100231       0.00      0.00      0.00         5\n",
            "              0063100723       0.25      0.40      0.31         5\n",
            "              0064090511       0.00      0.00      0.00         1\n",
            "              0064100104       0.00      0.00      0.00         1\n",
            "              0064100111       0.00      0.00      0.00         2\n",
            "              0064100191       0.00      0.00      0.00         1\n",
            "              0064100221       1.00      0.67      0.80         6\n",
            "              0064100231       0.00      0.00      0.00         1\n",
            "              0064100621       0.50      0.33      0.40         3\n",
            "              0065000301       1.00      1.00      1.00         5\n",
            "              0065090122       1.00      1.00      1.00         7\n",
            "              0065090411       1.00      1.00      1.00         5\n",
            "              0065090511       1.00      1.00      1.00        15\n",
            "              0065100103       1.00      0.67      0.80         3\n",
            "              0065100104       0.62      0.83      0.71         6\n",
            "              0065100191       0.75      0.75      0.75         4\n",
            "              0065100221       0.00      0.00      0.00         3\n",
            "              0065100231       1.00      0.83      0.91         6\n",
            "              0065100232       1.00      0.75      0.86         4\n",
            "              0065100371       1.00      0.50      0.67         2\n",
            "              0065100561       1.00      0.50      0.67         4\n",
            "              0065100621       1.00      0.50      0.67         4\n",
            "              0065100641       0.00      0.00      0.00         2\n",
            "              0071090122       0.70      0.88      0.78         8\n",
            "              0071100104       0.00      0.00      0.00         3\n",
            "              0071100111       0.50      0.25      0.33         4\n",
            "              0071100221       0.00      0.00      0.00         4\n",
            "              0071100321       0.00      0.00      0.00         1\n",
            "              0071100641       0.00      0.00      0.00         2\n",
            "              0076090122       0.70      0.64      0.67        11\n",
            "              0076100221       0.57      0.50      0.53         8\n",
            "              0076100231       0.20      0.25      0.22         4\n",
            "              0076100321       0.00      0.00      0.00         1\n",
            "              0076100421       1.00      1.00      1.00         1\n",
            "              0076100821       1.00      1.00      1.00         1\n",
            "              0077003030       0.00      0.00      0.00         1\n",
            "              0077090122       0.53      0.71      0.61        14\n",
            "              0077090511       0.00      0.00      0.00         1\n",
            "              0077100111       0.00      0.00      0.00         2\n",
            "              0077100221       0.43      0.93      0.59        14\n",
            "              0077100231       1.00      0.33      0.50         3\n",
            "              0077100621       1.00      0.33      0.50         3\n",
            "              0077100641       0.00      0.00      0.00         2\n",
            "              0079090122       1.00      0.50      0.67         2\n",
            "              0079100231       0.00      0.00      0.00         1\n",
            "      0301.A62610.020.01       0.00      0.00      0.00         1\n",
            "         0301.A63310.045       0.00      0.00      0.00         2\n",
            "      0301.B61800.010.02       0.57      1.00      0.73         4\n",
            "      0301.G41000.999.55       0.58      1.00      0.74         7\n",
            "      0301.G42000.999.55       1.00      1.00      1.00         5\n",
            "      0301.G43001.999.55       0.62      1.00      0.77         5\n",
            "      0301.G43002.999.55       0.43      0.60      0.50         5\n",
            "      0301.G46000.120.55       0.73      1.00      0.84         8\n",
            "      0301.G46000.999.55       0.00      0.00      0.00         1\n",
            "      0301.G46000.999.57       1.00      1.00      1.00         1\n",
            "      0301.G47001.999.55       0.62      0.83      0.71         6\n",
            "      0301.G47001.999.56       1.00      1.00      1.00        10\n",
            "      0301.G62000.085.55       1.00      1.00      1.00         4\n",
            "      0301.G62000.999.62       1.00      0.50      0.67         4\n",
            "      0301.G63000.084.55       0.78      1.00      0.88         7\n",
            "      0301.G64000.098.56       1.00      0.83      0.91         6\n",
            "      0301.G65000.041.60       1.00      1.00      1.00         5\n",
            "      0301.G65000.999.62       0.60      1.00      0.75         3\n",
            "      0301.G71000.999.55       0.00      0.00      0.00         3\n",
            "      0301.G73700.999.55       0.75      1.00      0.86         3\n",
            "      0301.G76000.999.55       1.00      0.33      0.50         6\n",
            "      0301.G77000.999.55       0.50      0.17      0.25         6\n",
            "      0301.G99999.100.99       0.00      0.00      0.00         2\n",
            "   0301.O77904.004.14.02       1.00      1.00      1.00         1\n",
            "   0301.R41420.021.02.01       0.75      0.50      0.60         6\n",
            "   0301.R46610.010.06.31       1.00      1.00      1.00         1\n",
            "   0301.R46610.010.06.32       0.00      0.00      0.00         1\n",
            "      0301.R62620.011.06       0.00      0.00      0.00         1\n",
            "      0301.R65601.001.03       1.00      1.00      1.00         1\n",
            "      0301.R71602.003.01       0.00      0.00      0.00         1\n",
            "      0301.R71602.003.06       1.00      1.00      1.00         1\n",
            "   0301.T12050.021.42.14       0.67      1.00      0.80         4\n",
            "   0301.T12050.021.47.11       0.00      0.00      0.00         1\n",
            "   0301.T12050.024.46.05       0.83      1.00      0.91         5\n",
            "      0301.T46801.005.01       0.00      0.00      0.00         1\n",
            "         0301.W12014.011       0.50      1.00      0.67         2\n",
            "         0301.W12157.051       0.89      0.47      0.62        17\n",
            "         0301.W12157.052       0.55      0.99      0.71       127\n",
            "         0301.W12157.059       0.00      0.00      0.00         9\n",
            "         0301.W12158.008       1.00      1.00      1.00        18\n",
            "         0301.W12601.001       1.00      0.33      0.50         3\n",
            "      0301.W28331.011.50       1.00      1.00      1.00         2\n",
            "         0301.W42100.004       1.00      1.00      1.00        13\n",
            "         0301.W46304.004       1.00      1.00      1.00         3\n",
            "         0301.W46304.005       0.50      0.50      0.50         2\n",
            "         0301.W71301.003       1.00      0.50      0.67         2\n",
            "         0301.W76301.004       0.67      1.00      0.80         6\n",
            "         0301.W76301.005       1.00      0.50      0.67         2\n",
            "         0301.W77303.004       1.00      0.67      0.80         6\n",
            "         0301.W77303.005       0.67      1.00      0.80         2\n",
            "         0301.W77304.004       1.00      1.00      1.00         5\n",
            "         0301.W77602.001       0.00      0.00      0.00         2\n",
            "         0301.W77602.004       1.00      0.50      0.67         2\n",
            "         0301.X12709.001       0.67      1.00      0.80         4\n",
            "         0301.Y12003.100       1.00      1.00      1.00        35\n",
            "      0301.Y12703.020.09       1.00      1.00      1.00         1\n",
            "         0301.Y12707.011       0.67      1.00      0.80         2\n",
            "         0301.Y12710.002       1.00      1.00      1.00         3\n",
            "         0301.Y12800.102       1.00      1.00      1.00         2\n",
            "      0301.Y28313.001.02       0.00      0.00      0.00         1\n",
            "      0301.Y28314.041.01       0.00      0.00      0.00         1\n",
            "      0301.Y28314.041.10       1.00      1.00      1.00         3\n",
            "      0301.Y28331.012.01       0.00      0.00      0.00         2\n",
            "      0301.Y28341.005.51       0.00      0.00      0.00         1\n",
            "   0301.Y28570.042.01.02       1.00      1.00      1.00         2\n",
            "0301.Y28575.042.01.01.02       0.00      0.00      0.00         1\n",
            "0301.Y28576.042.01.01.02       1.00      0.33      0.50         3\n",
            "0301.Y28578.042.01.01.02       1.00      1.00      1.00         4\n",
            "0301.Y28579.022.01.01.02       1.00      1.00      1.00         2\n",
            "0301.Y28579.042.01.01.02       0.50      0.15      0.23        34\n",
            "0301.Y28580.042.01.01.02       0.33      0.11      0.17         9\n",
            "0301.Y28582.042.01.01.02       0.72      0.99      0.83        97\n",
            "      0301.Y29006.001.05       1.00      0.33      0.50         3\n",
            "      0301.Y29009.001.05       0.00      0.00      0.00         1\n",
            "      0301.Y29009.001.06       0.00      0.00      0.00         1\n",
            "      0301.Y29907.001.05       0.60      1.00      0.75         3\n",
            "         0301.Y43801.002       0.00      0.00      0.00         1\n",
            "      0301.Y47501.001.01       1.00      1.00      1.00         1\n",
            "         0301.Y47813.001       0.00      0.00      0.00         1\n",
            "      0301.Y62559.001.01       0.00      0.00      0.00         1\n",
            "      0301.Y62559.003.01       0.00      0.00      0.00         1\n",
            "      0301.Y62559.004.01       0.00      0.00      0.00         1\n",
            "   0301.Y63501.024.03.30       1.00      0.67      0.80         3\n",
            "   0301.Y63501.028.40.03       0.00      0.00      0.00         1\n",
            "         0301.Y65302.002       0.00      0.00      0.00         1\n",
            "         0301.Y65707.001       1.00      1.00      1.00         2\n",
            "         0301.Y73700.010       0.50      0.25      0.33         4\n",
            "      0301.Y73700.020.05       0.00      0.00      0.00         1\n",
            "      0301.Y73700.040.03       1.00      1.00      1.00         2\n",
            "         0301.Y74800.010       0.00      0.00      0.00         1\n",
            "              1030110500       1.00      0.50      0.67         4\n",
            "              1030120210       0.40      0.67      0.50         3\n",
            "              1030120220       0.78      1.00      0.88         7\n",
            "              1030120400       1.00      1.00      1.00         4\n",
            "              1030131000       0.60      1.00      0.75         6\n",
            "              1030131100       0.90      0.75      0.82        12\n",
            "              1030140200       0.00      0.00      0.00         1\n",
            "              2030112200       0.67      0.80      0.73         5\n",
            "              2030116000       1.00      0.67      0.80         3\n",
            "              2030121100       1.00      1.00      1.00         7\n",
            "              2030121200       1.00      1.00      1.00         2\n",
            "              2030122000       0.00      0.00      0.00         2\n",
            "              2030122100       0.36      0.56      0.43         9\n",
            "              2030122300       0.00      0.00      0.00         2\n",
            "              2030122400       0.00      0.00      0.00         2\n",
            "              2030122500       0.00      0.00      0.00         2\n",
            "              2030123000       0.00      0.00      0.00         2\n",
            "              2030123400       0.75      0.75      0.75         4\n",
            "              2030127000       0.00      0.00      0.00         3\n",
            "              2030128100       0.78      0.50      0.61        14\n",
            "              2030134200       0.00      0.00      0.00         1\n",
            "              2030138600       0.00      0.00      0.00         1\n",
            "              2030143100       0.00      0.00      0.00         2\n",
            "              2030144100       0.80      0.57      0.67         7\n",
            "\n",
            "                accuracy                           0.68      1235\n",
            "               macro avg       0.47      0.44      0.43      1235\n",
            "            weighted avg       0.63      0.68      0.62      1235\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0303\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 8809 samples from 1207 relevant classes. (N=1)\n",
            "Reduced to 7314 samples from 350 relevant classes. (N=5)\n",
            "Training Accuracy: 0.539\n",
            "Test Accuracy: 0.430\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0110110000       0.44      0.47      0.45        15\n",
            "        0110510000       1.00      0.62      0.77         8\n",
            "        0111510000       0.75      0.75      0.75         4\n",
            "        0112010000       1.00      0.77      0.87        13\n",
            "        0112510000       1.00      0.79      0.88        14\n",
            "        0114010000       1.00      0.50      0.67         2\n",
            "        0121014000       0.71      0.83      0.77         6\n",
            "        0121515000       0.83      0.83      0.83         6\n",
            "        0121520500       0.00      0.00      0.00         1\n",
            "        0122010000       0.85      0.52      0.65        21\n",
            "        0122015000       0.00      0.00      0.00         1\n",
            "        0122015200       0.00      0.00      0.00         2\n",
            "        0122015300       1.00      0.83      0.91        12\n",
            "        0122015330       0.00      0.00      0.00         3\n",
            "        0122015350       0.31      0.80      0.44        15\n",
            "        0122015400       0.00      0.00      0.00         1\n",
            "        0122015700       0.00      0.00      0.00         1\n",
            "        0122020000       0.75      0.33      0.46         9\n",
            "        0122020500       0.60      0.60      0.60         5\n",
            "        0122021500       0.75      0.75      0.75         4\n",
            "        0122022000       0.91      0.71      0.80        14\n",
            "        0122022010       0.00      0.00      0.00         2\n",
            "        0122022020       0.00      0.00      0.00         1\n",
            "        0122022030       1.00      0.50      0.67         2\n",
            "        0122022050       0.67      0.82      0.74        39\n",
            "        0122023000       0.00      0.00      0.00         2\n",
            "        0122510000       0.43      0.60      0.50        10\n",
            "        0122515000       0.00      0.00      0.00         7\n",
            "        0122515100       0.50      0.33      0.40         3\n",
            "        0122520000       0.39      0.81      0.53        32\n",
            "        0122520500       0.00      0.00      0.00         3\n",
            "        0122522000       0.40      1.00      0.57         2\n",
            "        0124010000       0.50      0.57      0.53         7\n",
            "        0124010500       0.00      0.00      0.00         5\n",
            "        0124015200       0.88      0.78      0.82         9\n",
            "        0124015400       0.75      0.90      0.82        10\n",
            "        0124015500       0.00      0.00      0.00         1\n",
            "        0124015600       0.22      0.56      0.31         9\n",
            "        0124015700       0.00      0.00      0.00         1\n",
            "        0124020000       0.20      0.80      0.32        46\n",
            "        0124021000       0.15      0.82      0.25        38\n",
            "        0124510000       0.43      0.33      0.38         9\n",
            "        0124515000       0.00      0.00      0.00         1\n",
            "        0199999902       0.00      0.00      0.00         3\n",
            "        0199999903       1.00      1.00      1.00         4\n",
            "        0199999952       1.00      1.00      1.00         7\n",
            "0303.WE1210.130.09       1.00      1.00      1.00         5\n",
            "0303.WE1220.152.01       0.75      0.60      0.67         5\n",
            "0303.Y68910.050.01       0.00      0.00      0.00         3\n",
            "0303.YM1120.300.01       0.00      0.00      0.00         5\n",
            "0303.YM1120.300.17       0.00      0.00      0.00         1\n",
            "0303.YS1101.100.04       1.00      1.00      1.00         3\n",
            "0303.YS1101.100.09       0.00      0.00      0.00         2\n",
            "        80034611.0       0.00      0.00      0.00         3\n",
            "        80051961.0       0.00      0.00      0.00         2\n",
            "        80052091.0       1.00      0.25      0.40         4\n",
            "        80052734.0       0.00      0.00      0.00         3\n",
            "        80053035.0       0.17      0.10      0.12        10\n",
            "        80053039.0       0.00      0.00      0.00         1\n",
            "        80053412.0       0.00      0.00      0.00         1\n",
            "        80053637.0       0.00      0.00      0.00         1\n",
            "        80054018.0       0.00      0.00      0.00         2\n",
            "        80055055.0       0.00      0.00      0.00         1\n",
            "        80055535.0       0.54      0.58      0.56        12\n",
            "        80055563.0       0.71      0.83      0.76        35\n",
            "        80055614.0       0.00      0.00      0.00         1\n",
            "        80055655.0       1.00      0.20      0.33         5\n",
            "        80055692.0       0.00      0.00      0.00         2\n",
            "        80055694.0       0.24      0.58      0.34        12\n",
            "        80055752.0       1.00      0.50      0.67         2\n",
            "        80055874.0       0.00      0.00      0.00         2\n",
            "        80055926.0       0.00      0.00      0.00         2\n",
            "        80056042.0       0.78      1.00      0.88         7\n",
            "        80056043.0       1.00      0.50      0.67         2\n",
            "        80056046.0       0.00      0.00      0.00         2\n",
            "        80056083.0       0.00      0.00      0.00         1\n",
            "        80056198.0       0.25      0.14      0.18        14\n",
            "        80056201.0       0.00      0.00      0.00         2\n",
            "        80056202.0       0.00      0.00      0.00         3\n",
            "        80056235.0       0.22      0.67      0.33         6\n",
            "        80056344.0       0.00      0.00      0.00         1\n",
            "        80056385.0       0.00      0.00      0.00         1\n",
            "        80056420.0       0.00      0.00      0.00         2\n",
            "        80056422.0       1.00      0.80      0.89         5\n",
            "        80056438.0       0.00      0.00      0.00         2\n",
            "        80056497.0       0.00      0.00      0.00         1\n",
            "        80056500.0       0.00      0.00      0.00         2\n",
            "        80056755.0       0.00      0.00      0.00         1\n",
            "        80056845.0       0.00      0.00      0.00         3\n",
            "        80056904.0       0.00      0.00      0.00         2\n",
            "        80057093.0       0.00      0.00      0.00         1\n",
            "        80057096.0       0.00      0.00      0.00         1\n",
            "        80057140.0       0.00      0.00      0.00         2\n",
            "        80057141.0       0.00      0.00      0.00         2\n",
            "        80057265.0       0.00      0.00      0.00         1\n",
            "        80057283.0       0.00      0.00      0.00         1\n",
            "        80057340.0       0.00      0.00      0.00         2\n",
            "        80057349.0       0.00      0.00      0.00         6\n",
            "        80057360.0       0.00      0.00      0.00         3\n",
            "        80057374.0       0.00      0.00      0.00         1\n",
            "        80057415.0       0.00      0.00      0.00         1\n",
            "        80057434.0       1.00      0.67      0.80         3\n",
            "        80057468.0       1.00      1.00      1.00         1\n",
            "        80057501.0       0.00      0.00      0.00         1\n",
            "        80057505.0       0.00      0.00      0.00         1\n",
            "        80057517.0       1.00      0.20      0.33         5\n",
            "        80057526.0       0.00      0.00      0.00         3\n",
            "        80057648.0       0.31      0.44      0.36         9\n",
            "        80057678.0       0.00      0.00      0.00         4\n",
            "        80057725.0       0.33      0.25      0.29         4\n",
            "        80057727.0       0.00      0.00      0.00         3\n",
            "        80057729.0       1.00      0.50      0.67         2\n",
            "        80057733.0       0.50      0.67      0.57        18\n",
            "        80057734.0       0.66      0.95      0.78        22\n",
            "        80057735.0       0.00      0.00      0.00         1\n",
            "        80057758.0       0.35      0.46      0.40        13\n",
            "        80057792.0       0.00      0.00      0.00         2\n",
            "        80057806.0       0.00      0.00      0.00         5\n",
            "        80057823.0       0.00      0.00      0.00         1\n",
            "        80057826.0       0.45      0.71      0.55        35\n",
            "        80057835.0       0.33      0.40      0.36         5\n",
            "        80057860.0       0.00      0.00      0.00         1\n",
            "        80057901.0       0.00      0.00      0.00         3\n",
            "        80057921.0       0.00      0.00      0.00         1\n",
            "        80057923.0       0.00      0.00      0.00         1\n",
            "        80057941.0       0.00      0.00      0.00         4\n",
            "        80057959.0       0.00      0.00      0.00         1\n",
            "        80057966.0       0.00      0.00      0.00         8\n",
            "        80057967.0       0.00      0.00      0.00         2\n",
            "        80057968.0       0.00      0.00      0.00         1\n",
            "        80057970.0       0.00      0.00      0.00         2\n",
            "        80057976.0       0.79      0.92      0.85        24\n",
            "        80057977.0       0.00      0.00      0.00         2\n",
            "        80057995.0       0.31      0.45      0.37        11\n",
            "        80058002.0       0.40      0.31      0.35        13\n",
            "        80058003.0       0.35      0.65      0.46        17\n",
            "        80058004.0       0.00      0.00      0.00         2\n",
            "        80058005.0       0.83      0.56      0.67         9\n",
            "        80058014.0       0.00      0.00      0.00         2\n",
            "        80058015.0       0.00      0.00      0.00         1\n",
            "        80058019.0       0.00      0.00      0.00         3\n",
            "        80058039.0       0.00      0.00      0.00         2\n",
            "        80058040.0       0.00      0.00      0.00         2\n",
            "        80058042.0       0.12      0.18      0.15        11\n",
            "        80058050.0       0.00      0.00      0.00         5\n",
            "        80058053.0       0.00      0.00      0.00         1\n",
            "        80058073.0       0.00      0.00      0.00         1\n",
            "        80058075.0       0.00      0.00      0.00         1\n",
            "        80058082.0       0.00      0.00      0.00         2\n",
            "        80058085.0       0.00      0.00      0.00         1\n",
            "        80058099.0       0.00      0.00      0.00         3\n",
            "        80058101.0       0.00      0.00      0.00         2\n",
            "        80058133.0       0.00      0.00      0.00         1\n",
            "        80058165.0       0.00      0.00      0.00         3\n",
            "        80058166.0       1.00      0.40      0.57         5\n",
            "        80058187.0       0.00      0.00      0.00         4\n",
            "        80058202.0       0.00      0.00      0.00         2\n",
            "        80058203.0       0.00      0.00      0.00         1\n",
            "        80058211.0       0.11      0.20      0.14         5\n",
            "        80058218.0       0.00      0.00      0.00         2\n",
            "        80058224.0       0.53      0.64      0.58        14\n",
            "        80058225.0       0.00      0.00      0.00         1\n",
            "        80058238.0       0.00      0.00      0.00         3\n",
            "        80058267.0       0.00      0.00      0.00         1\n",
            "        80058275.0       0.00      0.00      0.00         1\n",
            "        80058288.0       0.00      0.00      0.00         1\n",
            "        80058290.0       0.00      0.00      0.00         2\n",
            "        80058292.0       0.00      0.00      0.00         1\n",
            "        80058299.0       0.21      0.33      0.26         9\n",
            "        80058311.0       0.00      0.00      0.00         1\n",
            "        80058332.0       0.40      0.67      0.50         3\n",
            "        80058355.0       0.00      0.00      0.00         3\n",
            "        80058356.0       0.00      0.00      0.00         3\n",
            "        80058358.0       0.00      0.00      0.00         2\n",
            "        80058366.0       0.00      0.00      0.00         7\n",
            "        80058369.0       0.00      0.00      0.00         2\n",
            "        80058372.0       0.00      0.00      0.00         5\n",
            "        80058377.0       0.41      0.61      0.49        18\n",
            "        80058378.0       0.00      0.00      0.00         4\n",
            "        80058379.0       0.00      0.00      0.00         1\n",
            "        80058395.0       0.00      0.00      0.00         2\n",
            "        80058398.0       0.00      0.00      0.00         1\n",
            "        80058400.0       0.00      0.00      0.00         2\n",
            "        80058402.0       0.00      0.00      0.00         1\n",
            "        80058403.0       0.00      0.00      0.00         5\n",
            "        80058447.0       1.00      0.20      0.33         5\n",
            "        80058472.0       0.00      0.00      0.00         1\n",
            "        80058475.0       0.00      0.00      0.00         1\n",
            "        80058479.0       1.00      0.33      0.50         3\n",
            "        80058494.0       0.00      0.00      0.00         4\n",
            "        80058502.0       0.00      0.00      0.00         4\n",
            "        80058510.0       0.00      0.00      0.00         1\n",
            "        80058518.0       0.00      0.00      0.00         1\n",
            "        80058520.0       0.00      0.00      0.00         1\n",
            "        80058524.0       0.00      0.00      0.00         2\n",
            "        80058525.0       0.67      1.00      0.80         4\n",
            "        80058529.0       0.00      0.00      0.00         1\n",
            "        80058534.0       0.00      0.00      0.00         1\n",
            "        80058540.0       0.50      0.50      0.50         4\n",
            "        80058551.0       0.00      0.00      0.00         1\n",
            "        80058554.0       0.00      0.00      0.00         6\n",
            "        80058572.0       0.00      0.00      0.00         2\n",
            "        80058573.0       0.00      0.00      0.00         2\n",
            "        80058574.0       0.00      0.00      0.00         1\n",
            "        80058578.0       0.00      0.00      0.00         7\n",
            "        80058586.0       0.00      0.00      0.00         1\n",
            "        80058607.0       0.00      0.00      0.00         1\n",
            "        80058618.0       0.00      0.00      0.00         1\n",
            "        80058624.0       0.00      0.00      0.00         1\n",
            "        80058630.0       0.00      0.00      0.00         3\n",
            "        80058631.0       0.00      0.00      0.00         1\n",
            "        80058632.0       0.00      0.00      0.00         1\n",
            "        80058641.0       0.00      0.00      0.00         2\n",
            "        80058645.0       0.00      0.00      0.00         3\n",
            "        80058663.0       0.00      0.00      0.00         2\n",
            "        80058670.0       0.00      0.00      0.00         2\n",
            "        80058672.0       0.00      0.00      0.00         2\n",
            "        80058694.0       0.00      0.00      0.00         1\n",
            "        80058704.0       0.00      0.00      0.00         2\n",
            "        80058708.0       0.00      0.00      0.00         2\n",
            "        80058739.0       0.00      0.00      0.00         1\n",
            "        80058756.0       1.00      0.50      0.67         2\n",
            "        80058762.0       0.00      0.00      0.00         2\n",
            "        80058764.0       0.20      0.25      0.22         4\n",
            "        80058771.0       0.00      0.00      0.00         1\n",
            "        80058772.0       0.00      0.00      0.00         1\n",
            "        80058777.0       0.00      0.00      0.00         4\n",
            "        80058801.0       0.00      0.00      0.00         1\n",
            "        80058802.0       0.47      0.58      0.52        12\n",
            "        80058804.0       0.00      0.00      0.00         2\n",
            "        80058809.0       0.57      0.50      0.53         8\n",
            "        80058823.0       0.00      0.00      0.00         2\n",
            "        80058829.0       0.00      0.00      0.00         2\n",
            "        80058840.0       0.00      0.00      0.00         1\n",
            "        80058847.0       0.67      0.67      0.67        12\n",
            "        80058855.0       0.00      0.00      0.00         1\n",
            "        80058856.0       1.00      0.38      0.56        13\n",
            "        80058875.0       0.00      0.00      0.00         1\n",
            "        80058879.0       0.00      0.00      0.00         4\n",
            "        80058880.0       0.00      0.00      0.00         4\n",
            "        80058884.0       1.00      0.25      0.40         4\n",
            "        80058886.0       0.00      0.00      0.00         1\n",
            "        80058897.0       0.00      0.00      0.00         1\n",
            "        80058898.0       0.00      0.00      0.00         4\n",
            "        80058903.0       0.00      0.00      0.00         1\n",
            "        80058907.0       0.40      0.67      0.50         3\n",
            "        80058909.0       0.00      0.00      0.00         2\n",
            "        80058914.0       0.00      0.00      0.00         3\n",
            "        80058915.0       0.00      0.00      0.00         1\n",
            "        80058928.0       0.00      0.00      0.00         3\n",
            "        80058959.0       0.00      0.00      0.00         1\n",
            "        80058961.0       0.00      0.00      0.00         3\n",
            "        80058963.0       0.00      0.00      0.00         1\n",
            "        80058980.0       0.00      0.00      0.00         1\n",
            "        80058983.0       0.00      0.00      0.00         2\n",
            "        80058997.0       0.00      0.00      0.00         1\n",
            "        80059007.0       0.00      0.00      0.00         1\n",
            "        80059030.0       0.00      0.00      0.00         2\n",
            "        80059036.0       0.00      0.00      0.00         2\n",
            "        80059040.0       0.00      0.00      0.00         1\n",
            "        80059054.0       0.00      0.00      0.00         1\n",
            "        80059055.0       0.00      0.00      0.00         1\n",
            "        80059058.0       0.00      0.00      0.00         1\n",
            "        80059063.0       0.60      0.60      0.60         5\n",
            "        80059082.0       0.00      0.00      0.00         2\n",
            "        80059111.0       1.00      1.00      1.00         1\n",
            "        80059175.0       0.00      0.00      0.00         1\n",
            "        80059183.0       0.00      0.00      0.00         1\n",
            "        80059195.0       0.50      0.92      0.65        13\n",
            "        80059196.0       1.00      0.25      0.40         8\n",
            "        80059198.0       0.00      0.00      0.00         1\n",
            "        80059238.0       0.00      0.00      0.00         3\n",
            "        80059240.0       0.00      0.00      0.00         2\n",
            "        80059244.0       0.00      0.00      0.00         2\n",
            "        80059254.0       0.00      0.00      0.00         1\n",
            "        80059260.0       0.00      0.00      0.00         1\n",
            "        80059262.0       0.00      0.00      0.00         1\n",
            "        80059306.0       0.61      0.65      0.63        17\n",
            "        80059307.0       0.00      0.00      0.00         2\n",
            "        80059327.0       0.00      0.00      0.00         2\n",
            "        80059354.0       1.00      0.33      0.50         3\n",
            "        80059360.0       0.00      0.00      0.00         8\n",
            "        80059368.0       0.00      0.00      0.00         2\n",
            "        80059391.0       0.67      0.67      0.67         3\n",
            "        80059480.0       0.00      0.00      0.00         4\n",
            "        80059498.0       1.00      1.00      1.00         3\n",
            "        80059557.0       0.69      0.90      0.78        10\n",
            "        80059598.0       0.00      0.00      0.00         1\n",
            "        80059612.0       0.00      0.00      0.00         2\n",
            "        80059621.0       0.00      0.00      0.00         2\n",
            "        80059673.0       0.00      0.00      0.00         1\n",
            "        80059691.0       0.00      0.00      0.00         2\n",
            "        80059696.0       0.00      0.00      0.00         1\n",
            "        80059736.0       0.00      0.00      0.00         3\n",
            "        80059757.0       0.00      0.00      0.00         1\n",
            "        80059770.0       0.00      0.00      0.00         1\n",
            "        80059788.0       0.00      0.00      0.00         2\n",
            "        80059815.0       1.00      0.33      0.50         3\n",
            "        80059835.0       0.80      0.67      0.73         6\n",
            "        80059838.0       0.00      0.00      0.00         1\n",
            "        80059852.0       0.00      0.00      0.00         1\n",
            "        80059880.0       0.00      0.00      0.00         1\n",
            "        80059909.0       0.00      0.00      0.00         2\n",
            "        80059945.0       0.00      0.00      0.00         4\n",
            "        80059950.0       0.00      0.00      0.00         1\n",
            "        80059966.0       0.00      0.00      0.00         2\n",
            "        80059973.0       0.00      0.00      0.00         2\n",
            "        80059975.0       0.00      0.00      0.00         2\n",
            "        80059993.0       0.00      0.00      0.00         1\n",
            "        80060002.0       0.00      0.00      0.00         1\n",
            "        80060034.0       0.00      0.00      0.00         2\n",
            "        80060037.0       0.00      0.00      0.00         1\n",
            "        80060075.0       0.00      0.00      0.00         2\n",
            "        80060076.0       0.00      0.00      0.00         1\n",
            "        80060103.0       0.00      0.00      0.00         1\n",
            "        80060142.0       0.00      0.00      0.00         2\n",
            "        80060187.0       0.00      0.00      0.00         1\n",
            "        80060192.0       0.00      0.00      0.00         1\n",
            "        80060230.0       0.50      0.60      0.55         5\n",
            "        80060236.0       0.00      0.00      0.00         2\n",
            "        80060260.0       0.00      0.00      0.00         2\n",
            "        80060267.0       1.00      0.22      0.36         9\n",
            "        80060333.0       0.00      0.00      0.00         2\n",
            "        80060426.0       0.00      0.00      0.00         1\n",
            "        80060439.0       0.00      0.00      0.00         1\n",
            "        80060479.0       0.00      0.00      0.00         2\n",
            "        80060480.0       0.00      0.00      0.00         1\n",
            "        80060513.0       0.00      0.00      0.00         1\n",
            "        80060524.0       0.00      0.00      0.00         1\n",
            "        80060576.0       0.00      0.00      0.00         3\n",
            "        80060647.0       1.00      1.00      1.00         2\n",
            "        80060704.0       0.00      0.00      0.00         2\n",
            "        80060890.0       0.00      0.00      0.00         1\n",
            "        80060905.0       0.00      0.00      0.00         1\n",
            "        80061048.0       0.00      0.00      0.00         1\n",
            "        80801909.0       0.00      0.00      0.00         5\n",
            "        80801934.0       0.00      0.00      0.00         2\n",
            "        80801969.0       0.00      0.00      0.00         7\n",
            "        80801975.0       0.41      0.88      0.56         8\n",
            "        80801982.0       1.00      0.50      0.67         2\n",
            "        80802001.0       0.00      0.00      0.00         1\n",
            "        80802006.0       0.28      0.75      0.41        16\n",
            "        80802013.0       0.00      0.00      0.00         6\n",
            "        80802037.0       1.00      1.00      1.00         1\n",
            "        80802080.0       0.38      1.00      0.55         3\n",
            "        80802089.0       1.00      0.30      0.46        10\n",
            "        80802101.0       0.00      0.00      0.00         1\n",
            "        80802114.0       0.00      0.00      0.00         3\n",
            "        80802120.0       0.00      0.00      0.00         2\n",
            "        80802146.0       1.00      0.86      0.92         7\n",
            "\n",
            "          accuracy                           0.43      1463\n",
            "         macro avg       0.20      0.18      0.17      1463\n",
            "      weighted avg       0.38      0.43      0.37      1463\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0330\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1605 samples from 107 relevant classes. (N=1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 1501 samples from 61 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.724\n",
            "Test Accuracy: 0.641\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   0330.Y12100.004.02       0.00      0.00      0.00         1\n",
            "   0330.Y12100.004.03       1.00      1.00      1.00         4\n",
            "   0330.Y12100.004.05       0.00      0.00      0.00         1\n",
            "   0330.Y12100.004.08       0.00      0.00      0.00         3\n",
            "   0330.Y12100.004.12       1.00      1.00      1.00         5\n",
            "   0330.Y12100.004.16       0.00      0.00      0.00         2\n",
            "   0330.Y12100.004.19       0.00      0.00      0.00         1\n",
            "   0330.Y12100.004.20       0.00      0.00      0.00         2\n",
            "      0330.Y12100.005       0.86      0.50      0.63        12\n",
            "   0330.Y12100.008.01       0.00      0.00      0.00         1\n",
            "   0330.Y12210.009.01       0.00      0.00      0.00         1\n",
            "   0330.Y12210.009.02       1.00      0.50      0.67         2\n",
            "      0330.Y12210.111       1.00      0.33      0.50         3\n",
            "0330.Y12807.001.02.02       1.00      1.00      1.00         4\n",
            "           1033010000       1.00      0.50      0.67         6\n",
            "           1033010094       1.00      1.00      1.00         2\n",
            "           1033010095       0.00      0.00      0.00         1\n",
            "           1033010100       0.00      0.00      0.00         1\n",
            "           1033010200       0.79      0.73      0.76        15\n",
            "           1033010201       0.00      0.00      0.00         1\n",
            "           1033010500       0.67      0.25      0.36         8\n",
            "           1033010600       0.00      0.00      0.00         1\n",
            "           1033010690       0.00      0.00      0.00         1\n",
            "           1033010693       1.00      0.40      0.57        10\n",
            "           1033010694       0.36      0.98      0.52        44\n",
            "           1033010695       0.83      0.45      0.59        11\n",
            "           1033016000       1.00      1.00      1.00         2\n",
            "           1033016300       1.00      1.00      1.00         4\n",
            "           1033016700       0.00      0.00      0.00         3\n",
            "           1033017000       0.00      0.00      0.00         2\n",
            "           1033017100       0.33      0.67      0.44         3\n",
            "           1033017110       1.00      0.50      0.67         2\n",
            "           1033017120       0.00      0.00      0.00         1\n",
            "           1033017200       0.00      0.00      0.00         4\n",
            "           1033017210       0.00      0.00      0.00         1\n",
            "           1033017300       0.00      0.00      0.00         2\n",
            "           1033017400       0.00      0.00      0.00         3\n",
            "           1033017500       0.00      0.00      0.00         1\n",
            "           1033017510       0.00      0.00      0.00         4\n",
            "           1033017520       1.00      0.88      0.93         8\n",
            "           1033020100       0.00      0.00      0.00         3\n",
            "           1033020830       0.00      0.00      0.00         1\n",
            "           1033022400       0.33      0.50      0.40         2\n",
            "           1033030091       1.00      1.00      1.00        14\n",
            "           1033030200       1.00      1.00      1.00         4\n",
            "           1033030201       0.92      0.92      0.92        12\n",
            "           1033030202       0.76      1.00      0.86        25\n",
            "           1033030210       1.00      0.67      0.80         3\n",
            "           1033030220       0.00      0.00      0.00         3\n",
            "           1033030230       1.00      0.57      0.73         7\n",
            "           1033030240       1.00      0.40      0.57         5\n",
            "           1033030300       0.00      0.00      0.00         3\n",
            "           1033030400       1.00      0.33      0.50         3\n",
            "           1033030401       0.00      0.00      0.00         1\n",
            "           1033030410       0.00      0.00      0.00         2\n",
            "           1033030830       0.00      0.00      0.00         1\n",
            "           1033038100       1.00      0.50      0.67         4\n",
            "           1033038200       0.50      0.88      0.64         8\n",
            "           1033038300       1.00      0.62      0.77         8\n",
            "           1033038400       0.00      0.00      0.00         1\n",
            "           1033050200       1.00      1.00      1.00         8\n",
            "\n",
            "             accuracy                           0.64       301\n",
            "            macro avg       0.45      0.36      0.38       301\n",
            "         weighted avg       0.65      0.64      0.60       301\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0362\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 563 samples from 36 relevant classes. (N=1)\n",
            "Reduced to 523 samples from 17 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.876\n",
            "Test Accuracy: 0.800\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0015030020       1.00      0.33      0.50         3\n",
            "        0015030030       0.00      0.00      0.00         3\n",
            "        0015030050       0.64      0.90      0.75        30\n",
            "        0015030065       0.50      0.25      0.33         4\n",
            "        0015030070       0.00      0.00      0.00         1\n",
            "        0015076001       0.67      1.00      0.80         4\n",
            "        0015097101       0.00      0.00      0.00         2\n",
            "        0015098103       0.91      1.00      0.95        10\n",
            "0362.G62000.088.55       0.67      1.00      0.80         4\n",
            "0362.G62000.092.58       0.00      0.00      0.00         2\n",
            "0362.G62000.093.58       1.00      1.00      1.00         1\n",
            "0362.G62000.095.01       0.00      0.00      0.00         1\n",
            "0362.G62000.096.58       1.00      1.00      1.00        28\n",
            "0362.G62000.999.30       1.00      0.80      0.89         5\n",
            "0362.G63000.082.55       1.00      1.00      1.00         3\n",
            "0362.T15109.069.02       0.00      0.00      0.00         2\n",
            "0362.T15109.070.02       1.00      0.50      0.67         2\n",
            "\n",
            "          accuracy                           0.80       105\n",
            "         macro avg       0.55      0.52      0.51       105\n",
            "      weighted avg       0.74      0.80      0.75       105\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0377\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 173 samples from 23 relevant classes. (N=1)\n",
            "Reduced to 153 samples from 10 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.943\n",
            "Test Accuracy: 0.774\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0077025030       1.00      1.00      1.00         1\n",
            "           0077030010       1.00      0.67      0.80         3\n",
            "           0077080122       0.57      0.67      0.62         6\n",
            "           0077100231       0.00      0.00      0.00         2\n",
            "   0377.G77000.072.65       0.80      0.80      0.80         5\n",
            "   0377.G77000.073.59       0.75      1.00      0.86         3\n",
            "   0377.G77000.096.56       0.67      0.67      0.67         3\n",
            "   0377.G77000.999.55       0.83      1.00      0.91         5\n",
            "0377.O77952.005.14.02       1.00      1.00      1.00         1\n",
            "      0377.W77305.005       1.00      1.00      1.00         2\n",
            "\n",
            "             accuracy                           0.77        31\n",
            "            macro avg       0.76      0.78      0.76        31\n",
            "         weighted avg       0.74      0.77      0.75        31\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0601\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 2245 samples from 276 relevant classes. (N=1)\n",
            "Reduced to 1913 samples from 62 relevant classes. (N=5)\n",
            "Training Accuracy: 0.810\n",
            "Test Accuracy: 0.692\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "        0090022015       1.00      1.00      1.00         6\n",
            "        0090024002       1.00      1.00      1.00         1\n",
            "        0090024009       1.00      1.00      1.00         2\n",
            "        0090026700       0.00      0.00      0.00         2\n",
            "        0090026810       0.00      0.00      0.00         3\n",
            "        0090030010       0.00      0.00      0.00         1\n",
            "        0090030020       0.75      0.67      0.71         9\n",
            "        0090032010       0.88      1.00      0.93        21\n",
            "        0090032042       1.00      0.77      0.87        13\n",
            "        0090034032       1.00      0.64      0.78        11\n",
            "        0090034035       0.64      0.82      0.72        17\n",
            "        0090034038       0.00      0.00      0.00         2\n",
            "        0090034041       0.00      0.00      0.00         3\n",
            "        0090036150       0.69      0.92      0.79        12\n",
            "        0090036190       1.00      1.00      1.00         1\n",
            "        0090043002       0.94      1.00      0.97        15\n",
            "        0090082001       1.00      1.00      1.00         5\n",
            "        0090095270       0.61      0.95      0.74        21\n",
            "        0090095275       0.80      1.00      0.89         8\n",
            "        0090096101       0.50      0.50      0.50         2\n",
            "        0090096307       0.00      0.00      0.00         1\n",
            "        0090096312       0.00      0.00      0.00         1\n",
            "        0090096702       1.00      0.71      0.83         7\n",
            "        0090098310       1.00      0.50      0.67         2\n",
            "        0090098341       0.67      1.00      0.80         2\n",
            "        0090098376       0.00      0.00      0.00         1\n",
            "        0090098377       0.33      0.50      0.40         2\n",
            "        0090098378       0.00      0.00      0.00         1\n",
            "        0090098379       0.00      0.00      0.00         1\n",
            "        0090098385       1.00      1.00      1.00         5\n",
            "        0090098397       1.00      0.80      0.89         5\n",
            "        0090098398       0.91      1.00      0.95        10\n",
            "        0090098399       1.00      1.00      1.00         5\n",
            "        0090098411       0.00      0.00      0.00         9\n",
            "        0090098414       0.82      0.88      0.85        16\n",
            "        0090098417       1.00      0.58      0.74        12\n",
            "        0090098424       0.67      0.93      0.78        15\n",
            "        0090098429       1.00      0.89      0.94        18\n",
            "        0090098434       0.30      0.21      0.25        14\n",
            "        0090098437       0.35      0.88      0.50        17\n",
            "        0090098480       0.00      0.00      0.00         3\n",
            "        0090098483       0.00      0.00      0.00         4\n",
            "        0090098487       0.00      0.00      0.00         6\n",
            "        0090098489       0.00      0.00      0.00         3\n",
            "0601.B95800.001.14       0.75      1.00      0.86         3\n",
            "0601.R15H11.001.90       0.00      0.00      0.00         1\n",
            "0601.R29G14.001.90       0.00      0.00      0.00         1\n",
            "0601.R29G24.001.90       0.00      0.00      0.00         1\n",
            "0601.R29H11.001.90       0.20      0.75      0.32         4\n",
            "0601.R29H14.001.90       0.38      0.75      0.50         8\n",
            "0601.R29H17.001.90       0.00      0.00      0.00         1\n",
            "0601.R29H24.001.90       0.50      0.25      0.33         4\n",
            "0601.R29H34.001.90       0.40      0.25      0.31         8\n",
            "0601.R29H37.001.90       0.00      0.00      0.00         2\n",
            "0601.R29I11.001.90       0.00      0.00      0.00         3\n",
            "0601.R29I14.001.90       0.00      0.00      0.00         2\n",
            "0601.R29I24.001.90       0.00      0.00      0.00         2\n",
            "0601.R29I34.001.90       0.67      0.33      0.44         6\n",
            "0601.Y93006.001.90       1.00      1.00      1.00         2\n",
            "0601.Y93080.002.01       1.00      1.00      1.00         5\n",
            "0601.Y96G06.001.90       0.69      1.00      0.82         9\n",
            "0601.Y96I06.001.90       1.00      0.33      0.50         6\n",
            "\n",
            "          accuracy                           0.69       383\n",
            "         macro avg       0.49      0.50      0.48       383\n",
            "      weighted avg       0.65      0.69      0.65       383\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0801\n",
            "\n",
            "Cost Element classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 4789 samples from 568 relevant classes. (N=1)\n",
            "Reduced to 4015 samples from 163 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.749\n",
            "Test Accuracy: 0.691\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0000010240       0.00      0.00      0.00         1\n",
            "           0000011440       0.00      0.00      0.00         1\n",
            "           0000012140       0.00      0.00      0.00         1\n",
            "           0000018010       1.00      0.67      0.80         3\n",
            "           0000040340       0.00      0.00      0.00         2\n",
            "           0000040440       0.00      0.00      0.00         1\n",
            "           0000040540       0.00      0.00      0.00         1\n",
            "           0000041340       0.00      0.00      0.00         1\n",
            "           0000041640       0.00      0.00      0.00         1\n",
            "           0000042040       0.00      0.00      0.00         3\n",
            "           0000042240       0.00      0.00      0.00         1\n",
            "           0000045841       0.00      0.00      0.00         1\n",
            "           0000047010       0.00      0.00      0.00         4\n",
            "           0000048010       0.00      0.00      0.00         1\n",
            "           0000053240       0.00      0.00      0.00         2\n",
            "           0000053940       0.00      0.00      0.00         1\n",
            "           0000081840       0.00      0.00      0.00         1\n",
            "           0000088052       1.00      0.67      0.80         3\n",
            "           0000088111       1.00      0.33      0.50         3\n",
            "           0000089010       0.00      0.00      0.00         3\n",
            "           0000091000       0.22      0.97      0.35        32\n",
            "           0000091010       0.00      0.00      0.00         3\n",
            "           0000091510       0.00      0.00      0.00         1\n",
            "           0000091511       1.00      0.50      0.67         8\n",
            "           0000091519       0.00      0.00      0.00         2\n",
            "           0000091541       0.38      0.56      0.45         9\n",
            "           0000091549       0.00      0.00      0.00         4\n",
            "           0000091550       0.00      0.00      0.00         1\n",
            "           0000091551       0.29      0.78      0.42         9\n",
            "           0000091559       0.17      0.25      0.20         4\n",
            "           0000091570       1.00      0.33      0.50         3\n",
            "           0000091571       0.67      0.50      0.57         8\n",
            "           0000091579       0.00      0.00      0.00         3\n",
            "           0000091580       0.00      0.00      0.00         4\n",
            "           0000091581       0.37      0.78      0.50         9\n",
            "           0000091589       0.00      0.00      0.00         2\n",
            "           0000091700       0.00      0.00      0.00         1\n",
            "           0000092695       1.00      0.50      0.67         4\n",
            "           0000092696       0.70      0.78      0.74         9\n",
            "           0000094000       0.67      0.60      0.63        10\n",
            "           0000094910       1.00      0.80      0.89         5\n",
            "           0000096200       0.00      0.00      0.00         1\n",
            "           0000096400       0.00      0.00      0.00         1\n",
            "           0000099206       1.00      1.00      1.00         1\n",
            "           0000099301       1.00      1.00      1.00         6\n",
            "           0000099970       1.00      1.00      1.00        10\n",
            "      0801.A41051.990       0.00      0.00      0.00         2\n",
            "      0801.C41001.100       0.00      0.00      0.00         4\n",
            "      0801.C41001.110       0.00      0.00      0.00         3\n",
            "      0801.C41001.120       0.00      0.00      0.00         3\n",
            "   0801.C41001.130.01       0.00      0.00      0.00         2\n",
            "      0801.C43015.150       0.00      0.00      0.00         1\n",
            "      0801.C91050.400       1.00      1.00      1.00         1\n",
            "      0801.C94102.200       1.00      1.00      1.00         4\n",
            "      0801.C94102.220       0.00      0.00      0.00         1\n",
            "      0801.C94102.230       1.00      1.00      1.00         4\n",
            "      0801.C94102.240       0.00      0.00      0.00         1\n",
            "      0801.C95001.119       0.67      0.80      0.73        15\n",
            "      0801.C95001.120       0.00      0.00      0.00         2\n",
            "      0801.C95001.402       1.00      0.70      0.82        10\n",
            "      0801.C95001.417       1.00      1.00      1.00         4\n",
            "      0801.C95001.420       0.44      0.88      0.58         8\n",
            "      0801.C95001.421       0.00      0.00      0.00         2\n",
            "      0801.C95001.423       0.67      0.67      0.67         3\n",
            "      0801.C95001.424       0.00      0.00      0.00         3\n",
            "      0801.C95001.431       1.00      1.00      1.00         4\n",
            "      0801.C95001.510       1.00      1.00      1.00         2\n",
            "      0801.C95001.511       0.00      0.00      0.00         2\n",
            "      0801.C95001.512       0.50      1.00      0.67         3\n",
            "      0801.C95001.513       0.00      0.00      0.00         4\n",
            "      0801.C95001.514       0.00      0.00      0.00         2\n",
            "      0801.C95001.515       0.00      0.00      0.00         2\n",
            "      0801.C95001.530       0.67      0.67      0.67         3\n",
            "      0801.C95001.539       1.00      0.50      0.67         4\n",
            "      0801.C95001.590       0.00      0.00      0.00         3\n",
            "      0801.C95001.591       0.71      1.00      0.83         5\n",
            "      0801.C95001.592       0.75      1.00      0.86         6\n",
            "      0801.C95001.593       1.00      1.00      1.00         4\n",
            "      0801.C95001.701       0.00      0.00      0.00         1\n",
            "      0801.C95001.702       1.00      1.00      1.00         5\n",
            "      0801.C95001.704       1.00      1.00      1.00         4\n",
            "      0801.C95001.801       0.78      1.00      0.88         7\n",
            "      0801.C95001.804       1.00      0.25      0.40         4\n",
            "      0801.C95001.806       0.00      0.00      0.00         4\n",
            "      0801.C95001.808       0.00      0.00      0.00         1\n",
            "      0801.C95001.819       0.00      0.00      0.00         2\n",
            "      0801.C95001.820       0.00      0.00      0.00         1\n",
            "      0801.C95001.822       0.00      0.00      0.00         1\n",
            "      0801.C95001.828       0.00      0.00      0.00         4\n",
            "   0801.C95001.913.01       1.00      1.00      1.00         5\n",
            "      0801.C95001.914       1.00      0.80      0.89         5\n",
            "   0801.C95001.923.01       1.00      1.00      1.00         7\n",
            "      0801.C95001.931       1.00      1.00      1.00         8\n",
            "   0801.C95001.933.08       1.00      1.00      1.00         9\n",
            "      0801.C95001.941       1.00      1.00      1.00         6\n",
            "      0801.C95001.945       0.89      1.00      0.94       304\n",
            "      0801.C95001.990       1.00      0.50      0.67         6\n",
            "0801.C96402.042.01.02       1.00      0.83      0.91         6\n",
            "      0801.S10101.119       0.00      0.00      0.00         1\n",
            "      0801.S10102.119       0.00      0.00      0.00         1\n",
            "      0801.S10103.121       1.00      0.67      0.80         3\n",
            "      0801.S10203.110       0.00      0.00      0.00         1\n",
            "      0801.S10203.114       1.00      0.50      0.67         2\n",
            "      0801.S10204.114       0.00      0.00      0.00         2\n",
            "      0801.S10302.102       0.00      0.00      0.00         1\n",
            "      0801.S10302.103       0.00      0.00      0.00         1\n",
            "      0801.S10303.101       0.00      0.00      0.00         3\n",
            "      0801.S10303.102       0.00      0.00      0.00         1\n",
            "      0801.S10303.103       0.00      0.00      0.00         1\n",
            "      0801.S10303.104       1.00      0.33      0.50         3\n",
            "      0801.S40101.407       0.00      0.00      0.00         1\n",
            "      0801.S40101.412       0.00      0.00      0.00         1\n",
            "      0801.S40101.413       0.00      0.00      0.00         1\n",
            "      0801.S40102.415       0.00      0.00      0.00         1\n",
            "      0801.S40102.416       0.00      0.00      0.00         1\n",
            "      0801.S40102.420       1.00      1.00      1.00         3\n",
            "      0801.S40103.402       0.00      0.00      0.00         3\n",
            "      0801.S40103.403       0.00      0.00      0.00         3\n",
            "      0801.S40103.405       0.12      0.20      0.15         5\n",
            "      0801.S40103.406       0.00      0.00      0.00         2\n",
            "      0801.S40103.407       0.00      0.00      0.00         2\n",
            "      0801.S40103.413       0.00      0.00      0.00         1\n",
            "      0801.S40103.416       0.00      0.00      0.00         2\n",
            "      0801.S40103.418       0.00      0.00      0.00         1\n",
            "      0801.S40103.420       0.00      0.00      0.00         2\n",
            "      0801.S40103.458       0.17      0.50      0.25         2\n",
            "   0801.S40107.403.02       0.00      0.00      0.00         2\n",
            "   0801.S40107.413.01       0.00      0.00      0.00         1\n",
            "   0801.S40107.413.03       0.00      0.00      0.00         2\n",
            "      0801.S43236.990       0.00      0.00      0.00         1\n",
            "      0801.S50103.538       0.00      0.00      0.00         1\n",
            "      0801.S50103.539       0.00      0.00      0.00         1\n",
            "      0801.S50203.501       0.00      0.00      0.00         1\n",
            "      0801.S50203.502       0.00      0.00      0.00         1\n",
            "      0801.S50203.506       0.00      0.00      0.00         1\n",
            "      0801.S50203.509       0.00      0.00      0.00         1\n",
            "      0801.S50203.517       0.00      0.00      0.00         2\n",
            "      0801.S50203.518       0.00      0.00      0.00         2\n",
            "      0801.S50203.529       0.00      0.00      0.00         1\n",
            "      0801.S50203.530       1.00      0.33      0.50         3\n",
            "      0801.S50203.531       0.00      0.00      0.00         1\n",
            "   0801.S50206.506.02       0.00      0.00      0.00         1\n",
            "      0801.S50301.532       0.00      0.00      0.00         1\n",
            "      0801.S50302.532       0.00      0.00      0.00         3\n",
            "      0801.S50303.532       0.50      0.25      0.33         4\n",
            "      0801.S50304.532       0.00      0.00      0.00         2\n",
            "      0801.S70101.700       1.00      1.00      1.00         4\n",
            "      0801.S70101.702       0.00      0.00      0.00         1\n",
            "      0801.S70102.700       0.00      0.00      0.00         1\n",
            "      0801.S70102.702       0.00      0.00      0.00         1\n",
            "      0801.S70103.702       0.00      0.00      0.00         1\n",
            "      0801.S70104.702       0.00      0.00      0.00         2\n",
            "      0801.S80103.800       0.00      0.00      0.00         1\n",
            "      0801.S80103.801       0.00      0.00      0.00         1\n",
            "      0801.S80103.806       0.00      0.00      0.00         2\n",
            "      0801.S80104.800       0.00      0.00      0.00         1\n",
            "      0801.S81096.900       0.00      0.00      0.00         4\n",
            "      0801.S94103.990       0.00      0.00      0.00         1\n",
            "      0801.W91002.990       0.00      0.00      0.00         1\n",
            "      0801.Z52027.990       0.00      0.00      0.00         1\n",
            "           2080116600       0.00      0.00      0.00         1\n",
            "           2080122300       0.00      0.00      0.00         3\n",
            "           2080181000       1.00      1.00      1.00         2\n",
            "\n",
            "             accuracy                           0.69       803\n",
            "            macro avg       0.29      0.27      0.26       803\n",
            "         weighted avg       0.64      0.69      0.64       803\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0806\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 229 samples from 20 relevant classes. (N=1)\n",
            "Reduced to 204 samples from 8 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.755\n",
            "Test Accuracy: 0.756\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000068000       0.72      1.00      0.84        26\n",
            "     0000068100       1.00      1.00      1.00         2\n",
            "     0000069000       1.00      0.60      0.75         5\n",
            "0806.A68009.950       0.00      0.00      0.00         1\n",
            "0806.S60000.010       0.00      0.00      0.00         2\n",
            "0806.S60000.500       0.00      0.00      0.00         1\n",
            "0806.S60000.800       0.00      0.00      0.00         3\n",
            "0806.S61016.990       0.00      0.00      0.00         1\n",
            "\n",
            "       accuracy                           0.76        41\n",
            "      macro avg       0.34      0.33      0.32        41\n",
            "   weighted avg       0.63      0.76      0.67        41\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0811\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 53 samples from 10 relevant classes. (N=1)\n",
            "Reduced to 36 samples from 3 relevant classes. (N=5)\n",
            "Training Accuracy: 0.929\n",
            "Test Accuracy: 0.875\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000010111       1.00      0.67      0.80         3\n",
            "     0000010511       0.75      1.00      0.86         3\n",
            "0811.C95002.105       1.00      1.00      1.00         2\n",
            "\n",
            "       accuracy                           0.88         8\n",
            "      macro avg       0.92      0.89      0.89         8\n",
            "   weighted avg       0.91      0.88      0.87         8\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 0821\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1069 samples from 105 relevant classes. (N=1)\n",
            "Reduced to 972 samples from 50 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.873\n",
            "Test Accuracy: 0.779\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000010721       0.00      0.00      0.00         2\n",
            "     0000010921       1.00      0.67      0.80         3\n",
            "     0000011421       0.00      0.00      0.00         1\n",
            "     0000050221       0.00      0.00      0.00         1\n",
            "     0000050321       0.00      0.00      0.00         2\n",
            "     0000050421       0.00      0.00      0.00         1\n",
            "     0000050721       1.00      1.00      1.00         1\n",
            "     0000050961       0.00      0.00      0.00         1\n",
            "     0000052221       0.00      0.00      0.00         2\n",
            "     0000052421       0.00      0.00      0.00         2\n",
            "     0000054221       0.00      0.00      0.00         2\n",
            "     0000058061       0.00      0.00      0.00         1\n",
            "     0000098000       0.73      1.00      0.85        11\n",
            "     0000098010       1.00      0.88      0.93         8\n",
            "     0000098300       1.00      1.00      1.00         3\n",
            "     0000098900       0.54      0.70      0.61        27\n",
            "     0000099083       0.64      0.69      0.67        13\n",
            "     0000099121       1.00      1.00      1.00         5\n",
            "0821.C95001.110       1.00      1.00      1.00         3\n",
            "0821.C95001.114       1.00      1.00      1.00         2\n",
            "0821.C95001.907       1.00      1.00      1.00         1\n",
            "0821.C95001.944       1.00      1.00      1.00         8\n",
            "0821.C95001.945       1.00      1.00      1.00         4\n",
            "0821.C95002.107       0.50      0.25      0.33         4\n",
            "0821.C95002.108       0.00      0.00      0.00         1\n",
            "0821.C95002.109       0.40      1.00      0.57         4\n",
            "0821.C95002.112       1.00      1.00      1.00         4\n",
            "0821.C95002.113       1.00      1.00      1.00         4\n",
            "0821.C95002.501       1.00      1.00      1.00         3\n",
            "0821.C95002.502       1.00      0.75      0.86         4\n",
            "0821.C95002.503       1.00      1.00      1.00         4\n",
            "0821.C95002.504       1.00      1.00      1.00         4\n",
            "0821.C95002.505       1.00      1.00      1.00         6\n",
            "0821.C95002.506       0.67      0.67      0.67         3\n",
            "0821.C95002.507       1.00      1.00      1.00         2\n",
            "0821.C95002.518       0.80      1.00      0.89         4\n",
            "0821.C95002.519       1.00      1.00      1.00         3\n",
            "0821.C95002.520       1.00      0.67      0.80         3\n",
            "0821.C95002.521       0.50      0.75      0.60         4\n",
            "0821.C95002.522       1.00      1.00      1.00         3\n",
            "0821.C95002.523       1.00      0.67      0.80         3\n",
            "0821.C95002.524       1.00      1.00      1.00         4\n",
            "0821.C95002.525       0.33      0.33      0.33         3\n",
            "0821.C95002.527       1.00      1.00      1.00         4\n",
            "0821.C95002.528       0.67      1.00      0.80         4\n",
            "0821.C95002.529       1.00      1.00      1.00         4\n",
            "0821.C95002.530       0.67      0.50      0.57         4\n",
            "0821.C98001.990       1.00      0.50      0.67         2\n",
            "0821.W98000.700       0.67      1.00      0.80         2\n",
            "0821.W98000.800       1.00      1.00      1.00         1\n",
            "\n",
            "       accuracy                           0.78       195\n",
            "      macro avg       0.68      0.68      0.67       195\n",
            "   weighted avg       0.75      0.78      0.75       195\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 2101\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 1321 samples from 137 relevant classes. (N=1)\n",
            "Reduced to 1158 samples from 42 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.740\n",
            "Test Accuracy: 0.625\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "           0000001000       1.00      0.80      0.89         5\n",
            "           0000002000       0.33      0.12      0.18         8\n",
            "           0000002004       0.55      0.75      0.63        16\n",
            "           0000002200       1.00      1.00      1.00         2\n",
            "           0000002401       0.67      0.50      0.57         8\n",
            "           0000002404       0.00      0.00      0.00         1\n",
            "           0000002502       0.75      0.60      0.67         5\n",
            "           0000003000       0.83      0.83      0.83        12\n",
            "           0000005100       0.50      0.25      0.33         4\n",
            "           0000005101       0.00      0.00      0.00         3\n",
            "           0000005206       1.00      0.25      0.40         4\n",
            "           0000006102       1.00      0.29      0.44         7\n",
            "           0000006104       0.00      0.00      0.00         4\n",
            "           0000006106       0.75      0.38      0.50         8\n",
            "           0000006200       0.00      0.00      0.00         3\n",
            "           0000006202       0.42      0.81      0.56        21\n",
            "           0000006203       0.21      0.35      0.27        17\n",
            "           0000006204       0.25      0.25      0.25         4\n",
            "           0000006205       0.00      0.00      0.00         2\n",
            "           0000006302       0.38      0.45      0.42        11\n",
            "           0000006401       0.00      0.00      0.00         1\n",
            "           0000009101       1.00      0.90      0.95        10\n",
            "           0000009103       1.00      0.85      0.92        13\n",
            "           0000009110       0.88      1.00      0.93         7\n",
            "           0000009303       0.86      1.00      0.92         6\n",
            "           0000009401       1.00      1.00      1.00         2\n",
            "2101.C20200.001.62.03       0.67      0.67      0.67         3\n",
            "2101.C64100.011.20.00       1.00      1.00      1.00         2\n",
            "2101.C78200.800.52.06       0.67      1.00      0.80         2\n",
            "2101.C78200.930.52.06       0.80      1.00      0.89         4\n",
            "2101.C78300.800.52.06       0.00      0.00      0.00         2\n",
            "2101.C78300.930.52.06       0.70      1.00      0.82         7\n",
            "2101.C83232.001.61.02       0.00      0.00      0.00         1\n",
            "2101.C83239.001.61.02       0.75      1.00      0.86         3\n",
            "2101.C86008.191.61.06       0.00      0.00      0.00         1\n",
            "2101.C86008.360.61.06       1.00      1.00      1.00         3\n",
            "2101.C86008.396.61.06       0.80      1.00      0.89        12\n",
            "2101.C86008.791.61.06       0.00      0.00      0.00         1\n",
            "2101.C88005.010.51.01       0.00      0.00      0.00         1\n",
            "2101.C92000.001.62.03       0.00      0.00      0.00         2\n",
            "2101.C92192.002.62.03       1.00      0.50      0.67         2\n",
            "2101.Z80004.001.20.00       1.00      1.00      1.00         2\n",
            "\n",
            "             accuracy                           0.62       232\n",
            "            macro avg       0.54      0.51      0.51       232\n",
            "         weighted avg       0.62      0.62      0.59       232\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 2111\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 624 samples from 57 relevant classes. (N=1)\n",
            "Reduced to 551 samples from 19 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.770\n",
            "Test Accuracy: 0.712\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "     0000007100       0.65      0.98      0.79        54\n",
            "     0000007111       0.25      1.00      0.40         1\n",
            "     0000007112       0.00      0.00      0.00         1\n",
            "     0000007114       0.00      0.00      0.00         2\n",
            "     0000007116       0.00      0.00      0.00         1\n",
            "     0000007156       0.00      0.00      0.00         2\n",
            "     0000007160       0.00      0.00      0.00         2\n",
            "     0000007175       1.00      0.88      0.93        16\n",
            "     0000007800       0.00      0.00      0.00         3\n",
            "2111.C02006.001       1.00      0.67      0.80         3\n",
            "2111.C40073.001       0.00      0.00      0.00         2\n",
            "2111.C50121.001       0.00      0.00      0.00         2\n",
            "2111.C60074.001       0.00      0.00      0.00         2\n",
            "2111.C70055.001       0.00      0.00      0.00         1\n",
            "2111.C80015.001       0.00      0.00      0.00         1\n",
            "2111.C80028.001       0.75      0.50      0.60         6\n",
            "2111.C80057.001       1.00      0.75      0.86         8\n",
            "2111.C80067.001       0.00      0.00      0.00         2\n",
            "2111.C90003.001       0.00      0.00      0.00         2\n",
            "\n",
            "       accuracy                           0.71       111\n",
            "      macro avg       0.24      0.25      0.23       111\n",
            "   weighted avg       0.60      0.71      0.64       111\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 3104\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 172 samples from 21 relevant classes. (N=1)\n",
            "Reduced to 155 samples from 10 relevant classes. (N=5)\n",
            "Training Accuracy: 0.944\n",
            "Test Accuracy: 0.581\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  3104C20000       0.67      0.50      0.57         4\n",
            "  3104C21000       0.25      0.20      0.22         5\n",
            "  3104C22000       0.56      1.00      0.71        10\n",
            "  3104C23000       0.00      0.00      0.00         3\n",
            "  3104C53000       1.00      1.00      1.00         1\n",
            "  3104C55200       1.00      1.00      1.00         1\n",
            "  3104C56000       0.00      0.00      0.00         1\n",
            " 802909914.0       0.00      0.00      0.00         2\n",
            " 802909920.0       1.00      1.00      1.00         2\n",
            " 802909926.0       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.58        31\n",
            "   macro avg       0.50      0.52      0.50        31\n",
            "weighted avg       0.47      0.58      0.50        31\n",
            "\n",
            "--------------------\n",
            "\n",
            "Company Code: 3401\n",
            "\n",
            "Cost Element classification:\n",
            "Reduced to 62 samples from 1 relevant classes. (N=1)\n",
            "Reduced to 62 samples from 1 relevant classes. (N=5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c4fe680597ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_com_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-a472a854518c>\u001b[0m in \u001b[0;36mtrain_com_model\u001b[0;34m(label, save_pkl)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_certain_class_after_vec_country\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain_com_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-eee192aebb43>\u001b[0m in \u001b[0;36mtrain_com_statistical_model\u001b[0;34m(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_com_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0;34m\" class: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m             )\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: '2340141000'"
          ]
        }
      ],
      "source": [
        "for company_code in clf_bukr.classes_:\n",
        "    print(f\"Company Code: {str(company_code).zfill(4)}\" + \"\\n\")\n",
        "    # reduce dataframe to entries for fixed company code\n",
        "    df_cost_elem = df_pka[df_pka[\"gl_legal_entity_id\"] == company_code]\n",
        "    \n",
        "    print(\"Cost Element classification:\")\n",
        "    # reduce to relevant samples\n",
        "    df_ce = reduce_to_relevant(df_cost_elem, \"Target\", min_num_samples=local_min_num)\n",
        "    \n",
        "    if df_ce.shape[0] == 0:  # do not proceed if df is empty\n",
        "        print(20 * \"-\" + \"\\n\")\n",
        "        continue\n",
        "    train_com_model('Target')\n",
        "    \n",
        "    print(20 * \"-\" + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYOm6l1cWr3T"
      },
      "source": [
        "# gl_approver (usually N=1 is used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "3bkrZ9_B95r6",
        "outputId": "d1d706a5-0d1c-4e6e-9244-232b8974efa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced to 27354 samples from 594 relevant classes. (N=5)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2ddcca600db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gl_approver'\u001b[0m \u001b[0;31m# define label here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-0581aa55f6ad>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(label, save_pkl)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#model.get_params().keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-339f059afee3>\u001b[0m in \u001b[0;36mtrain_statistical_model\u001b[0;34m(model, X_train_vec, y_train, X_test_vec, y_test, save_pkl)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_statistical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "label = 'gl_approver' # define label here\n",
        "train_model(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp5VR5XfWr3T"
      },
      "source": [
        "# gl_posting_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4y4iWoGEcf1"
      },
      "outputs": [],
      "source": [
        "label = 'gl_posting_id' # define label here\n",
        "train_model(label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Runyao_LR.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ba53e05a62b8ced6f0f8ce0c83d2d7f60547f32cea2b339741e51997916c6c74"
    },
    "kernelspec": {
      "display_name": "uniper_py_38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}