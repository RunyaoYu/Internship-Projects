{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install imblearn\n",
    "#! pip install xgboost\n",
    "\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Sequence, Tuple, Any\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pkl  # dill is used because pickle cannot handle lambda functions\n",
    "import pickle\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import dill as pkl\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "TODAY = date.today().strftime(\"%Y%m%d\")\n",
    "CLIENT = \"Uniper\"\n",
    "MIN_NUM_OF_SAMPLES = 5\n",
    "COUNTRY = 'ALL'\n",
    "SAVE_CLFS = True\n",
    "RES_DIR = Path(\"./retraining_october21/\")\n",
    "SCAN_ID_COL = \"filename\"  # document identifier col used when reducing global df to relevant examples for attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c703ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RES_DIR):\n",
    "    os.makedirs(RES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "448f60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_relevant(df: DataFrame, col: str, min_num_samples: int) -> DataFrame:\n",
    "    # find rows for values that appear at least min_num_samples times\n",
    "    relevant = [x for x in df[col].value_counts().index\n",
    "                if df[col].value_counts()[x] >= min_num_samples]\n",
    "    # create boolean mask\n",
    "    mask = [(x in relevant) for x in df[col]]\n",
    "    print(\n",
    "        f\"Reduced to {len(df[mask])} samples from {len(relevant)} relevant classes. (N={min_num_samples})\"\n",
    "    )\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def get_reduced_df(\n",
    "        df: DataFrame,\n",
    "        feature_col: str,\n",
    "        scan_id_col: str,\n",
    "        min_num_samples: int) -> DataFrame:\n",
    "    # keep only documents with unambiguous value for this col\n",
    "    df_ = df.drop_duplicates(subset=[scan_id_col, feature_col])\\\n",
    "            .groupby(scan_id_col)\\\n",
    "            .filter(lambda x: len(x) == 1)\n",
    "\n",
    "    return reduce_to_relevant(df_, feature_col, min_num_samples)\n",
    "\n",
    "\n",
    "def split_for_target_col(df, col, test_size=0.2, random_state=42):\n",
    "    return train_test_split(\n",
    "        df,\n",
    "        df[col],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "\n",
    "def split_for_target_col_stratified(df, col, test_size=0.2, random_state=42,):\n",
    "    return train_test_split(df,\n",
    "        df[col],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df[col]\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_top_n_results_with_confs(\n",
    "        clazzes: Sequence[str],\n",
    "        probs: List[float],\n",
    "        n: int = 1) -> List[Tuple[str, float]]:\n",
    "    return sorted(\n",
    "        zip(clazzes, probs),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:n]\n",
    "\n",
    "\n",
    "def get_results_for_target(\n",
    "        target_clf: Any,\n",
    "        df: DataFrame) -> List[Tuple[str, float]]:\n",
    "    probs = target_clf.predict_proba(df)\n",
    "    clazzes = target_clf.classes_\n",
    "    results = []\n",
    "    for prob_list in probs:\n",
    "        results.append(_get_top_n_results_with_confs(clazzes, prob_list)[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cb4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c3a68545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained on Tuesday 25. January 2022 with package versions: \n",
      "\n",
      "scikit-learn: 0.24.2\n",
      "dill: 0.3.4\n"
     ]
    }
   ],
   "source": [
    "tday = date.today().strftime(\"%A %d. %B %Y\") \n",
    "print(f\"Models trained on {tday} with package versions: \\n\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"dill: {pkl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b97ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict used to enrich ground truth later\n",
    "country_to_country_group = {\n",
    "    \"DE\": 'de',\n",
    "    \"SE\": 'se',\n",
    "    \"AT\": 'at',\n",
    "    \"GB\": 'uk',\n",
    "    \"BE\": 'ubx',\n",
    "    \"NL\": 'ubx',\n",
    "    \"LU\": 'ubx'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9da3fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = pd.read_excel(\"C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/Uniper_GT_09_21.xlsx\") #Uniper_GT_09_21.xlsx\n",
    "df_ground_truth.dropna(subset=['gl_document_scan_id'], inplace=True)\n",
    "df_ground_truth[\"gl_document_scan_id\"] = df_ground_truth[\"gl_document_scan_id\"].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2245e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de     47313\n",
       "se     36580\n",
       "ubx     4684\n",
       "uk      3829\n",
       "at      2429\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/texts_all_rt202110.pkl\", \"rb\") as file:\n",
    "    df_lume = pickle.load(file)\n",
    "\n",
    "df_merged = df_lume.merge(df_ground_truth, left_on=[\"filename\"], right_on=[\"gl_document_scan_id\"], how=\"inner\")\n",
    "df_merged[\"country\"] = df_merged['le_country_id'].apply(lambda x: country_to_country_group[x])\n",
    "df_merged[\"country\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "03f9c4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94835, 57) \n",
      "\n",
      "shape de: (23618, 3)\n",
      "\n",
      "shape at: (478, 3)\n",
      "\n",
      "shape uk: (7865, 3)\n",
      "\n",
      "shape se: (22014, 3)\n",
      "\n",
      "shape ubx: (4629, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lume = df_merged\n",
    "print(df_lume.shape, \"\\n\")\n",
    "\n",
    "for country in [\"de\", \"at\", \"uk\", \"se\", \"ubx\"]:\n",
    "    with open(f\"C:/Users/runyaoyu/Desktop/Project1101/Sebastian/RetrainingOctober2021/Daten/ocr_text_df_{country}.pkl\", 'rb') as file:  \n",
    "        df_lume_country = pickle.load(file)\n",
    "    df_lume_country[\"country\"] = country\n",
    "    print(f\"shape {country}: {df_lume_country.shape}\\n\")\n",
    "    df_lume = pd.concat([df_lume, df_lume_country], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ff7a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lume = df_lume[[\"filename\", \"text\", \"country\", \"gl_posting_id\"]]\n",
    "df_lume.head()\n",
    "\n",
    "# some cleaning\n",
    "df_lume[\"filename\"] = df_lume[\"filename\"].apply(lambda filename: filename.lower())\n",
    "df_lume = df_lume[df_lume[\"text\"] != 0]\n",
    "df_lume = df_lume[df_lume[\"text\"] != '']\n",
    "df_lume[\"text\"] = df_lume[\"text\"].apply(lambda text: str(text).replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").lower())\n",
    "df_lume.dropna(subset=['text'], inplace=True)\n",
    "df_lume.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6585110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_certain_class_after_vec_country(df_lume, label):\n",
    "    label = label\n",
    "    df_attr = get_reduced_df(df_lume, label, SCAN_ID_COL, MIN_NUM_OF_SAMPLES)\n",
    "    x_train, x_test, y_train, y_test = split_for_target_col_stratified(df_attr, label)\n",
    "    vectorizer = TfidfVectorizer(max_features=20000, max_df=0.75, sublinear_tf=True,)\n",
    "    X_train = x_train['text']\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test = x_test['text']\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    return df_attr, X_train, X_test, y_train, y_test, X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5efc4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 89503 samples from 5 relevant classes. (N=5)\n"
     ]
    }
   ],
   "source": [
    "# country\n",
    "label = 'country'\n",
    "df_attr, X_train, X_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_country(df_lume, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "670dedbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>country</th>\n",
       "      <th>gl_posting_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140713</th>\n",
       "      <td>ecb1d7761cc01ee7bc8efa9318d5b84a</td>\n",
       "      <td>1800305sebfd01m03788  u. faktura sida 1 av 1 5...</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24371</th>\n",
       "      <td>000d3a2c37f81eea88cdd6d13234d72d</td>\n",
       "      <td>$726  1_314577_2 e10 b27v56  l wo &lt;bas  zebi_n...</td>\n",
       "      <td>de</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11947</th>\n",
       "      <td>000d3a2c37f81eeabb81cd063e30e60d</td>\n",
       "      <td>faktura  - —— db &gt; ı 8 169 04 solna nr 9652851...</td>\n",
       "      <td>se</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146777</th>\n",
       "      <td>fc15b423ffe81ee8998fa426e4e7b315</td>\n",
       "      <td>e10  c..  |  'ei  leveransadress uniper gastur...</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120251</th>\n",
       "      <td>9cb65498ce781ee898f70478d513de3c</td>\n",
       "      <td>alstom power ltd. lichfield road stafford, st1...</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44316</th>\n",
       "      <td>b05adabc4b681ee9b0c6850b9907627e</td>\n",
       "      <td>il |  75300003056240  dds conferencing &amp; cater...</td>\n",
       "      <td>de</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68913</th>\n",
       "      <td>ecb1d7761cc01ed99fa12e46bc71a4cf</td>\n",
       "      <td>1914204sebfdo1m43810  39 = » en io ». db- ii t...</td>\n",
       "      <td>se</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33222</th>\n",
       "      <td>9cb65498ce781ed988f1b72530973b19</td>\n",
       "      <td>heavy haul power international  hh pi gmbh. st...</td>\n",
       "      <td>de</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136693</th>\n",
       "      <td>b05adabc4b681ee8b38e331ef60a64b5</td>\n",
       "      <td>1828305sebfdo1m20260  i mannheimer swartling  ...</td>\n",
       "      <td>se</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29040</th>\n",
       "      <td>000d3a2c37f81eeab5a967d104da39d0</td>\n",
       "      <td>sixt:pn electronically signed on 03.08.2020 00...</td>\n",
       "      <td>de</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "140713  ecb1d7761cc01ee7bc8efa9318d5b84a   \n",
       "24371   000d3a2c37f81eea88cdd6d13234d72d   \n",
       "11947   000d3a2c37f81eeabb81cd063e30e60d   \n",
       "146777  fc15b423ffe81ee8998fa426e4e7b315   \n",
       "120251  9cb65498ce781ee898f70478d513de3c   \n",
       "44316   b05adabc4b681ee9b0c6850b9907627e   \n",
       "68913   ecb1d7761cc01ed99fa12e46bc71a4cf   \n",
       "33222   9cb65498ce781ed988f1b72530973b19   \n",
       "136693  b05adabc4b681ee8b38e331ef60a64b5   \n",
       "29040   000d3a2c37f81eeab5a967d104da39d0   \n",
       "\n",
       "                                                     text country  \\\n",
       "140713  1800305sebfd01m03788  u. faktura sida 1 av 1 5...      se   \n",
       "24371   $726  1_314577_2 e10 b27v56  l wo <bas  zebi_n...      de   \n",
       "11947   faktura  - —— db > ı 8 169 04 solna nr 9652851...      se   \n",
       "146777  e10  c..  |  'ei  leveransadress uniper gastur...      se   \n",
       "120251  alstom power ltd. lichfield road stafford, st1...      uk   \n",
       "44316   il |  75300003056240  dds conferencing & cater...      de   \n",
       "68913   1914204sebfdo1m43810  39 = » en io ». db- ii t...      se   \n",
       "33222   heavy haul power international  hh pi gmbh. st...      de   \n",
       "136693  1828305sebfdo1m20260  i mannheimer swartling  ...      se   \n",
       "29040   sixt:pn electronically signed on 03.08.2020 00...      de   \n",
       "\n",
       "        gl_posting_id  \n",
       "140713            NaN  \n",
       "24371            40.0  \n",
       "11947            40.0  \n",
       "146777            NaN  \n",
       "120251            NaN  \n",
       "44316            40.0  \n",
       "68913            40.0  \n",
       "33222            50.0  \n",
       "136693            NaN  \n",
       "29040            40.0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attr.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "06199c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71602,) (17901,)\n",
      "(71602, 20000) (17901, 20000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(X_train_vec.shape, X_test_vec.shape)\n",
    "#print(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b94ccf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizers_new = Tokenizer(num_words=20000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizers_new.fit_on_texts(df_attr['text'].values)\n",
    "word_index = tokenizers_new.word_index\n",
    "token_id = tokenizers_new.texts_to_sequences(df_attr['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7a698aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "adb479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels_temp = df_attr[label]\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels_temp)\n",
    "\n",
    "meta = []\n",
    "\n",
    "for i in range(len(token_id)):\n",
    "    df_temp = {'id': encoded_labels[i], 'token': token_id[i]}\n",
    "    meta.append(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "aa6d7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(meta, train_size = 0.8, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e844d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample['token'] = torch.Tensor(sample['token'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "01644f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text_Dataset(train_data)\n",
    "valid_dataset = Text_Dataset(valid_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6001eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train dataloader: \n",
      "USER ID:  tensor([1], dtype=torch.int32)\n",
      "TOKEN ID:  tensor([[4.6700e+02, 6.6000e+02, 5.5880e+03, 1.1260e+03, 2.5000e+01, 1.9900e+02,\n",
      "         1.3700e+02, 1.9570e+03, 1.7000e+01, 1.9900e+02, 1.3700e+02, 2.9390e+03,\n",
      "         3.7500e+02, 1.7400e+02, 1.1050e+03, 7.3360e+03, 2.6000e+01, 1.4900e+02,\n",
      "         1.7000e+01, 7.6800e+02, 1.0112e+04, 7.0710e+03, 2.5640e+03, 1.9070e+03,\n",
      "         6.9990e+03, 6.7100e+02, 5.0400e+02, 4.0000e+01, 3.2310e+03, 6.7000e+01,\n",
      "         1.1641e+04, 1.2600e+02, 7.8000e+01, 2.5600e+02, 5.9300e+03, 7.8000e+01,\n",
      "         2.5600e+02, 2.4640e+03, 6.7000e+01, 6.8200e+02, 5.8550e+03, 9.0000e+00,\n",
      "         5.0390e+03, 5.3000e+02, 2.2280e+03, 7.6710e+03, 1.6250e+03, 7.1490e+03,\n",
      "         8.0500e+02, 6.6760e+03, 3.8080e+03, 8.1810e+03, 2.0530e+03, 2.9670e+03,\n",
      "         2.3000e+01, 5.5000e+01, 7.0000e+00, 4.5700e+03, 7.5900e+02, 1.5700e+02,\n",
      "         5.8230e+03, 7.5900e+02, 1.5234e+04, 1.5700e+02, 1.1059e+04, 3.8740e+03,\n",
      "         1.5080e+03, 1.6350e+03, 8.6000e+01, 8.3000e+02, 6.4000e+02, 3.8200e+02,\n",
      "         1.3700e+02, 1.2570e+03, 5.5200e+02, 8.7100e+02, 2.7200e+02, 5.1300e+02,\n",
      "         8.7000e+01, 1.9500e+02, 8.6500e+02, 1.9500e+02, 3.3090e+03, 1.9500e+02,\n",
      "         4.2120e+03, 1.8100e+02, 2.0000e+00, 1.0000e+00, 4.2120e+03, 1.8100e+02,\n",
      "         8.0000e+00, 1.0000e+00, 2.9390e+03, 3.7500e+02, 1.7400e+02, 3.0400e+02,\n",
      "         4.0300e+02, 1.1050e+03, 9.7200e+02, 9.0200e+02, 1.9570e+03, 1.4039e+04,\n",
      "         9.7200e+02, 1.4000e+02, 2.0360e+03, 1.0958e+04, 9.7200e+02, 1.0300e+02,\n",
      "         2.0000e+00, 1.8100e+02, 3.7400e+02, 1.0630e+04, 8.6300e+02, 9.0000e+00,\n",
      "         1.5189e+04, 2.6100e+02, 2.0000e+00, 1.8100e+02, 3.7400e+02, 1.2037e+04,\n",
      "         3.5000e+01, 1.7000e+02, 2.3600e+02, 1.9570e+03, 9.7200e+02, 2.2000e+01,\n",
      "         6.2000e+01, 1.9570e+03, 9.7200e+02, 2.2000e+01, 6.5000e+02, 1.0900e+02,\n",
      "         1.0090e+03, 1.0900e+02, 2.0900e+02, 7.3000e+01, 9.0000e+00, 7.0110e+03,\n",
      "         1.5200e+02, 3.0000e+00, 7.0000e+01, 5.5000e+01, 7.0000e+00, 7.0650e+03,\n",
      "         8.2350e+03, 4.9290e+03, 9.4000e+01, 5.9710e+03, 3.9020e+03, 8.4500e+02,\n",
      "         2.1630e+03, 3.0000e+00, 1.3200e+02, 2.4500e+02, 4.2120e+03, 1.8100e+02,\n",
      "         4.2120e+03, 1.8100e+02, 1.9500e+02, 1.4300e+02, 1.6480e+03, 9.7600e+02,\n",
      "         1.3200e+03, 2.9500e+02, 1.1000e+01, 3.6530e+03, 1.5000e+01, 4.8000e+01,\n",
      "         6.5920e+03, 9.4400e+03, 1.8030e+03, 5.1600e+02, 1.4588e+04, 2.9400e+02,\n",
      "         7.1000e+01, 1.3933e+04, 4.8000e+01, 4.1220e+03, 5.8480e+03, 1.4460e+03,\n",
      "         1.1034e+04, 1.4812e+04, 1.0000e+00, 7.1000e+01, 7.9240e+03, 1.1540e+03,\n",
      "         1.2880e+03, 8.0300e+02, 4.8000e+01, 1.1557e+04, 1.0319e+04, 6.8910e+03,\n",
      "         1.1304e+04, 1.0000e+00, 7.1000e+01, 1.6202e+04, 1.2000e+01, 7.6980e+03,\n",
      "         4.4800e+02, 1.0280e+03, 3.7000e+02, 7.3000e+01, 3.0280e+03, 1.8060e+03,\n",
      "         5.8600e+03, 4.0100e+03, 7.1800e+02, 6.5860e+03, 3.0000e+00, 1.0900e+02,\n",
      "         3.4000e+01, 1.4570e+03, 2.3790e+03, 1.7000e+01, 1.5400e+02, 8.4990e+03,\n",
      "         1.5000e+02, 1.0000e+01, 1.1059e+04, 3.8740e+03, 6.7000e+01, 4.1140e+03,\n",
      "         9.0000e+00, 2.3000e+01, 5.5000e+01, 8.0000e+00, 1.8000e+01, 6.3000e+01,\n",
      "         7.0000e+01, 8.6000e+01, 1.7000e+01, 1.0000e+01, 1.1059e+04, 3.8740e+03,\n",
      "         2.9790e+03, 1.5409e+04, 7.5900e+02, 1.5770e+03, 9.0000e+00, 1.4400e+02,\n",
      "         1.0657e+04, 3.4000e+01, 5.8230e+03, 7.5900e+02, 1.1283e+04, 1.9570e+03,\n",
      "         1.9570e+03, 1.7000e+01, 1.9900e+02, 1.3700e+02, 1.4850e+03, 9.0000e+00,\n",
      "         2.9390e+03, 3.6000e+02, 1.7400e+02, 1.1050e+03, 9.7200e+02, 4.6200e+02,\n",
      "         1.2570e+03, 8.0000e+00, 4.5000e+01, 3.1000e+01, 8.5600e+03, 1.5080e+03,\n",
      "         9.2000e+01, 7.3580e+03, 3.5300e+02, 9.2000e+01, 1.5790e+03, 5.7000e+01,\n",
      "         1.6171e+04, 5.7100e+02, 3.9100e+02, 1.8300e+02, 4.9310e+03, 1.5950e+04,\n",
      "         4.4100e+02, 3.6460e+03, 1.8900e+02, 1.9000e+02, 2.3000e+01, 5.5000e+01,\n",
      "         7.0000e+00, 7.0000e+01, 2.0000e+01, 3.2000e+01, 7.6450e+03, 9.0000e+00,\n",
      "         1.1280e+03, 8.5800e+03, 5.5700e+02, 1.1200e+02, 3.6880e+03, 1.9700e+02,\n",
      "         1.1000e+02, 1.1280e+03, 1.9700e+02, 1.9400e+02, 5.5700e+02, 1.1200e+02,\n",
      "         3.6880e+03, 6.8200e+02, 9.0000e+00, 6.1230e+03, 8.7600e+02, 2.4300e+02,\n",
      "         3.3300e+02, 1.3800e+02, 2.3860e+03, 9.0000e+00, 4.5800e+02, 7.0000e+01,\n",
      "         3.7100e+02, 8.1700e+02, 1.2736e+04, 9.0000e+00, 8.7000e+01, 8.6000e+01,\n",
      "         3.9000e+01, 1.3800e+02, 4.2100e+02, 6.2500e+02, 5.3580e+03, 3.7300e+02,\n",
      "         8.8000e+01, 1.7000e+01, 6.0000e+00, 9.7800e+02, 3.0460e+03, 9.0000e+00,\n",
      "         1.0000e+01, 3.1100e+02, 1.1059e+04, 3.8740e+03, 3.7300e+02, 8.8000e+01,\n",
      "         1.2996e+04, 3.5300e+02, 2.0900e+03, 6.0000e+00, 3.5000e+01, 9.0550e+03,\n",
      "         1.4981e+04, 9.0550e+03, 1.4981e+04, 1.8800e+02, 8.8000e+01, 1.5430e+04,\n",
      "         2.0900e+03, 3.0000e+00, 3.7000e+01, 1.2000e+01, 1.6630e+03, 6.9710e+03,\n",
      "         6.0500e+02, 1.3300e+02, 4.4000e+01, 2.9000e+01, 7.6380e+03, 8.8000e+01,\n",
      "         1.9930e+04, 4.5840e+03, 2.9000e+01, 1.3325e+04, 7.1970e+03, 3.6000e+01,\n",
      "         6.0500e+02, 1.3300e+02, 1.3688e+04, 3.8150e+03, 1.0280e+03, 6.0500e+02,\n",
      "         1.2700e+02, 8.8000e+01, 2.7000e+02, 1.7575e+04, 2.6380e+03, 1.0900e+02,\n",
      "         2.5200e+02, 7.4800e+03, 6.5750e+03, 7.4160e+03, 3.0000e+00, 1.6904e+04,\n",
      "         1.4400e+02, 2.3860e+03, 1.2000e+01, 1.2170e+03, 6.0000e+01, 2.9900e+03,\n",
      "         9.0000e+00, 3.7000e+02, 3.5200e+03, 3.4380e+03, 1.4200e+02, 1.1500e+02,\n",
      "         1.2000e+01, 3.7300e+02, 8.8000e+01, 1.1151e+04, 1.2799e+04, 1.1400e+02,\n",
      "         1.2737e+04, 8.8000e+01, 1.1151e+04, 1.2700e+02, 3.9390e+03, 1.2885e+04,\n",
      "         3.9870e+03, 9.1600e+02, 1.2400e+02, 1.7680e+03, 3.7000e+01, 1.7660e+03,\n",
      "         8.8200e+03, 2.6200e+02, 5.3200e+02, 3.7000e+01, 1.3400e+02, 5.7000e+01,\n",
      "         5.7000e+01, 5.7000e+01, 5.7000e+01, 5.7000e+01, 5.7000e+01, 2.1500e+02,\n",
      "         5.7000e+01, 5.7000e+01, 5.7000e+01, 5.7000e+01, 3.0070e+03, 4.3360e+03,\n",
      "         1.6800e+02, 1.7200e+02, 1.7720e+03, 6.6000e+02, 6.5200e+02, 1.2400e+02,\n",
      "         3.7000e+01, 3.4000e+01, 3.5000e+01, 3.1560e+03, 1.3800e+02, 4.0380e+03,\n",
      "         4.1160e+03, 6.7450e+03, 1.6820e+03, 5.9110e+03, 2.7170e+03, 1.5770e+03,\n",
      "         6.0000e+01, 2.4130e+03, 1.2170e+03, 1.2100e+02, 1.5770e+03, 3.5000e+01,\n",
      "         8.8970e+03, 2.3760e+03, 1.5000e+01, 6.7000e+01, 1.2000e+01, 3.3670e+03,\n",
      "         6.4500e+02, 3.6200e+02, 4.9600e+02, 6.7000e+01, 1.2000e+01, 1.2170e+03,\n",
      "         6.4500e+02, 3.6200e+02, 4.9600e+02, 6.7000e+01, 1.2000e+01, 1.8170e+03,\n",
      "         6.4500e+02, 3.6200e+02, 4.9600e+02, 9.2000e+01, 2.4760e+03, 9.3000e+01,\n",
      "         5.3000e+01, 3.7000e+01, 3.5980e+03, 2.5000e+01, 2.7000e+01, 6.0000e+00,\n",
      "         2.8600e+02, 2.9460e+03, 1.6000e+02, 1.4850e+03, 1.2700e+02, 3.9390e+03,\n",
      "         2.3800e+02, 7.9200e+02, 1.4850e+03, 2.3800e+02, 7.9200e+02, 4.0470e+03,\n",
      "         2.3800e+02, 7.9200e+02, 1.3400e+02, 3.1100e+02, 4.4500e+02, 4.3000e+02,\n",
      "         1.1280e+03, 5.7400e+02, 1.2820e+03, 1.9570e+03, 1.9900e+02, 1.3700e+02,\n",
      "         2.1390e+03, 6.5630e+03, 4.0000e+01, 1.0740e+03, 5.5700e+02, 2.0740e+03,\n",
      "         6.0960e+03, 2.9390e+03, 3.7500e+02, 1.8100e+02, 9.8500e+02, 3.7000e+01,\n",
      "         4.1630e+03, 3.5000e+01, 6.0000e+00, 7.7000e+01, 1.1050e+03, 7.3360e+03,\n",
      "         9.2000e+01, 9.0800e+02, 8.0000e+01, 4.4740e+03, 1.1260e+03, 1.5430e+03,\n",
      "         1.5400e+02, 1.4483e+04, 4.3200e+02, 2.1200e+02, 1.4483e+04, 8.1300e+02,\n",
      "         1.2820e+03, 3.7300e+02, 2.8700e+02, 1.0640e+03, 1.2000e+01, 3.3050e+03,\n",
      "         3.8700e+03, 9.3400e+02, 3.1100e+02, 2.3770e+03, 2.8600e+02, 3.7300e+02,\n",
      "         2.8700e+02, 1.0640e+03, 1.2000e+01, 1.0939e+04, 1.1716e+04, 1.8910e+03,\n",
      "         1.9700e+02, 2.8600e+02, 3.7300e+02, 2.8700e+02, 1.0640e+03, 1.2000e+01,\n",
      "         1.8170e+03, 1.7800e+02, 2.1070e+03, 1.5310e+03, 1.2790e+03, 1.1000e+03,\n",
      "         3.5000e+01, 8.6500e+02, 4.4000e+01, 3.5430e+03, 4.7740e+03, 8.2650e+03,\n",
      "         1.2700e+02, 2.3610e+03, 6.9400e+02, 1.1574e+04, 9.4240e+03, 3.0000e+00,\n",
      "         1.7300e+02, 1.1625e+04, 1.7540e+03, 7.5880e+03, 9.7000e+03, 7.5360e+03,\n",
      "         3.8110e+03, 7.5880e+03, 7.5360e+03, 3.8110e+03, 1.7800e+02, 3.1250e+03,\n",
      "         1.2890e+03, 9.0000e+00, 9.8120e+03, 4.5670e+03, 2.9000e+01, 1.1812e+04,\n",
      "         6.6800e+02, 8.2980e+03, 1.4400e+02, 2.8660e+03, 1.2000e+01, 1.2338e+04,\n",
      "         9.7570e+03, 7.3000e+02, 1.2705e+04, 1.4200e+02, 9.4160e+03, 1.1901e+04,\n",
      "         8.0000e+00, 1.1782e+04, 1.5400e+02, 1.7550e+03, 4.1100e+02, 3.0290e+03,\n",
      "         1.3200e+02, 2.0000e+00, 1.5000e+02, 1.7550e+03, 4.1100e+02, 3.0290e+03,\n",
      "         1.3200e+02, 2.8000e+01, 6.2000e+01, 1.2106e+04, 2.2000e+01]])\n",
      "TOKEN ID shape should be BATCH by LENGTH:  torch.Size([1, 629])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "print('Sample from train dataloader: ')\n",
    "print('USER ID: ', sample['id'])\n",
    "print('TOKEN ID: ', sample['token'])\n",
    "print('TOKEN ID shape should be BATCH by LENGTH: ', sample['token'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bc21a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_token = num_token\n",
    "        self.num_user = num_user\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_token, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, rnn_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.out_linear = nn.Linear(rnn_dim, num_user)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, token_id):\n",
    "        embed = self.embedding(token_id)\n",
    "        embed = self.dropout(embed)\n",
    "        out, _ = self.rnn(embed)\n",
    "        out = self.dropout(out)\n",
    "        return self.out_linear(out[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda'\n",
    "device = 'cpu'\n",
    "\n",
    "model = Model(num_token=len(word_index), num_user=5, embed_dim=512, rnn_dim=1024, num_layers=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fea1b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 707470341\n",
      "Prediction shape would be BATCH X NUM_USER(OUTPUT) :  torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {}'.format(num_param))\n",
    "pred = model(sample['token'].long().to(device))\n",
    "print('Prediction shape would be BATCH X NUM_USER(OUTPUT) : ', pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "avg_loss = 0.0\n",
    "best_valid_accu = 0.0\n",
    "best_epoch = -1\n",
    "best_model = None\n",
    "num_epoch = 30\n",
    "x,y = [],[]\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # start training\n",
    "    for sample in train_dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(sample['token'].long().to(device))\n",
    "\n",
    "        loss = criteria(pred, sample['id'].long().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "    # start validation\n",
    "    correct_cnt = 0.0\n",
    "    data_cnt = 0.0\n",
    "    for sample in valid_dataloader:\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(sample['token'].long().to(device))\n",
    "\n",
    "        pred_user_id = torch.argmax(pred, dim=-1)\n",
    "\n",
    "        accu = pred_user_id.detach().cpu() == sample['id']\n",
    "\n",
    "        correct_cnt += torch.sum(accu)\n",
    "        data_cnt += sample['token'].shape[0]\n",
    "\n",
    "    # calculate best valid accuracy, and save the best model. \n",
    "    curr_valid_accu = (correct_cnt / data_cnt).item()\n",
    "    print('[EPOCH {}] VALID ACCURACY: {}'.format(epoch, curr_valid_accu))\n",
    "    x.append(epoch)\n",
    "    y.append(curr_valid_accu)\n",
    "\n",
    "    best_valid_accu = max(best_valid_accu, curr_valid_accu)\n",
    "    if best_valid_accu == curr_valid_accu:\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'GRU_best_baseline.pth')\n",
    "        print('[EPOCH {}] BEST VALID ACCURACY UPDATED: {}'.format(epoch, best_valid_accu))\n",
    "\n",
    "print('FINISHED TRAINING : BEST MODEL AT EPOCH {} WITH ACCURACY {}'.format(best_epoch, best_valid_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc504fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7051503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0    61677\n",
       "NaN     58383\n",
       "50.0     5581\n",
       "70.0        5\n",
       "91.0        4\n",
       "80.0        1\n",
       "Name: gl_posting_id, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lume['gl_posting_id'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78d9099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lume.dropna(subset=['gl_posting_id'], inplace=True)\n",
    "df_lume[\"gl_posting_id\"] = df_lume[\"gl_posting_id\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "431232a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_certain_class_after_vec_gl_posting(df_lume, label):\n",
    "    label = label\n",
    "    df_attr = get_reduced_df(df_lume, label, SCAN_ID_COL, min_num_samples=6)\n",
    "    x_train, x_test, y_train, y_test = split_for_target_col_stratified(df_attr, label)\n",
    "    vectorizer = TfidfVectorizer(max_features=20000, max_df=0.75, sublinear_tf=True,)\n",
    "    X_train = x_train['text']\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test = x_test['text']\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    return x_train, x_test, y_train, y_test, X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a39e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 62601 samples from 2 relevant classes. (N=6)\n"
     ]
    }
   ],
   "source": [
    "label = 'gl_posting_id'\n",
    "x_train, x_test, y_train, y_test, X_train_vec, X_test_vec = get_certain_class_after_vec_gl_posting(df_lume, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
